{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tabular-playground-series-jan-2022 Modeling\n",
    "- データに関する知見だけでなく、データ分析の基礎的な方法をコメントで残す形とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold, GroupKFold, learning_curve, train_test_split\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, InputLayer, Add\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping, TerminateOnNaN\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "#train = pd.read_csv('../data/processed/train_v1_label.csv', index_col=0)\n",
    "#test = pd.read_csv('../data/processed/test_v1_label.csv', index_col=0)\n",
    "#train = pd.read_csv('../data/processed/train_v2_onehot.csv', index_col=0)\n",
    "#test = pd.read_csv('../data/processed/test_v2_onehot.csv', index_col=0)\n",
    "#train = pd.read_csv('../data/processed/train_v3_onehot_economy.csv', index_col=0)\n",
    "#test = pd.read_csv('../data/processed/test_v3_onehot_economy.csv', index_col=0)\n",
    "train = pd.read_csv('../data/processed/train_v4_onehot_holiday.csv', index_col=0)\n",
    "test = pd.read_csv('../data/processed/test_v4_onehot_holiday.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>Christmas</th>\n",
       "      <th>NewYear</th>\n",
       "      <th>Easter</th>\n",
       "      <th>Pentecost</th>\n",
       "      <th>country_0</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>store_0</th>\n",
       "      <th>store_1</th>\n",
       "      <th>product_0</th>\n",
       "      <th>product_1</th>\n",
       "      <th>product_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>520.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>911.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sold  year  month  day  dayofweek  Christmas  NewYear  Easter  \\\n",
       "0     329.0  2015      1    1          3          0        1       0   \n",
       "1     520.0  2015      1    1          3          0        1       0   \n",
       "2     146.0  2015      1    1          3          0        1       0   \n",
       "3     572.0  2015      1    1          3          0        1       0   \n",
       "4     911.0  2015      1    1          3          0        1       0   \n",
       "\n",
       "   Pentecost  country_0  country_1  country_2  store_0  store_1  product_0  \\\n",
       "0          0          1          0          0        1        0          0   \n",
       "1          0          1          0          0        1        0          1   \n",
       "2          0          1          0          0        1        0          0   \n",
       "3          0          1          0          0        0        1          0   \n",
       "4          0          1          0          0        0        1          1   \n",
       "\n",
       "   product_1  product_2  \n",
       "0          1          0  \n",
       "1          0          0  \n",
       "2          0          1  \n",
       "3          1          0  \n",
       "4          0          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):\n",
    "    \"\"\"Preprocess the data (select columns and scale)\n",
    "    ### MinMaxScaler: 正規化（値を0~1の範囲に）\n",
    "    ### RobustScalar: 正規化（ただし、四分位範囲を分母とする）\n",
    "    ### StandardScaler: 標準化（平均との差をとり標準偏差で割る）\n",
    "    \"\"\"\n",
    "    # preproc = make_pipeline(MinMaxScaler(), StandardScaler(with_std=False))\n",
    "    preproc = make_pipeline(RobustScaler(), StandardScaler(with_std=False))\n",
    "    df_f = pd.DataFrame(preproc.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    return df_f\n",
    "\n",
    "def normalize_dataset(df):\n",
    "    \"\"\"Preprocess the data (select columns and scale)\n",
    "    ### MinMaxScaler: 正規化（値を0~1の範囲に）\n",
    "    \"\"\"\n",
    "    preproc = make_pipeline(MinMaxScaler())\n",
    "    df_f = pd.DataFrame(preproc.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    return df_f\n",
    "\n",
    "def standardize_dataset(df):\n",
    "    \"\"\"Preprocess the data (select columns and scale)\n",
    "    ### StandardScaler: 標準化（平均との差をとり標準偏差で割る）\n",
    "    \"\"\"\n",
    "    preproc = make_pipeline(StandardScaler(with_std=False))\n",
    "    df_f = pd.DataFrame(preproc.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['num_sold'] = train['num_sold'].astype(int)\n",
    "\n",
    "#train = preprocess_dataset(train)\n",
    "#print(train.head())\n",
    "\n",
    "train_len = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n",
    "\n",
    "# preprocessed_columns = ['num_sold', 'year', 'month', 'day', 'dayofweek']\n",
    "# dataset[preprocessed_columns] = preprocess_dataset(dataset[preprocessed_columns])\n",
    "\n",
    "normlized_columns = ['year', 'month', 'day', 'dayofweek']\n",
    "standarized_columns = ['num_sold']\n",
    "\n",
    "dataset[normlized_columns] = normalize_dataset(dataset[normlized_columns])\n",
    "#dataset[standarized_columns] = standardize_dataset(dataset[standarized_columns])\n",
    "#dataset[standarized_columns] = preprocess_dataset(dataset[standarized_columns])\n",
    "\n",
    "\n",
    "train = dataset[:train_len]\n",
    "test = dataset[train_len:]\n",
    "\n",
    "Y_train = train['num_sold']\n",
    "X_train = train.drop(columns=['num_sold'])\n",
    "\n",
    "\"\"\"\n",
    "train['num_sold_base'] = train['num_sold_base'].astype(int)\n",
    "Y_train = train['num_sold_base']\n",
    "X_train = train.drop(columns=['num_sold', 'num_sold_base'])\n",
    "\"\"\"\n",
    "raw_test = pd.read_csv(\"../data/raw/test.csv\")\n",
    "test_id = raw_test['row_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>Christmas</th>\n",
       "      <th>NewYear</th>\n",
       "      <th>Easter</th>\n",
       "      <th>Pentecost</th>\n",
       "      <th>country_0</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>store_0</th>\n",
       "      <th>store_1</th>\n",
       "      <th>product_0</th>\n",
       "      <th>product_1</th>\n",
       "      <th>product_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>911.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_sold  year  month  day  dayofweek  Christmas  NewYear  Easter  \\\n",
       "0     329.0   0.0    0.0  0.0        0.5          0        1       0   \n",
       "1     520.0   0.0    0.0  0.0        0.5          0        1       0   \n",
       "2     146.0   0.0    0.0  0.0        0.5          0        1       0   \n",
       "3     572.0   0.0    0.0  0.0        0.5          0        1       0   \n",
       "4     911.0   0.0    0.0  0.0        0.5          0        1       0   \n",
       "\n",
       "   Pentecost  country_0  country_1  country_2  store_0  store_1  product_0  \\\n",
       "0          0          1          0          0        1        0          0   \n",
       "1          0          1          0          0        1        0          1   \n",
       "2          0          1          0          0        1        0          0   \n",
       "3          0          1          0          0        0        1          0   \n",
       "4          0          1          0          0        0        1          1   \n",
       "\n",
       "   product_1  product_2  \n",
       "0          1          0  \n",
       "1          0          0  \n",
       "2          0          1  \n",
       "3          1          0  \n",
       "4          0          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>Christmas</th>\n",
       "      <th>NewYear</th>\n",
       "      <th>Easter</th>\n",
       "      <th>Pentecost</th>\n",
       "      <th>country_0</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>store_0</th>\n",
       "      <th>store_1</th>\n",
       "      <th>product_0</th>\n",
       "      <th>product_1</th>\n",
       "      <th>product_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26293</th>\n",
       "      <td>823.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26294</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26295</th>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>1441.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>388.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       num_sold  year  month  day  dayofweek  Christmas  NewYear  Easter  \\\n",
       "26293     823.0  0.75    1.0  1.0        0.0          1        0       0   \n",
       "26294     250.0  0.75    1.0  1.0        0.0          1        0       0   \n",
       "26295    1004.0  0.75    1.0  1.0        0.0          1        0       0   \n",
       "26296    1441.0  0.75    1.0  1.0        0.0          1        0       0   \n",
       "26297     388.0  0.75    1.0  1.0        0.0          1        0       0   \n",
       "\n",
       "       Pentecost  country_0  country_1  country_2  store_0  store_1  \\\n",
       "26293          0          0          0          1        1        0   \n",
       "26294          0          0          0          1        1        0   \n",
       "26295          0          0          0          1        0        1   \n",
       "26296          0          0          0          1        0        1   \n",
       "26297          0          0          0          1        0        1   \n",
       "\n",
       "       product_0  product_1  product_2  \n",
       "26293          1          0          0  \n",
       "26294          0          0          1  \n",
       "26295          0          1          0  \n",
       "26296          1          0          0  \n",
       "26297          0          0          1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    329.0\n",
       "1    520.0\n",
       "2    146.0\n",
       "3    572.0\n",
       "4    911.0\n",
       "Name: num_sold, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 define setting parameters\n",
    "Cross validate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再現性確保のためのrandom_state の設定\n",
    "random_state = 43\n",
    "\n",
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 define evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_smape_score(true, pred):\n",
    "    \"\"\"SMAPEを計算する\n",
    "\n",
    "    true (np.array) : 実測値\n",
    "    pred (np.array) : 予測値\n",
    "\n",
    "    np.array        : smapeの計算結果\n",
    "    \"\"\"\n",
    "\n",
    "    return 100 / len(true) * np.sum(2 * np.abs(pred - true) / (np.abs(pred) + np.abs(true)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_loss(true, pred):\n",
    "    return 100 / len(true) * tf.sum(2 * tf.abs(pred - true) / (tf.abs(pred) + tf.abs(true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_smape = make_scorer(calc_smape_score, greater_is_better=False)\n",
    "\n",
    "score_funcs = {\n",
    "    'smape': scoring_smape,\n",
    "    'R2': make_scorer(r2_score, greater_is_better=True),\n",
    "    'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 split datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_dim):\n",
    "    \"\"\"simple linear model\n",
    "\n",
    "    The model is to be used with a log-transformed target.\n",
    "    \"\"\"\n",
    "    x = Input(shape=(input_dim))\n",
    "    z = Dense(32, use_bias=True)(x)\n",
    "    output = Dense(1, use_bias=True)(z)\n",
    "    model = Model(x, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_callbacks():\n",
    "    # Define the learning rate schedule and EarlyStopping\n",
    "    if True:\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7,\n",
    "                               patience=150, verbose=1)\n",
    "        es = EarlyStopping(monitor=\"val_loss\",\n",
    "                           patience=300,\n",
    "                           verbose=1,\n",
    "                           mode=\"min\",\n",
    "                           restore_best_weights=True)\n",
    "        callbacks = [lr, es, TerminateOnNaN()]\n",
    "\n",
    "    else:\n",
    "        epochs = EPOCHS_COSINEDECAY\n",
    "        lr_start=0.02\n",
    "        lr_end=0.00001\n",
    "        def cosine_decay(epoch):\n",
    "            if epochs > 1:\n",
    "                w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "            else:\n",
    "                w = 1\n",
    "            return w * lr_start + (1 - w) * lr_end\n",
    "\n",
    "        lr = LearningRateScheduler(cosine_decay, verbose=0)\n",
    "        callbacks = [lr, TerminateOnNaN()]\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_epochs():\n",
    "    # Define the learning rate schedule and EarlyStopping\n",
    "    if USE_PLATEAU and X_va is not None:\n",
    "        epochs = EPOCHS\n",
    "\n",
    "    else:\n",
    "        epochs = EPOCHS_COSINEDECAY\n",
    "        lr_start=0.02\n",
    "        lr_end=0.00001\n",
    "        def cosine_decay(epoch):\n",
    "            if epochs > 1:\n",
    "                w = (1 + math.cos(epoch / (epochs-1) * math.pi)) / 2\n",
    "            else:\n",
    "                w = 1\n",
    "            return w * lr_start + (1 - w) * lr_end\n",
    "\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n",
    "    \"\"\"Plot (the last n_epochs epochs of) the training history\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    if n_epochs is None:\n",
    "        from_epoch = 0\n",
    "    else:\n",
    "        from_epoch = max(len(history['loss']) - n_epochs, 0)\n",
    "\n",
    "    # Plot training and validation losses\n",
    "    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n",
    "    plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n",
    "\n",
    "\n",
    "    best_epoch = np.argmin(np.array(history['val_loss']))\n",
    "    best_val_loss = history['val_loss'][best_epoch]\n",
    "\n",
    "\n",
    "    if best_epoch >= from_epoch:\n",
    "        plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n",
    "    if best_epoch > 0:\n",
    "        almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n",
    "        almost_val_loss = history['val_loss'][almost_epoch]\n",
    "        if almost_epoch >= from_epoch:\n",
    "            plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n",
    "\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='lower left')\n",
    "\n",
    "    if bottom is not None:\n",
    "        plt.ylim(bottom=bottom)\n",
    "    if top is not None:\n",
    "        plt.ylim(top=top)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "    # Plot learning rate\n",
    "    if plot_lr and 'lr' in history:\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n",
    "        ax2.set_ylabel('Learning rate')\n",
    "        ax2.legend(loc='upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_scatter(true, pred):\n",
    "\n",
    "    # Plot y_true vs. y_pred\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(true, pred, s=1, color='r')\n",
    "    #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n",
    "\n",
    "    plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.xlabel('y_true')\n",
    "    plt.ylabel('y_pred')\n",
    "    plt.title('OOF Predictions')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(x_train, y_train, x_val, y_val, X_va=None):\n",
    "    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n",
    "\n",
    "    # Construct and compile the model\n",
    "    model = simple_model(len(x_train.columns))\n",
    "    print(model.summary())\n",
    "    #model.compile(loss='mse', metrics=smape_loss)\n",
    "    #model.compile(loss='mse')\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
    "    #model.compile(optimizer=optimizers.adam(learning_rate=0.01), loss='mse')\n",
    "    #model.compile(optimizer=optimizers.SGD(), loss='mse')\n",
    "\n",
    "    callbacks = setting_callbacks()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x=x_train, y=y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        batch_size=512,\n",
    "                        epochs=5000,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        shuffle=True)\n",
    "\n",
    "    if X_va is not None:\n",
    "        # Inference for validation\n",
    "        y_va_pred = np.exp(model.predict([X_va_f[features]]))\n",
    "        oof_list[run][val_idx] = y_va_pred\n",
    "\n",
    "        # Evaluation: Execution time and SMAPE\n",
    "        smape = np.mean(smape_loss(y_va, y_va_pred))\n",
    "        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n",
    "              f\" | SMAPE: {smape:.5f} validated on {X_va.iloc[0].date.year}\")\n",
    "        score_list.append(smape)\n",
    "\n",
    "    return model, history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, x):\n",
    "    pred = model.predict(x)\n",
    "    pred = pred.reshape(-1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0.0\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuya/.pyenv/versions/3.9.7/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 239626.4062 - val_loss: 178187.2188 - lr: 0.0100\n",
      "Epoch 2/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 206683.1562 - val_loss: 139549.1406 - lr: 0.0100\n",
      "Epoch 3/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 150141.4844 - val_loss: 91189.8047 - lr: 0.0100\n",
      "Epoch 4/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 97555.8750 - val_loss: 60766.9492 - lr: 0.0100\n",
      "Epoch 5/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 70870.8984 - val_loss: 49020.8320 - lr: 0.0100\n",
      "Epoch 6/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 59322.8320 - val_loss: 40994.6562 - lr: 0.0100\n",
      "Epoch 7/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 50361.4336 - val_loss: 33929.3008 - lr: 0.0100\n",
      "Epoch 8/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 42507.9297 - val_loss: 27851.4141 - lr: 0.0100\n",
      "Epoch 9/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 35586.3789 - val_loss: 22851.5234 - lr: 0.0100\n",
      "Epoch 10/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 29835.6758 - val_loss: 18930.8965 - lr: 0.0100\n",
      "Epoch 11/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 25207.5469 - val_loss: 16220.9150 - lr: 0.0100\n",
      "Epoch 12/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 21832.5391 - val_loss: 14518.1367 - lr: 0.0100\n",
      "Epoch 13/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 19501.9453 - val_loss: 13590.7451 - lr: 0.0100\n",
      "Epoch 14/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 18009.1055 - val_loss: 13159.6348 - lr: 0.0100\n",
      "Epoch 15/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 17061.4082 - val_loss: 12999.3799 - lr: 0.0100\n",
      "Epoch 16/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 16456.4609 - val_loss: 12915.0264 - lr: 0.0100\n",
      "Epoch 17/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 16074.9092 - val_loss: 12895.4404 - lr: 0.0100\n",
      "Epoch 18/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15792.2725 - val_loss: 12878.2676 - lr: 0.0100\n",
      "Epoch 19/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15585.1045 - val_loss: 12831.4443 - lr: 0.0100\n",
      "Epoch 20/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15426.9180 - val_loss: 12787.2373 - lr: 0.0100\n",
      "Epoch 21/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15286.2148 - val_loss: 12729.3350 - lr: 0.0100\n",
      "Epoch 22/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15171.2158 - val_loss: 12671.1338 - lr: 0.0100\n",
      "Epoch 23/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15073.1582 - val_loss: 12634.8926 - lr: 0.0100\n",
      "Epoch 24/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14992.8477 - val_loss: 12581.0225 - lr: 0.0100\n",
      "Epoch 25/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14914.5586 - val_loss: 12580.4941 - lr: 0.0100\n",
      "Epoch 26/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14850.8936 - val_loss: 12547.3145 - lr: 0.0100\n",
      "Epoch 27/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14793.0596 - val_loss: 12489.8271 - lr: 0.0100\n",
      "Epoch 28/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14735.8174 - val_loss: 12497.9561 - lr: 0.0100\n",
      "Epoch 29/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14696.2471 - val_loss: 12461.8174 - lr: 0.0100\n",
      "Epoch 30/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14658.0156 - val_loss: 12485.1943 - lr: 0.0100\n",
      "Epoch 31/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14623.2090 - val_loss: 12451.7393 - lr: 0.0100\n",
      "Epoch 32/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14599.1621 - val_loss: 12456.1523 - lr: 0.0100\n",
      "Epoch 33/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14572.6660 - val_loss: 12436.5840 - lr: 0.0100\n",
      "Epoch 34/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14552.5117 - val_loss: 12367.8096 - lr: 0.0100\n",
      "Epoch 35/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14536.8887 - val_loss: 12393.8447 - lr: 0.0100\n",
      "Epoch 36/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14522.3096 - val_loss: 12421.4814 - lr: 0.0100\n",
      "Epoch 37/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14509.8848 - val_loss: 12360.7002 - lr: 0.0100\n",
      "Epoch 38/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14514.9541 - val_loss: 12370.1562 - lr: 0.0100\n",
      "Epoch 39/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14497.5674 - val_loss: 12502.2080 - lr: 0.0100\n",
      "Epoch 40/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14487.0381 - val_loss: 12363.4717 - lr: 0.0100\n",
      "Epoch 41/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14479.8232 - val_loss: 12401.5586 - lr: 0.0100\n",
      "Epoch 42/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.9541 - val_loss: 12379.9395 - lr: 0.0100\n",
      "Epoch 43/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.1309 - val_loss: 12476.7021 - lr: 0.0100\n",
      "Epoch 44/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.6279 - val_loss: 12418.0723 - lr: 0.0100\n",
      "Epoch 45/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.6123 - val_loss: 12425.8271 - lr: 0.0100\n",
      "Epoch 46/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.5684 - val_loss: 12452.1729 - lr: 0.0100\n",
      "Epoch 47/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.1006 - val_loss: 12373.3906 - lr: 0.0100\n",
      "Epoch 48/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.7471 - val_loss: 12357.0000 - lr: 0.0100\n",
      "Epoch 49/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8262 - val_loss: 12366.4336 - lr: 0.0100\n",
      "Epoch 50/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.9316 - val_loss: 12337.8740 - lr: 0.0100\n",
      "Epoch 51/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2217 - val_loss: 12412.1113 - lr: 0.0100\n",
      "Epoch 52/5000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 14460.3047 - val_loss: 12471.7764 - lr: 0.0100\n",
      "Epoch 53/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.7539 - val_loss: 12281.1367 - lr: 0.0100\n",
      "Epoch 54/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.7998 - val_loss: 12290.0244 - lr: 0.0100\n",
      "Epoch 55/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.3770 - val_loss: 12318.2930 - lr: 0.0100\n",
      "Epoch 56/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.7520 - val_loss: 12503.0811 - lr: 0.0100\n",
      "Epoch 57/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.6982 - val_loss: 12358.3574 - lr: 0.0100\n",
      "Epoch 58/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.9424 - val_loss: 12270.3193 - lr: 0.0100\n",
      "Epoch 59/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.3818 - val_loss: 12224.5469 - lr: 0.0100\n",
      "Epoch 60/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.3213 - val_loss: 12335.1660 - lr: 0.0100\n",
      "Epoch 61/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.5098 - val_loss: 12330.8057 - lr: 0.0100\n",
      "Epoch 62/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.6836 - val_loss: 12292.0664 - lr: 0.0100\n",
      "Epoch 63/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.0840 - val_loss: 12439.7256 - lr: 0.0100\n",
      "Epoch 64/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.8584 - val_loss: 12350.8730 - lr: 0.0100\n",
      "Epoch 65/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.9150 - val_loss: 12296.5498 - lr: 0.0100\n",
      "Epoch 66/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.9541 - val_loss: 12283.5029 - lr: 0.0100\n",
      "Epoch 67/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.8877 - val_loss: 12401.8252 - lr: 0.0100\n",
      "Epoch 68/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.1445 - val_loss: 12265.3047 - lr: 0.0100\n",
      "Epoch 69/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.1406 - val_loss: 12353.8994 - lr: 0.0100\n",
      "Epoch 70/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.5264 - val_loss: 12306.0322 - lr: 0.0100\n",
      "Epoch 71/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.7344 - val_loss: 12284.9316 - lr: 0.0100\n",
      "Epoch 72/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.7119 - val_loss: 12295.1807 - lr: 0.0100\n",
      "Epoch 73/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3057 - val_loss: 12323.0762 - lr: 0.0100\n",
      "Epoch 74/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3779 - val_loss: 12365.3428 - lr: 0.0100\n",
      "Epoch 75/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.7148 - val_loss: 12430.0352 - lr: 0.0100\n",
      "Epoch 76/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.8242 - val_loss: 12243.7705 - lr: 0.0100\n",
      "Epoch 77/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.7090 - val_loss: 12288.4961 - lr: 0.0100\n",
      "Epoch 78/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.6523 - val_loss: 12254.5039 - lr: 0.0100\n",
      "Epoch 79/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.3477 - val_loss: 12233.7471 - lr: 0.0100\n",
      "Epoch 80/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.5742 - val_loss: 12391.4053 - lr: 0.0100\n",
      "Epoch 81/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.0225 - val_loss: 12479.6924 - lr: 0.0100\n",
      "Epoch 82/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.5205 - val_loss: 12193.8193 - lr: 0.0100\n",
      "Epoch 83/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14450.7773 - val_loss: 12367.8281 - lr: 0.0100\n",
      "Epoch 84/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.7021 - val_loss: 12361.2793 - lr: 0.0100\n",
      "Epoch 85/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.0605 - val_loss: 12196.4375 - lr: 0.0100\n",
      "Epoch 86/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.6631 - val_loss: 12239.1270 - lr: 0.0100\n",
      "Epoch 87/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.5615 - val_loss: 12211.1260 - lr: 0.0100\n",
      "Epoch 88/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.1572 - val_loss: 12330.8867 - lr: 0.0100\n",
      "Epoch 89/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.1133 - val_loss: 12327.0361 - lr: 0.0100\n",
      "Epoch 90/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8281 - val_loss: 12310.8799 - lr: 0.0100\n",
      "Epoch 91/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.0664 - val_loss: 12263.5781 - lr: 0.0100\n",
      "Epoch 92/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.2451 - val_loss: 12217.0273 - lr: 0.0100\n",
      "Epoch 93/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.9971 - val_loss: 12309.4307 - lr: 0.0100\n",
      "Epoch 94/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.8506 - val_loss: 12148.8525 - lr: 0.0100\n",
      "Epoch 95/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.7139 - val_loss: 12257.7656 - lr: 0.0100\n",
      "Epoch 96/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.3818 - val_loss: 12248.4678 - lr: 0.0100\n",
      "Epoch 97/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.2588 - val_loss: 12263.2969 - lr: 0.0100\n",
      "Epoch 98/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.5312 - val_loss: 12209.3594 - lr: 0.0100\n",
      "Epoch 99/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.5098 - val_loss: 12259.8496 - lr: 0.0100\n",
      "Epoch 100/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.9004 - val_loss: 12234.4746 - lr: 0.0100\n",
      "Epoch 101/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.9199 - val_loss: 12192.5869 - lr: 0.0100\n",
      "Epoch 102/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14485.9121 - val_loss: 12172.2969 - lr: 0.0100\n",
      "Epoch 103/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.6396 - val_loss: 12207.8447 - lr: 0.0100\n",
      "Epoch 104/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.2744 - val_loss: 12247.5684 - lr: 0.0100\n",
      "Epoch 105/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.0645 - val_loss: 12274.0713 - lr: 0.0100\n",
      "Epoch 106/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.2598 - val_loss: 12242.2451 - lr: 0.0100\n",
      "Epoch 107/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.9746 - val_loss: 12219.5508 - lr: 0.0100\n",
      "Epoch 108/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.4346 - val_loss: 12411.7148 - lr: 0.0100\n",
      "Epoch 109/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.7461 - val_loss: 12254.8203 - lr: 0.0100\n",
      "Epoch 110/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.5264 - val_loss: 12339.3398 - lr: 0.0100\n",
      "Epoch 111/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3887 - val_loss: 12157.7598 - lr: 0.0100\n",
      "Epoch 112/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7275 - val_loss: 12243.5791 - lr: 0.0100\n",
      "Epoch 113/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2598 - val_loss: 12246.3418 - lr: 0.0100\n",
      "Epoch 114/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.1299 - val_loss: 12140.6660 - lr: 0.0100\n",
      "Epoch 115/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.7734 - val_loss: 12242.3711 - lr: 0.0100\n",
      "Epoch 116/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.1963 - val_loss: 12276.0059 - lr: 0.0100\n",
      "Epoch 117/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.0596 - val_loss: 12233.9688 - lr: 0.0100\n",
      "Epoch 118/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5654 - val_loss: 12152.5420 - lr: 0.0100\n",
      "Epoch 119/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.6162 - val_loss: 12249.6611 - lr: 0.0100\n",
      "Epoch 120/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.5498 - val_loss: 12173.5732 - lr: 0.0100\n",
      "Epoch 121/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.4629 - val_loss: 12246.1748 - lr: 0.0100\n",
      "Epoch 122/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.4941 - val_loss: 12338.4805 - lr: 0.0100\n",
      "Epoch 123/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.1016 - val_loss: 12183.4414 - lr: 0.0100\n",
      "Epoch 124/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.6582 - val_loss: 12200.6650 - lr: 0.0100\n",
      "Epoch 125/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.9033 - val_loss: 12251.5625 - lr: 0.0100\n",
      "Epoch 126/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.9980 - val_loss: 12192.1689 - lr: 0.0100\n",
      "Epoch 127/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.7754 - val_loss: 12235.6064 - lr: 0.0100\n",
      "Epoch 128/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.4014 - val_loss: 12207.0986 - lr: 0.0100\n",
      "Epoch 129/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.0254 - val_loss: 12182.3965 - lr: 0.0100\n",
      "Epoch 130/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.9785 - val_loss: 12234.2803 - lr: 0.0100\n",
      "Epoch 131/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.9629 - val_loss: 12241.3584 - lr: 0.0100\n",
      "Epoch 132/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.6416 - val_loss: 12294.7363 - lr: 0.0100\n",
      "Epoch 133/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.1738 - val_loss: 12370.2480 - lr: 0.0100\n",
      "Epoch 134/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.1885 - val_loss: 12165.2285 - lr: 0.0100\n",
      "Epoch 135/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.2617 - val_loss: 12155.1152 - lr: 0.0100\n",
      "Epoch 136/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.1758 - val_loss: 12346.9219 - lr: 0.0100\n",
      "Epoch 137/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.3535 - val_loss: 12197.8916 - lr: 0.0100\n",
      "Epoch 138/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.9873 - val_loss: 12214.1455 - lr: 0.0100\n",
      "Epoch 139/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.6318 - val_loss: 12261.8711 - lr: 0.0100\n",
      "Epoch 140/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.9248 - val_loss: 12171.9414 - lr: 0.0100\n",
      "Epoch 141/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.1240 - val_loss: 12273.6846 - lr: 0.0100\n",
      "Epoch 142/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.7734 - val_loss: 12334.4082 - lr: 0.0100\n",
      "Epoch 143/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3564 - val_loss: 12077.4521 - lr: 0.0100\n",
      "Epoch 144/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.3525 - val_loss: 12222.9844 - lr: 0.0100\n",
      "Epoch 145/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8662 - val_loss: 12177.3125 - lr: 0.0100\n",
      "Epoch 146/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.7363 - val_loss: 12273.3555 - lr: 0.0100\n",
      "Epoch 147/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.8174 - val_loss: 12254.5400 - lr: 0.0100\n",
      "Epoch 148/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.6572 - val_loss: 12331.6982 - lr: 0.0100\n",
      "Epoch 149/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.0898 - val_loss: 12240.1836 - lr: 0.0100\n",
      "Epoch 150/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.1777 - val_loss: 12165.4746 - lr: 0.0100\n",
      "Epoch 151/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.9932 - val_loss: 12199.9951 - lr: 0.0100\n",
      "Epoch 152/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.9395 - val_loss: 12229.2021 - lr: 0.0100\n",
      "Epoch 153/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3164 - val_loss: 12322.1074 - lr: 0.0100\n",
      "Epoch 154/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.5889 - val_loss: 12113.7949 - lr: 0.0100\n",
      "Epoch 155/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.4697 - val_loss: 12208.2051 - lr: 0.0100\n",
      "Epoch 156/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.4727 - val_loss: 12145.0117 - lr: 0.0100\n",
      "Epoch 157/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.8174 - val_loss: 12287.7012 - lr: 0.0100\n",
      "Epoch 158/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.1943 - val_loss: 12337.8359 - lr: 0.0100\n",
      "Epoch 159/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.9170 - val_loss: 12241.6973 - lr: 0.0100\n",
      "Epoch 160/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.8350 - val_loss: 12193.3096 - lr: 0.0100\n",
      "Epoch 161/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.5049 - val_loss: 12198.7510 - lr: 0.0100\n",
      "Epoch 162/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.1895 - val_loss: 12312.3555 - lr: 0.0100\n",
      "Epoch 163/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.0625 - val_loss: 12251.4023 - lr: 0.0100\n",
      "Epoch 164/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.7627 - val_loss: 12209.5781 - lr: 0.0100\n",
      "Epoch 165/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.5391 - val_loss: 12268.4854 - lr: 0.0100\n",
      "Epoch 166/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.5361 - val_loss: 12238.1484 - lr: 0.0100\n",
      "Epoch 167/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.6416 - val_loss: 12293.4248 - lr: 0.0100\n",
      "Epoch 168/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.1836 - val_loss: 12139.4473 - lr: 0.0100\n",
      "Epoch 169/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.8447 - val_loss: 12231.4111 - lr: 0.0100\n",
      "Epoch 170/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.0107 - val_loss: 12199.0439 - lr: 0.0100\n",
      "Epoch 171/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.8145 - val_loss: 12097.8369 - lr: 0.0100\n",
      "Epoch 172/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.0039 - val_loss: 12168.6689 - lr: 0.0100\n",
      "Epoch 173/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.0420 - val_loss: 12313.7266 - lr: 0.0100\n",
      "Epoch 174/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14497.2812 - val_loss: 12197.4453 - lr: 0.0100\n",
      "Epoch 175/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.4004 - val_loss: 12238.1846 - lr: 0.0100\n",
      "Epoch 176/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.0381 - val_loss: 12264.2178 - lr: 0.0100\n",
      "Epoch 177/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.5508 - val_loss: 12307.6816 - lr: 0.0100\n",
      "Epoch 178/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.8193 - val_loss: 12300.5078 - lr: 0.0100\n",
      "Epoch 179/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7891 - val_loss: 12249.9834 - lr: 0.0100\n",
      "Epoch 180/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.7158 - val_loss: 12141.0732 - lr: 0.0100\n",
      "Epoch 181/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.0352 - val_loss: 12280.1797 - lr: 0.0100\n",
      "Epoch 182/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14488.0234 - val_loss: 12376.0957 - lr: 0.0100\n",
      "Epoch 183/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.2373 - val_loss: 12207.1631 - lr: 0.0100\n",
      "Epoch 184/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.5439 - val_loss: 12207.5039 - lr: 0.0100\n",
      "Epoch 185/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.2871 - val_loss: 12140.9658 - lr: 0.0100\n",
      "Epoch 186/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.4004 - val_loss: 12145.7676 - lr: 0.0100\n",
      "Epoch 187/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.7832 - val_loss: 12179.4141 - lr: 0.0100\n",
      "Epoch 188/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.8232 - val_loss: 12294.8945 - lr: 0.0100\n",
      "Epoch 189/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14496.4902 - val_loss: 12385.0068 - lr: 0.0100\n",
      "Epoch 190/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14486.4561 - val_loss: 12196.4297 - lr: 0.0100\n",
      "Epoch 191/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.1240 - val_loss: 12248.9209 - lr: 0.0100\n",
      "Epoch 192/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14497.1309 - val_loss: 12354.5693 - lr: 0.0100\n",
      "Epoch 193/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.9619 - val_loss: 12210.9521 - lr: 0.0100\n",
      "Epoch 194/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.7383 - val_loss: 12341.6260 - lr: 0.0100\n",
      "Epoch 195/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14498.1787 - val_loss: 12145.3594 - lr: 0.0100\n",
      "Epoch 196/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.3945 - val_loss: 12188.3896 - lr: 0.0100\n",
      "Epoch 197/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7148 - val_loss: 12255.5625 - lr: 0.0100\n",
      "Epoch 198/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.6992 - val_loss: 12339.1846 - lr: 0.0100\n",
      "Epoch 199/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.3203 - val_loss: 12215.0391 - lr: 0.0100\n",
      "Epoch 200/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.3721 - val_loss: 12209.5312 - lr: 0.0100\n",
      "Epoch 201/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.1016 - val_loss: 12141.3525 - lr: 0.0100\n",
      "Epoch 202/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.6533 - val_loss: 12233.2256 - lr: 0.0100\n",
      "Epoch 203/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.2441 - val_loss: 12179.1191 - lr: 0.0100\n",
      "Epoch 204/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14481.3926 - val_loss: 12304.0762 - lr: 0.0100\n",
      "Epoch 205/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.4824 - val_loss: 12192.3604 - lr: 0.0100\n",
      "Epoch 206/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.7441 - val_loss: 12333.9482 - lr: 0.0100\n",
      "Epoch 207/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.0430 - val_loss: 12266.0928 - lr: 0.0100\n",
      "Epoch 208/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.7754 - val_loss: 12298.8955 - lr: 0.0100\n",
      "Epoch 209/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.5566 - val_loss: 12179.5498 - lr: 0.0100\n",
      "Epoch 210/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14495.9756 - val_loss: 12185.4531 - lr: 0.0100\n",
      "Epoch 211/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14481.9199 - val_loss: 12256.0479 - lr: 0.0100\n",
      "Epoch 212/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.7842 - val_loss: 12359.6631 - lr: 0.0100\n",
      "Epoch 213/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.6162 - val_loss: 12116.2705 - lr: 0.0100\n",
      "Epoch 214/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14491.7109 - val_loss: 12181.5684 - lr: 0.0100\n",
      "Epoch 215/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14518.1221 - val_loss: 12337.6084 - lr: 0.0100\n",
      "Epoch 216/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.6934 - val_loss: 12263.0098 - lr: 0.0100\n",
      "Epoch 217/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.8975 - val_loss: 12362.8857 - lr: 0.0100\n",
      "Epoch 218/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.7256 - val_loss: 12329.0898 - lr: 0.0100\n",
      "Epoch 219/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.0850 - val_loss: 12117.7197 - lr: 0.0100\n",
      "Epoch 220/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.2061 - val_loss: 12264.9658 - lr: 0.0100\n",
      "Epoch 221/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.0322 - val_loss: 12311.6611 - lr: 0.0100\n",
      "Epoch 222/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.3008 - val_loss: 12172.0332 - lr: 0.0100\n",
      "Epoch 223/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.3916 - val_loss: 12210.3652 - lr: 0.0100\n",
      "Epoch 224/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.5732 - val_loss: 12313.3232 - lr: 0.0100\n",
      "Epoch 225/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.4219 - val_loss: 12262.6182 - lr: 0.0100\n",
      "Epoch 226/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6025 - val_loss: 12164.4150 - lr: 0.0100\n",
      "Epoch 227/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.7539 - val_loss: 12276.4150 - lr: 0.0100\n",
      "Epoch 228/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.1807 - val_loss: 12249.6260 - lr: 0.0100\n",
      "Epoch 229/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.4805 - val_loss: 12222.2754 - lr: 0.0100\n",
      "Epoch 230/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.0938 - val_loss: 12244.9902 - lr: 0.0100\n",
      "Epoch 231/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.9121 - val_loss: 12317.5713 - lr: 0.0100\n",
      "Epoch 232/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.2510 - val_loss: 12191.6221 - lr: 0.0100\n",
      "Epoch 233/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14486.8457 - val_loss: 12175.8467 - lr: 0.0100\n",
      "Epoch 234/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14479.2275 - val_loss: 12207.0361 - lr: 0.0100\n",
      "Epoch 235/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14484.9746 - val_loss: 12277.9648 - lr: 0.0100\n",
      "Epoch 236/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14489.8682 - val_loss: 12241.1270 - lr: 0.0100\n",
      "Epoch 237/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.5947 - val_loss: 12145.3184 - lr: 0.0100\n",
      "Epoch 238/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.2969 - val_loss: 12161.2051 - lr: 0.0100\n",
      "Epoch 239/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.5820 - val_loss: 12185.2705 - lr: 0.0100\n",
      "Epoch 240/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.4434 - val_loss: 12169.6826 - lr: 0.0100\n",
      "Epoch 241/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.6553 - val_loss: 12137.0527 - lr: 0.0100\n",
      "Epoch 242/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14489.0459 - val_loss: 12227.4883 - lr: 0.0100\n",
      "Epoch 243/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.0684 - val_loss: 12235.8789 - lr: 0.0100\n",
      "Epoch 244/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.2334 - val_loss: 12103.9473 - lr: 0.0100\n",
      "Epoch 245/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.8203 - val_loss: 12336.7773 - lr: 0.0100\n",
      "Epoch 246/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.7148 - val_loss: 12203.7168 - lr: 0.0100\n",
      "Epoch 247/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14483.2646 - val_loss: 12167.0596 - lr: 0.0100\n",
      "Epoch 248/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14508.8926 - val_loss: 12257.8994 - lr: 0.0100\n",
      "Epoch 249/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.4912 - val_loss: 12236.2900 - lr: 0.0100\n",
      "Epoch 250/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.7617 - val_loss: 12207.9600 - lr: 0.0100\n",
      "Epoch 251/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.7783 - val_loss: 12146.6348 - lr: 0.0100\n",
      "Epoch 252/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.5059 - val_loss: 12281.5625 - lr: 0.0100\n",
      "Epoch 253/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.3545 - val_loss: 12170.7168 - lr: 0.0100\n",
      "Epoch 254/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.6914 - val_loss: 12232.4863 - lr: 0.0100\n",
      "Epoch 255/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.3652 - val_loss: 12305.6309 - lr: 0.0100\n",
      "Epoch 256/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.0078 - val_loss: 12205.8984 - lr: 0.0100\n",
      "Epoch 257/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14479.7236 - val_loss: 12211.5068 - lr: 0.0100\n",
      "Epoch 258/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.6641 - val_loss: 12158.3320 - lr: 0.0100\n",
      "Epoch 259/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14487.1670 - val_loss: 12284.2900 - lr: 0.0100\n",
      "Epoch 260/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.2861 - val_loss: 12201.8984 - lr: 0.0100\n",
      "Epoch 261/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.2686 - val_loss: 12336.8711 - lr: 0.0100\n",
      "Epoch 262/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.6006 - val_loss: 12203.7207 - lr: 0.0100\n",
      "Epoch 263/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.5986 - val_loss: 12352.3770 - lr: 0.0100\n",
      "Epoch 264/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.9746 - val_loss: 12299.6953 - lr: 0.0100\n",
      "Epoch 265/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.2373 - val_loss: 12273.2793 - lr: 0.0100\n",
      "Epoch 266/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.6670 - val_loss: 12308.9043 - lr: 0.0100\n",
      "Epoch 267/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14488.7686 - val_loss: 12264.6709 - lr: 0.0100\n",
      "Epoch 268/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.2148 - val_loss: 12205.1611 - lr: 0.0100\n",
      "Epoch 269/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.9688 - val_loss: 12269.6357 - lr: 0.0100\n",
      "Epoch 270/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.5654 - val_loss: 12372.5967 - lr: 0.0100\n",
      "Epoch 271/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14486.9043 - val_loss: 12169.7490 - lr: 0.0100\n",
      "Epoch 272/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8145 - val_loss: 12299.7344 - lr: 0.0100\n",
      "Epoch 273/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.6982 - val_loss: 12315.8252 - lr: 0.0100\n",
      "Epoch 274/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.2002 - val_loss: 12163.0371 - lr: 0.0100\n",
      "Epoch 275/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14488.2217 - val_loss: 12191.0088 - lr: 0.0100\n",
      "Epoch 276/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14494.4414 - val_loss: 12306.7295 - lr: 0.0100\n",
      "Epoch 277/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.4551 - val_loss: 12293.7969 - lr: 0.0100\n",
      "Epoch 278/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14503.7988 - val_loss: 12185.9102 - lr: 0.0100\n",
      "Epoch 279/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.7617 - val_loss: 12198.7100 - lr: 0.0100\n",
      "Epoch 280/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.0879 - val_loss: 12199.9023 - lr: 0.0100\n",
      "Epoch 281/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.3047 - val_loss: 12143.2119 - lr: 0.0100\n",
      "Epoch 282/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.0645 - val_loss: 12138.0820 - lr: 0.0100\n",
      "Epoch 283/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.5107 - val_loss: 12174.8330 - lr: 0.0100\n",
      "Epoch 284/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.2998 - val_loss: 12152.5264 - lr: 0.0100\n",
      "Epoch 285/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.2793 - val_loss: 12150.4814 - lr: 0.0100\n",
      "Epoch 286/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14515.3457 - val_loss: 12308.7568 - lr: 0.0100\n",
      "Epoch 287/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.6152 - val_loss: 12166.5518 - lr: 0.0100\n",
      "Epoch 288/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.4775 - val_loss: 12232.6562 - lr: 0.0100\n",
      "Epoch 289/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14470.7734 - val_loss: 12158.3936 - lr: 0.0100\n",
      "Epoch 290/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8066 - val_loss: 12250.1709 - lr: 0.0100\n",
      "Epoch 291/5000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 14488.1572 - val_loss: 12126.8779 - lr: 0.0100\n",
      "Epoch 292/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.3926 - val_loss: 12341.4062 - lr: 0.0100\n",
      "Epoch 293/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 10677.2715\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.9131 - val_loss: 12341.0176 - lr: 0.0100\n",
      "Epoch 294/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.9404 - val_loss: 12214.0674 - lr: 0.0070\n",
      "Epoch 295/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.4521 - val_loss: 12320.7656 - lr: 0.0070\n",
      "Epoch 296/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.0166 - val_loss: 12303.7373 - lr: 0.0070\n",
      "Epoch 297/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.3936 - val_loss: 12131.5967 - lr: 0.0070\n",
      "Epoch 298/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.2637 - val_loss: 12218.1768 - lr: 0.0070\n",
      "Epoch 299/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.4248 - val_loss: 12245.5596 - lr: 0.0070\n",
      "Epoch 300/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0303 - val_loss: 12274.8799 - lr: 0.0070\n",
      "Epoch 301/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.0527 - val_loss: 12275.0068 - lr: 0.0070\n",
      "Epoch 302/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.4404 - val_loss: 12237.8643 - lr: 0.0070\n",
      "Epoch 303/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.3994 - val_loss: 12261.7744 - lr: 0.0070\n",
      "Epoch 304/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.3438 - val_loss: 12273.7705 - lr: 0.0070\n",
      "Epoch 305/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.7061 - val_loss: 12195.4414 - lr: 0.0070\n",
      "Epoch 306/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.9141 - val_loss: 12170.6250 - lr: 0.0070\n",
      "Epoch 307/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.2275 - val_loss: 12249.8262 - lr: 0.0070\n",
      "Epoch 308/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.5967 - val_loss: 12233.1104 - lr: 0.0070\n",
      "Epoch 309/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14507.6533 - val_loss: 12265.4443 - lr: 0.0070\n",
      "Epoch 310/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14493.7988 - val_loss: 12251.0010 - lr: 0.0070\n",
      "Epoch 311/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.4258 - val_loss: 12216.9688 - lr: 0.0070\n",
      "Epoch 312/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.4189 - val_loss: 12221.9902 - lr: 0.0070\n",
      "Epoch 313/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.7061 - val_loss: 12183.9844 - lr: 0.0070\n",
      "Epoch 314/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.1221 - val_loss: 12185.2666 - lr: 0.0070\n",
      "Epoch 315/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.0547 - val_loss: 12280.2529 - lr: 0.0070\n",
      "Epoch 316/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.4609 - val_loss: 12188.0664 - lr: 0.0070\n",
      "Epoch 317/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.4590 - val_loss: 12231.9111 - lr: 0.0070\n",
      "Epoch 318/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.2031 - val_loss: 12233.3154 - lr: 0.0070\n",
      "Epoch 319/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.2295 - val_loss: 12171.1611 - lr: 0.0070\n",
      "Epoch 320/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.7393 - val_loss: 12226.9375 - lr: 0.0070\n",
      "Epoch 321/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0674 - val_loss: 12245.0439 - lr: 0.0070\n",
      "Epoch 322/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.8740 - val_loss: 12232.5850 - lr: 0.0070\n",
      "Epoch 323/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.8145 - val_loss: 12228.0410 - lr: 0.0070\n",
      "Epoch 324/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.3701 - val_loss: 12298.4951 - lr: 0.0070\n",
      "Epoch 325/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.9336 - val_loss: 12122.5996 - lr: 0.0070\n",
      "Epoch 326/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.4385 - val_loss: 12231.9268 - lr: 0.0070\n",
      "Epoch 327/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.7480 - val_loss: 12251.7646 - lr: 0.0070\n",
      "Epoch 328/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.6494 - val_loss: 12174.9150 - lr: 0.0070\n",
      "Epoch 329/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.6748 - val_loss: 12357.6875 - lr: 0.0070\n",
      "Epoch 330/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14492.1338 - val_loss: 12301.7832 - lr: 0.0070\n",
      "Epoch 331/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.4941 - val_loss: 12263.8926 - lr: 0.0070\n",
      "Epoch 332/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.8711 - val_loss: 12319.2754 - lr: 0.0070\n",
      "Epoch 333/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.5117 - val_loss: 12260.4619 - lr: 0.0070\n",
      "Epoch 334/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.3691 - val_loss: 12202.3545 - lr: 0.0070\n",
      "Epoch 335/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.8594 - val_loss: 12309.3750 - lr: 0.0070\n",
      "Epoch 336/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.1885 - val_loss: 12206.6504 - lr: 0.0070\n",
      "Epoch 337/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14453.8193 - val_loss: 12267.1924 - lr: 0.0070\n",
      "Epoch 338/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.8301 - val_loss: 12226.4844 - lr: 0.0070\n",
      "Epoch 339/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.2764 - val_loss: 12201.1201 - lr: 0.0070\n",
      "Epoch 340/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.2412 - val_loss: 12222.6074 - lr: 0.0070\n",
      "Epoch 341/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.5586 - val_loss: 12209.1299 - lr: 0.0070\n",
      "Epoch 342/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.7051 - val_loss: 12184.9561 - lr: 0.0070\n",
      "Epoch 343/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.2041 - val_loss: 12171.4531 - lr: 0.0070\n",
      "Epoch 344/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.5166 - val_loss: 12248.4531 - lr: 0.0070\n",
      "Epoch 345/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.8271 - val_loss: 12336.8438 - lr: 0.0070\n",
      "Epoch 346/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.5234 - val_loss: 12251.5605 - lr: 0.0070\n",
      "Epoch 347/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14453.6035 - val_loss: 12256.8438 - lr: 0.0070\n",
      "Epoch 348/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.3555 - val_loss: 12187.7607 - lr: 0.0070\n",
      "Epoch 349/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.0244 - val_loss: 12277.6523 - lr: 0.0070\n",
      "Epoch 350/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.4512 - val_loss: 12237.5742 - lr: 0.0070\n",
      "Epoch 351/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.9736 - val_loss: 12283.0996 - lr: 0.0070\n",
      "Epoch 352/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14453.2559 - val_loss: 12283.6445 - lr: 0.0070\n",
      "Epoch 353/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.3525 - val_loss: 12254.0996 - lr: 0.0070\n",
      "Epoch 354/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.0742 - val_loss: 12238.5898 - lr: 0.0070\n",
      "Epoch 355/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.6270 - val_loss: 12259.8447 - lr: 0.0070\n",
      "Epoch 356/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.3477 - val_loss: 12169.3457 - lr: 0.0070\n",
      "Epoch 357/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.9492 - val_loss: 12202.3271 - lr: 0.0070\n",
      "Epoch 358/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.5498 - val_loss: 12210.9453 - lr: 0.0070\n",
      "Epoch 359/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.9873 - val_loss: 12104.6943 - lr: 0.0070\n",
      "Epoch 360/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.9346 - val_loss: 12224.1396 - lr: 0.0070\n",
      "Epoch 361/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.3057 - val_loss: 12260.0557 - lr: 0.0070\n",
      "Epoch 362/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.8242 - val_loss: 12240.5566 - lr: 0.0070\n",
      "Epoch 363/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.3926 - val_loss: 12217.2012 - lr: 0.0070\n",
      "Epoch 364/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.2451 - val_loss: 12226.7734 - lr: 0.0070\n",
      "Epoch 365/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.1777 - val_loss: 12192.1670 - lr: 0.0070\n",
      "Epoch 366/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.4014 - val_loss: 12153.7715 - lr: 0.0070\n",
      "Epoch 367/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.2988 - val_loss: 12193.1543 - lr: 0.0070\n",
      "Epoch 368/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.8457 - val_loss: 12330.8486 - lr: 0.0070\n",
      "Epoch 369/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.6270 - val_loss: 12265.8965 - lr: 0.0070\n",
      "Epoch 370/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.6475 - val_loss: 12220.1152 - lr: 0.0070\n",
      "Epoch 371/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.4971 - val_loss: 12240.5010 - lr: 0.0070\n",
      "Epoch 372/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.5352 - val_loss: 12196.0908 - lr: 0.0070\n",
      "Epoch 373/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.8965 - val_loss: 12254.2891 - lr: 0.0070\n",
      "Epoch 374/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.0098 - val_loss: 12283.4209 - lr: 0.0070\n",
      "Epoch 375/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3057 - val_loss: 12209.4590 - lr: 0.0070\n",
      "Epoch 376/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.5732 - val_loss: 12183.4307 - lr: 0.0070\n",
      "Epoch 377/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.5850 - val_loss: 12178.5635 - lr: 0.0070\n",
      "Epoch 378/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.5117 - val_loss: 12305.3945 - lr: 0.0070\n",
      "Epoch 379/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14504.4336 - val_loss: 12225.9844 - lr: 0.0070\n",
      "Epoch 380/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14487.5010 - val_loss: 12218.6699 - lr: 0.0070\n",
      "Epoch 381/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.8809 - val_loss: 12193.4131 - lr: 0.0070\n",
      "Epoch 382/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.5908 - val_loss: 12345.8662 - lr: 0.0070\n",
      "Epoch 383/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.8018 - val_loss: 12143.4287 - lr: 0.0070\n",
      "Epoch 384/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.7881 - val_loss: 12197.1230 - lr: 0.0070\n",
      "Epoch 385/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.6270 - val_loss: 12214.1055 - lr: 0.0070\n",
      "Epoch 386/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.9131 - val_loss: 12213.7197 - lr: 0.0070\n",
      "Epoch 387/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.6699 - val_loss: 12221.1367 - lr: 0.0070\n",
      "Epoch 388/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.2949 - val_loss: 12264.4072 - lr: 0.0070\n",
      "Epoch 389/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.0371 - val_loss: 12389.0391 - lr: 0.0070\n",
      "Epoch 390/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.4922 - val_loss: 12219.3184 - lr: 0.0070\n",
      "Epoch 391/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.9180 - val_loss: 12286.6689 - lr: 0.0070\n",
      "Epoch 392/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.9316 - val_loss: 12252.8145 - lr: 0.0070\n",
      "Epoch 393/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.6006 - val_loss: 12264.4316 - lr: 0.0070\n",
      "Epoch 394/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.9385 - val_loss: 12209.3535 - lr: 0.0070\n",
      "Epoch 395/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.1016 - val_loss: 12195.0615 - lr: 0.0070\n",
      "Epoch 396/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.3760 - val_loss: 12212.8652 - lr: 0.0070\n",
      "Epoch 397/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.4424 - val_loss: 12200.1719 - lr: 0.0070\n",
      "Epoch 398/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.2119 - val_loss: 12274.6846 - lr: 0.0070\n",
      "Epoch 399/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.2363 - val_loss: 12269.9795 - lr: 0.0070\n",
      "Epoch 400/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.6582 - val_loss: 12214.6123 - lr: 0.0070\n",
      "Epoch 401/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14454.0918 - val_loss: 12235.9414 - lr: 0.0070\n",
      "Epoch 402/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.7891 - val_loss: 12160.5371 - lr: 0.0070\n",
      "Epoch 403/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14497.1113 - val_loss: 12237.2812 - lr: 0.0070\n",
      "Epoch 404/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.4453 - val_loss: 12170.5098 - lr: 0.0070\n",
      "Epoch 405/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.5928 - val_loss: 12179.2090 - lr: 0.0070\n",
      "Epoch 406/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.1807 - val_loss: 12238.6113 - lr: 0.0070\n",
      "Epoch 407/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.9854 - val_loss: 12277.2754 - lr: 0.0070\n",
      "Epoch 408/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.8799 - val_loss: 12276.4189 - lr: 0.0070\n",
      "Epoch 409/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.8701 - val_loss: 12203.2793 - lr: 0.0070\n",
      "Epoch 410/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.5928 - val_loss: 12211.0869 - lr: 0.0070\n",
      "Epoch 411/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.6318 - val_loss: 12160.6162 - lr: 0.0070\n",
      "Epoch 412/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.2373 - val_loss: 12264.7256 - lr: 0.0070\n",
      "Epoch 413/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.6084 - val_loss: 12207.3398 - lr: 0.0070\n",
      "Epoch 414/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.1963 - val_loss: 12234.6582 - lr: 0.0070\n",
      "Epoch 415/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.8945 - val_loss: 12198.4141 - lr: 0.0070\n",
      "Epoch 416/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.8369 - val_loss: 12212.1230 - lr: 0.0070\n",
      "Epoch 417/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.6699 - val_loss: 12200.5420 - lr: 0.0070\n",
      "Epoch 418/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.7236 - val_loss: 12344.3955 - lr: 0.0070\n",
      "Epoch 419/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.5605 - val_loss: 12150.4209 - lr: 0.0070\n",
      "Epoch 420/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.3613 - val_loss: 12249.7900 - lr: 0.0070\n",
      "Epoch 421/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.9717 - val_loss: 12248.0850 - lr: 0.0070\n",
      "Epoch 422/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.1709 - val_loss: 12222.0811 - lr: 0.0070\n",
      "Epoch 423/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.9844 - val_loss: 12240.1475 - lr: 0.0070\n",
      "Epoch 424/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.3887 - val_loss: 12194.8721 - lr: 0.0070\n",
      "Epoch 425/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14455.3848 - val_loss: 12273.9062 - lr: 0.0070\n",
      "Epoch 426/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.7861 - val_loss: 12275.5127 - lr: 0.0070\n",
      "Epoch 427/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.2979 - val_loss: 12224.3516 - lr: 0.0070\n",
      "Epoch 428/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14471.7129 - val_loss: 12123.9883 - lr: 0.0070\n",
      "Epoch 429/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.0664 - val_loss: 12204.7061 - lr: 0.0070\n",
      "Epoch 430/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.6025 - val_loss: 12186.5479 - lr: 0.0070\n",
      "Epoch 431/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.1094 - val_loss: 12185.3779 - lr: 0.0070\n",
      "Epoch 432/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.3262 - val_loss: 12242.5439 - lr: 0.0070\n",
      "Epoch 433/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.4033 - val_loss: 12212.6338 - lr: 0.0070\n",
      "Epoch 434/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.9453 - val_loss: 12190.0449 - lr: 0.0070\n",
      "Epoch 435/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.5684 - val_loss: 12232.0732 - lr: 0.0070\n",
      "Epoch 436/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14458.1104 - val_loss: 12208.9404 - lr: 0.0070\n",
      "Epoch 437/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.2285 - val_loss: 12099.0547 - lr: 0.0070\n",
      "Epoch 438/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.9883 - val_loss: 12218.9365 - lr: 0.0070\n",
      "Epoch 439/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.4463 - val_loss: 12215.8916 - lr: 0.0070\n",
      "Epoch 440/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.0020 - val_loss: 12217.2734 - lr: 0.0070\n",
      "Epoch 441/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.2266 - val_loss: 12169.0127 - lr: 0.0070\n",
      "Epoch 442/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.3398 - val_loss: 12242.7930 - lr: 0.0070\n",
      "Epoch 443/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 11889.3906\n",
      "Epoch 00443: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "Restoring model weights from the end of the best epoch: 143.\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.5186 - val_loss: 12242.9170 - lr: 0.0070\n",
      "Epoch 00443: early stopping\n",
      "run-fold: 0-0 SMAPE: 39.31370\n",
      "Fold 0.1\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuya/.pyenv/versions/3.9.7/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 189476.0156 - val_loss: 230524.2188 - lr: 0.0100\n",
      "Epoch 2/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 164511.7188 - val_loss: 184281.8750 - lr: 0.0100\n",
      "Epoch 3/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 118832.6953 - val_loss: 122359.4531 - lr: 0.0100\n",
      "Epoch 4/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 75974.0312 - val_loss: 80942.8203 - lr: 0.0100\n",
      "Epoch 5/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 54488.4648 - val_loss: 64392.1445 - lr: 0.0100\n",
      "Epoch 6/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 45010.7852 - val_loss: 54677.5859 - lr: 0.0100\n",
      "Epoch 7/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 37551.9961 - val_loss: 46567.6953 - lr: 0.0100\n",
      "Epoch 8/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 31118.0234 - val_loss: 39453.6484 - lr: 0.0100\n",
      "Epoch 9/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 25673.7188 - val_loss: 33281.5469 - lr: 0.0100\n",
      "Epoch 10/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 21348.0742 - val_loss: 28261.9023 - lr: 0.0100\n",
      "Epoch 11/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 18051.6445 - val_loss: 24579.7656 - lr: 0.0100\n",
      "Epoch 12/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15795.7559 - val_loss: 21905.3906 - lr: 0.0100\n",
      "Epoch 13/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14327.3848 - val_loss: 20083.4668 - lr: 0.0100\n",
      "Epoch 14/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 13415.8809 - val_loss: 18894.4004 - lr: 0.0100\n",
      "Epoch 15/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 12830.0352 - val_loss: 18108.7188 - lr: 0.0100\n",
      "Epoch 16/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 12455.3398 - val_loss: 17632.2285 - lr: 0.0100\n",
      "Epoch 17/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 12187.2920 - val_loss: 17265.2637 - lr: 0.0100\n",
      "Epoch 18/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11991.0293 - val_loss: 17055.4551 - lr: 0.0100\n",
      "Epoch 19/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11832.3838 - val_loss: 16836.6055 - lr: 0.0100\n",
      "Epoch 20/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11702.7803 - val_loss: 16847.9727 - lr: 0.0100\n",
      "Epoch 21/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11590.0107 - val_loss: 16699.0957 - lr: 0.0100\n",
      "Epoch 22/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11496.8574 - val_loss: 16740.1387 - lr: 0.0100\n",
      "Epoch 23/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11413.4219 - val_loss: 16708.3809 - lr: 0.0100\n",
      "Epoch 24/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11335.4277 - val_loss: 16663.7871 - lr: 0.0100\n",
      "Epoch 25/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11271.9014 - val_loss: 16717.2031 - lr: 0.0100\n",
      "Epoch 26/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11216.5742 - val_loss: 16638.1289 - lr: 0.0100\n",
      "Epoch 27/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11168.2676 - val_loss: 16722.5469 - lr: 0.0100\n",
      "Epoch 28/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11124.3789 - val_loss: 16735.5078 - lr: 0.0100\n",
      "Epoch 29/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11088.5225 - val_loss: 16772.9727 - lr: 0.0100\n",
      "Epoch 30/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11063.4307 - val_loss: 16962.5566 - lr: 0.0100\n",
      "Epoch 31/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11033.5928 - val_loss: 16839.1465 - lr: 0.0100\n",
      "Epoch 32/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11009.0322 - val_loss: 16876.8750 - lr: 0.0100\n",
      "Epoch 33/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10992.6367 - val_loss: 16869.1973 - lr: 0.0100\n",
      "Epoch 34/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10975.5039 - val_loss: 17006.9434 - lr: 0.0100\n",
      "Epoch 35/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10963.1240 - val_loss: 17041.8535 - lr: 0.0100\n",
      "Epoch 36/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10949.9238 - val_loss: 17122.7344 - lr: 0.0100\n",
      "Epoch 37/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10942.6689 - val_loss: 16989.3887 - lr: 0.0100\n",
      "Epoch 38/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10938.0791 - val_loss: 17310.7500 - lr: 0.0100\n",
      "Epoch 39/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10928.4326 - val_loss: 17186.7324 - lr: 0.0100\n",
      "Epoch 40/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10927.8896 - val_loss: 17489.0312 - lr: 0.0100\n",
      "Epoch 41/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10927.8252 - val_loss: 17268.9492 - lr: 0.0100\n",
      "Epoch 42/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.8291 - val_loss: 17334.4277 - lr: 0.0100\n",
      "Epoch 43/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.5566 - val_loss: 17674.2969 - lr: 0.0100\n",
      "Epoch 44/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.6592 - val_loss: 17077.4316 - lr: 0.0100\n",
      "Epoch 45/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10925.0137 - val_loss: 17200.4043 - lr: 0.0100\n",
      "Epoch 46/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10927.7119 - val_loss: 17654.1621 - lr: 0.0100\n",
      "Epoch 47/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.4150 - val_loss: 17294.8867 - lr: 0.0100\n",
      "Epoch 48/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.7500 - val_loss: 17376.8945 - lr: 0.0100\n",
      "Epoch 49/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.9277 - val_loss: 17634.3594 - lr: 0.0100\n",
      "Epoch 50/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.7852 - val_loss: 17607.7930 - lr: 0.0100\n",
      "Epoch 51/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.5361 - val_loss: 17560.2188 - lr: 0.0100\n",
      "Epoch 52/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.4004 - val_loss: 17671.7227 - lr: 0.0100\n",
      "Epoch 53/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.7344 - val_loss: 17609.0000 - lr: 0.0100\n",
      "Epoch 54/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.7637 - val_loss: 17585.8965 - lr: 0.0100\n",
      "Epoch 55/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.7207 - val_loss: 17596.1777 - lr: 0.0100\n",
      "Epoch 56/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.9551 - val_loss: 17554.8672 - lr: 0.0100\n",
      "Epoch 57/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.3994 - val_loss: 17533.3574 - lr: 0.0100\n",
      "Epoch 58/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.9385 - val_loss: 17704.4434 - lr: 0.0100\n",
      "Epoch 59/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.9629 - val_loss: 17732.2441 - lr: 0.0100\n",
      "Epoch 60/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.5850 - val_loss: 17972.7969 - lr: 0.0100\n",
      "Epoch 61/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.1035 - val_loss: 17500.1113 - lr: 0.0100\n",
      "Epoch 62/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.2188 - val_loss: 17410.1152 - lr: 0.0100\n",
      "Epoch 63/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.1689 - val_loss: 17786.2480 - lr: 0.0100\n",
      "Epoch 64/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10905.9375 - val_loss: 17388.3770 - lr: 0.0100\n",
      "Epoch 65/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.6416 - val_loss: 17854.8613 - lr: 0.0100\n",
      "Epoch 66/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.2773 - val_loss: 17620.9219 - lr: 0.0100\n",
      "Epoch 67/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10906.1494 - val_loss: 17816.5508 - lr: 0.0100\n",
      "Epoch 68/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.9453 - val_loss: 17906.5000 - lr: 0.0100\n",
      "Epoch 69/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.3926 - val_loss: 17678.2832 - lr: 0.0100\n",
      "Epoch 70/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10905.8008 - val_loss: 17699.4648 - lr: 0.0100\n",
      "Epoch 71/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10906.9463 - val_loss: 17590.6523 - lr: 0.0100\n",
      "Epoch 72/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.4326 - val_loss: 17824.7012 - lr: 0.0100\n",
      "Epoch 73/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.3047 - val_loss: 17725.6484 - lr: 0.0100\n",
      "Epoch 74/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.3496 - val_loss: 17558.6992 - lr: 0.0100\n",
      "Epoch 75/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.5723 - val_loss: 17437.0059 - lr: 0.0100\n",
      "Epoch 76/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.5049 - val_loss: 17427.0273 - lr: 0.0100\n",
      "Epoch 77/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.8457 - val_loss: 17619.6484 - lr: 0.0100\n",
      "Epoch 78/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.5713 - val_loss: 17650.2012 - lr: 0.0100\n",
      "Epoch 79/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.9043 - val_loss: 17691.3281 - lr: 0.0100\n",
      "Epoch 80/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.5303 - val_loss: 17738.5039 - lr: 0.0100\n",
      "Epoch 81/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.5020 - val_loss: 17553.7188 - lr: 0.0100\n",
      "Epoch 82/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.2822 - val_loss: 17962.0977 - lr: 0.0100\n",
      "Epoch 83/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.4111 - val_loss: 17852.2871 - lr: 0.0100\n",
      "Epoch 84/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.6631 - val_loss: 17845.9238 - lr: 0.0100\n",
      "Epoch 85/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.0723 - val_loss: 17511.4824 - lr: 0.0100\n",
      "Epoch 86/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.4346 - val_loss: 17502.3906 - lr: 0.0100\n",
      "Epoch 87/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.5166 - val_loss: 17412.8223 - lr: 0.0100\n",
      "Epoch 88/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.5469 - val_loss: 17345.9004 - lr: 0.0100\n",
      "Epoch 89/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10926.0098 - val_loss: 17473.5957 - lr: 0.0100\n",
      "Epoch 90/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.4404 - val_loss: 17699.2051 - lr: 0.0100\n",
      "Epoch 91/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.4600 - val_loss: 17386.0156 - lr: 0.0100\n",
      "Epoch 92/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.0166 - val_loss: 17431.2676 - lr: 0.0100\n",
      "Epoch 93/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.4980 - val_loss: 17652.1152 - lr: 0.0100\n",
      "Epoch 94/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.3799 - val_loss: 17733.2324 - lr: 0.0100\n",
      "Epoch 95/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.4521 - val_loss: 17645.3828 - lr: 0.0100\n",
      "Epoch 96/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.9395 - val_loss: 17717.5156 - lr: 0.0100\n",
      "Epoch 97/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.2422 - val_loss: 17716.2734 - lr: 0.0100\n",
      "Epoch 98/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.0430 - val_loss: 17889.0391 - lr: 0.0100\n",
      "Epoch 99/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.3262 - val_loss: 17402.7285 - lr: 0.0100\n",
      "Epoch 100/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.9824 - val_loss: 17429.2305 - lr: 0.0100\n",
      "Epoch 101/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.9600 - val_loss: 17513.2188 - lr: 0.0100\n",
      "Epoch 102/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.7188 - val_loss: 17824.8926 - lr: 0.0100\n",
      "Epoch 103/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.9102 - val_loss: 17334.4434 - lr: 0.0100\n",
      "Epoch 104/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.8389 - val_loss: 18171.9199 - lr: 0.0100\n",
      "Epoch 105/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.5264 - val_loss: 18043.1953 - lr: 0.0100\n",
      "Epoch 106/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.7998 - val_loss: 17932.3379 - lr: 0.0100\n",
      "Epoch 107/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.7129 - val_loss: 18023.8086 - lr: 0.0100\n",
      "Epoch 108/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.3086 - val_loss: 17900.0410 - lr: 0.0100\n",
      "Epoch 109/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.2305 - val_loss: 17767.4199 - lr: 0.0100\n",
      "Epoch 110/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.1123 - val_loss: 17633.5234 - lr: 0.0100\n",
      "Epoch 111/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5303 - val_loss: 17823.3320 - lr: 0.0100\n",
      "Epoch 112/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.6445 - val_loss: 17287.0527 - lr: 0.0100\n",
      "Epoch 113/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.4727 - val_loss: 17707.9297 - lr: 0.0100\n",
      "Epoch 114/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.8945 - val_loss: 17629.9316 - lr: 0.0100\n",
      "Epoch 115/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.6455 - val_loss: 17750.3203 - lr: 0.0100\n",
      "Epoch 116/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.0215 - val_loss: 17953.2285 - lr: 0.0100\n",
      "Epoch 117/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.3936 - val_loss: 17981.5547 - lr: 0.0100\n",
      "Epoch 118/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10926.1562 - val_loss: 18622.4570 - lr: 0.0100\n",
      "Epoch 119/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10927.5439 - val_loss: 17699.8711 - lr: 0.0100\n",
      "Epoch 120/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.3252 - val_loss: 17427.5234 - lr: 0.0100\n",
      "Epoch 121/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10925.1514 - val_loss: 16851.8945 - lr: 0.0100\n",
      "Epoch 122/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10930.4531 - val_loss: 17519.6328 - lr: 0.0100\n",
      "Epoch 123/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.4893 - val_loss: 17435.7500 - lr: 0.0100\n",
      "Epoch 124/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.3604 - val_loss: 17247.8867 - lr: 0.0100\n",
      "Epoch 125/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.1807 - val_loss: 17512.7070 - lr: 0.0100\n",
      "Epoch 126/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.9902 - val_loss: 17554.0938 - lr: 0.0100\n",
      "Epoch 127/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1904 - val_loss: 17830.1191 - lr: 0.0100\n",
      "Epoch 128/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.8906 - val_loss: 17655.0020 - lr: 0.0100\n",
      "Epoch 129/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.0586 - val_loss: 17555.5430 - lr: 0.0100\n",
      "Epoch 130/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.6855 - val_loss: 17527.2715 - lr: 0.0100\n",
      "Epoch 131/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.5566 - val_loss: 17130.9590 - lr: 0.0100\n",
      "Epoch 132/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.9287 - val_loss: 17363.6562 - lr: 0.0100\n",
      "Epoch 133/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10921.8750 - val_loss: 17667.6387 - lr: 0.0100\n",
      "Epoch 134/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.9297 - val_loss: 17538.8887 - lr: 0.0100\n",
      "Epoch 135/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.3818 - val_loss: 17657.2012 - lr: 0.0100\n",
      "Epoch 136/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.3213 - val_loss: 17567.6406 - lr: 0.0100\n",
      "Epoch 137/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.4570 - val_loss: 18005.3477 - lr: 0.0100\n",
      "Epoch 138/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.2373 - val_loss: 17849.0742 - lr: 0.0100\n",
      "Epoch 139/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.4111 - val_loss: 18071.6426 - lr: 0.0100\n",
      "Epoch 140/5000\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 10929.3945 - val_loss: 18119.1270 - lr: 0.0100\n",
      "Epoch 141/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.1104 - val_loss: 17546.2207 - lr: 0.0100\n",
      "Epoch 142/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.6396 - val_loss: 17665.4375 - lr: 0.0100\n",
      "Epoch 143/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.5234 - val_loss: 17677.5273 - lr: 0.0100\n",
      "Epoch 144/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.2852 - val_loss: 17542.4453 - lr: 0.0100\n",
      "Epoch 145/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.3857 - val_loss: 17689.1777 - lr: 0.0100\n",
      "Epoch 146/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.4814 - val_loss: 17523.4004 - lr: 0.0100\n",
      "Epoch 147/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10919.3369 - val_loss: 17993.7344 - lr: 0.0100\n",
      "Epoch 148/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10929.2930 - val_loss: 17574.1348 - lr: 0.0100\n",
      "Epoch 149/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10923.4453 - val_loss: 17659.9297 - lr: 0.0100\n",
      "Epoch 150/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10919.1924 - val_loss: 18101.1680 - lr: 0.0100\n",
      "Epoch 151/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.4971 - val_loss: 17445.3008 - lr: 0.0100\n",
      "Epoch 152/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.7734 - val_loss: 17254.1934 - lr: 0.0100\n",
      "Epoch 153/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.7119 - val_loss: 17866.0703 - lr: 0.0100\n",
      "Epoch 154/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.9004 - val_loss: 17798.3438 - lr: 0.0100\n",
      "Epoch 155/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10930.6777 - val_loss: 18134.7246 - lr: 0.0100\n",
      "Epoch 156/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10924.7910 - val_loss: 17466.7480 - lr: 0.0100\n",
      "Epoch 157/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10932.0576 - val_loss: 17303.2500 - lr: 0.0100\n",
      "Epoch 158/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.0967 - val_loss: 18257.1465 - lr: 0.0100\n",
      "Epoch 159/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.8203 - val_loss: 18215.4238 - lr: 0.0100\n",
      "Epoch 160/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10927.7324 - val_loss: 17667.4297 - lr: 0.0100\n",
      "Epoch 161/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10924.2852 - val_loss: 17350.5039 - lr: 0.0100\n",
      "Epoch 162/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.1660 - val_loss: 17851.5312 - lr: 0.0100\n",
      "Epoch 163/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.5234 - val_loss: 17298.7871 - lr: 0.0100\n",
      "Epoch 164/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.4131 - val_loss: 17422.3223 - lr: 0.0100\n",
      "Epoch 165/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.8525 - val_loss: 17433.3184 - lr: 0.0100\n",
      "Epoch 166/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.6006 - val_loss: 17776.9844 - lr: 0.0100\n",
      "Epoch 167/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.7393 - val_loss: 17971.1387 - lr: 0.0100\n",
      "Epoch 168/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.0186 - val_loss: 17888.3164 - lr: 0.0100\n",
      "Epoch 169/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.5322 - val_loss: 17802.4941 - lr: 0.0100\n",
      "Epoch 170/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10931.9121 - val_loss: 17471.7031 - lr: 0.0100\n",
      "Epoch 171/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10953.8652 - val_loss: 17168.0020 - lr: 0.0100\n",
      "Epoch 172/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.9746 - val_loss: 18038.6406 - lr: 0.0100\n",
      "Epoch 173/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10939.0947 - val_loss: 18422.2559 - lr: 0.0100\n",
      "Epoch 174/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.7158 - val_loss: 17852.6484 - lr: 0.0100\n",
      "Epoch 175/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.5596 - val_loss: 17587.1680 - lr: 0.0100\n",
      "Epoch 176/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 10594.3086\n",
      "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.8750 - val_loss: 17760.7168 - lr: 0.0100\n",
      "Epoch 177/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.5303 - val_loss: 17710.4980 - lr: 0.0070\n",
      "Epoch 178/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.9082 - val_loss: 17467.4746 - lr: 0.0070\n",
      "Epoch 179/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.8477 - val_loss: 17469.9297 - lr: 0.0070\n",
      "Epoch 180/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.8525 - val_loss: 17878.0059 - lr: 0.0070\n",
      "Epoch 181/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.8379 - val_loss: 17650.0391 - lr: 0.0070\n",
      "Epoch 182/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.5215 - val_loss: 17647.6270 - lr: 0.0070\n",
      "Epoch 183/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.0332 - val_loss: 17538.2188 - lr: 0.0070\n",
      "Epoch 184/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.8379 - val_loss: 17518.4551 - lr: 0.0070\n",
      "Epoch 185/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.3701 - val_loss: 17298.9102 - lr: 0.0070\n",
      "Epoch 186/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.6494 - val_loss: 17400.6465 - lr: 0.0070\n",
      "Epoch 187/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.6455 - val_loss: 17674.8027 - lr: 0.0070\n",
      "Epoch 188/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.8076 - val_loss: 17608.1641 - lr: 0.0070\n",
      "Epoch 189/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.7246 - val_loss: 17721.5508 - lr: 0.0070\n",
      "Epoch 190/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.0459 - val_loss: 17795.6562 - lr: 0.0070\n",
      "Epoch 191/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.7422 - val_loss: 17606.8906 - lr: 0.0070\n",
      "Epoch 192/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.6982 - val_loss: 18116.3477 - lr: 0.0070\n",
      "Epoch 193/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.2822 - val_loss: 17360.7285 - lr: 0.0070\n",
      "Epoch 194/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.4043 - val_loss: 17709.3398 - lr: 0.0070\n",
      "Epoch 195/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.7598 - val_loss: 17768.4570 - lr: 0.0070\n",
      "Epoch 196/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.7461 - val_loss: 17235.2285 - lr: 0.0070\n",
      "Epoch 197/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10932.0537 - val_loss: 17325.9551 - lr: 0.0070\n",
      "Epoch 198/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.1035 - val_loss: 17505.8164 - lr: 0.0070\n",
      "Epoch 199/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10906.5449 - val_loss: 18136.7578 - lr: 0.0070\n",
      "Epoch 200/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.7559 - val_loss: 17785.3535 - lr: 0.0070\n",
      "Epoch 201/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.9434 - val_loss: 17715.1680 - lr: 0.0070\n",
      "Epoch 202/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.7617 - val_loss: 17680.5703 - lr: 0.0070\n",
      "Epoch 203/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.7588 - val_loss: 17430.1543 - lr: 0.0070\n",
      "Epoch 204/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.4014 - val_loss: 17553.6309 - lr: 0.0070\n",
      "Epoch 205/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.6689 - val_loss: 17322.1094 - lr: 0.0070\n",
      "Epoch 206/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.2012 - val_loss: 17918.9395 - lr: 0.0070\n",
      "Epoch 207/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.6689 - val_loss: 18106.4824 - lr: 0.0070\n",
      "Epoch 208/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10926.2158 - val_loss: 17889.2910 - lr: 0.0070\n",
      "Epoch 209/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.8350 - val_loss: 17731.7402 - lr: 0.0070\n",
      "Epoch 210/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.7236 - val_loss: 17327.6953 - lr: 0.0070\n",
      "Epoch 211/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10923.6289 - val_loss: 17951.6445 - lr: 0.0070\n",
      "Epoch 212/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.5605 - val_loss: 18186.9023 - lr: 0.0070\n",
      "Epoch 213/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.6670 - val_loss: 17933.9219 - lr: 0.0070\n",
      "Epoch 214/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.8838 - val_loss: 17785.1289 - lr: 0.0070\n",
      "Epoch 215/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.2178 - val_loss: 17896.7480 - lr: 0.0070\n",
      "Epoch 216/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.7900 - val_loss: 17338.4141 - lr: 0.0070\n",
      "Epoch 217/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.8818 - val_loss: 17716.7656 - lr: 0.0070\n",
      "Epoch 218/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.0820 - val_loss: 18325.8516 - lr: 0.0070\n",
      "Epoch 219/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.6211 - val_loss: 17514.9863 - lr: 0.0070\n",
      "Epoch 220/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.4316 - val_loss: 17085.8770 - lr: 0.0070\n",
      "Epoch 221/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10921.4277 - val_loss: 17408.9648 - lr: 0.0070\n",
      "Epoch 222/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.2051 - val_loss: 17839.8477 - lr: 0.0070\n",
      "Epoch 223/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.2686 - val_loss: 17951.4004 - lr: 0.0070\n",
      "Epoch 224/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.9551 - val_loss: 17789.5273 - lr: 0.0070\n",
      "Epoch 225/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.7432 - val_loss: 17851.0215 - lr: 0.0070\n",
      "Epoch 226/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.8828 - val_loss: 18014.0137 - lr: 0.0070\n",
      "Epoch 227/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.2490 - val_loss: 17555.4414 - lr: 0.0070\n",
      "Epoch 228/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.9980 - val_loss: 17827.1387 - lr: 0.0070\n",
      "Epoch 229/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.0967 - val_loss: 17944.7617 - lr: 0.0070\n",
      "Epoch 230/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.7100 - val_loss: 17786.5527 - lr: 0.0070\n",
      "Epoch 231/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10923.4229 - val_loss: 17307.7168 - lr: 0.0070\n",
      "Epoch 232/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10947.0107 - val_loss: 17302.2734 - lr: 0.0070\n",
      "Epoch 233/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10927.7012 - val_loss: 17606.6172 - lr: 0.0070\n",
      "Epoch 234/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5186 - val_loss: 17703.6289 - lr: 0.0070\n",
      "Epoch 235/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.0469 - val_loss: 17583.1309 - lr: 0.0070\n",
      "Epoch 236/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.0996 - val_loss: 17541.5059 - lr: 0.0070\n",
      "Epoch 237/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.4658 - val_loss: 17975.7559 - lr: 0.0070\n",
      "Epoch 238/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.2129 - val_loss: 17960.9062 - lr: 0.0070\n",
      "Epoch 239/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.8350 - val_loss: 17684.2051 - lr: 0.0070\n",
      "Epoch 240/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.7373 - val_loss: 18069.5703 - lr: 0.0070\n",
      "Epoch 241/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.1621 - val_loss: 18168.6016 - lr: 0.0070\n",
      "Epoch 242/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10923.1562 - val_loss: 17766.2578 - lr: 0.0070\n",
      "Epoch 243/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.9072 - val_loss: 17626.0352 - lr: 0.0070\n",
      "Epoch 244/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.2666 - val_loss: 17394.1465 - lr: 0.0070\n",
      "Epoch 245/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.0684 - val_loss: 17694.3496 - lr: 0.0070\n",
      "Epoch 246/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.6719 - val_loss: 17373.8809 - lr: 0.0070\n",
      "Epoch 247/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.5430 - val_loss: 17605.1211 - lr: 0.0070\n",
      "Epoch 248/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.4902 - val_loss: 17590.1973 - lr: 0.0070\n",
      "Epoch 249/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.4326 - val_loss: 17828.0410 - lr: 0.0070\n",
      "Epoch 250/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.8154 - val_loss: 17300.4180 - lr: 0.0070\n",
      "Epoch 251/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.6006 - val_loss: 17334.4043 - lr: 0.0070\n",
      "Epoch 252/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.4404 - val_loss: 17732.8555 - lr: 0.0070\n",
      "Epoch 253/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10933.6875 - val_loss: 18267.4062 - lr: 0.0070\n",
      "Epoch 254/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.7871 - val_loss: 17637.5957 - lr: 0.0070\n",
      "Epoch 255/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.1904 - val_loss: 17858.4512 - lr: 0.0070\n",
      "Epoch 256/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.6611 - val_loss: 17532.6445 - lr: 0.0070\n",
      "Epoch 257/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.2070 - val_loss: 17524.5254 - lr: 0.0070\n",
      "Epoch 258/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.8828 - val_loss: 17236.2109 - lr: 0.0070\n",
      "Epoch 259/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.2705 - val_loss: 17454.2520 - lr: 0.0070\n",
      "Epoch 260/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.3750 - val_loss: 17830.7656 - lr: 0.0070\n",
      "Epoch 261/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10919.4922 - val_loss: 17625.2578 - lr: 0.0070\n",
      "Epoch 262/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.6055 - val_loss: 17790.0742 - lr: 0.0070\n",
      "Epoch 263/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.1143 - val_loss: 17939.1387 - lr: 0.0070\n",
      "Epoch 264/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.8838 - val_loss: 18037.1523 - lr: 0.0070\n",
      "Epoch 265/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.7920 - val_loss: 17324.3008 - lr: 0.0070\n",
      "Epoch 266/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10931.1592 - val_loss: 17438.3301 - lr: 0.0070\n",
      "Epoch 267/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.4883 - val_loss: 17947.8242 - lr: 0.0070\n",
      "Epoch 268/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.2217 - val_loss: 18265.4180 - lr: 0.0070\n",
      "Epoch 269/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.2021 - val_loss: 17732.6230 - lr: 0.0070\n",
      "Epoch 270/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5068 - val_loss: 17746.1094 - lr: 0.0070\n",
      "Epoch 271/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1631 - val_loss: 17327.7598 - lr: 0.0070\n",
      "Epoch 272/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.0371 - val_loss: 17599.3770 - lr: 0.0070\n",
      "Epoch 273/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.0225 - val_loss: 18060.2949 - lr: 0.0070\n",
      "Epoch 274/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.3447 - val_loss: 17756.5391 - lr: 0.0070\n",
      "Epoch 275/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.2471 - val_loss: 17972.7695 - lr: 0.0070\n",
      "Epoch 276/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.4316 - val_loss: 17551.7559 - lr: 0.0070\n",
      "Epoch 277/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.6592 - val_loss: 18046.1816 - lr: 0.0070\n",
      "Epoch 278/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.7217 - val_loss: 17965.1621 - lr: 0.0070\n",
      "Epoch 279/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.1133 - val_loss: 17617.3066 - lr: 0.0070\n",
      "Epoch 280/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.5791 - val_loss: 17935.1855 - lr: 0.0070\n",
      "Epoch 281/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.4141 - val_loss: 17947.6914 - lr: 0.0070\n",
      "Epoch 282/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.0996 - val_loss: 17889.7988 - lr: 0.0070\n",
      "Epoch 283/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.9307 - val_loss: 17700.9961 - lr: 0.0070\n",
      "Epoch 284/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10929.0410 - val_loss: 17594.7793 - lr: 0.0070\n",
      "Epoch 285/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.6348 - val_loss: 17623.1152 - lr: 0.0070\n",
      "Epoch 286/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.0342 - val_loss: 17273.7148 - lr: 0.0070\n",
      "Epoch 287/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.5342 - val_loss: 17829.9453 - lr: 0.0070\n",
      "Epoch 288/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.1191 - val_loss: 17697.3906 - lr: 0.0070\n",
      "Epoch 289/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.8232 - val_loss: 17513.5059 - lr: 0.0070\n",
      "Epoch 290/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.6865 - val_loss: 17639.9590 - lr: 0.0070\n",
      "Epoch 291/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.8125 - val_loss: 18008.5469 - lr: 0.0070\n",
      "Epoch 292/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.3691 - val_loss: 17612.4688 - lr: 0.0070\n",
      "Epoch 293/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.2607 - val_loss: 17468.4766 - lr: 0.0070\n",
      "Epoch 294/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10905.8242 - val_loss: 17952.1191 - lr: 0.0070\n",
      "Epoch 295/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.5381 - val_loss: 17903.0566 - lr: 0.0070\n",
      "Epoch 296/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.1719 - val_loss: 18028.0430 - lr: 0.0070\n",
      "Epoch 297/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.8936 - val_loss: 18130.6230 - lr: 0.0070\n",
      "Epoch 298/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.9473 - val_loss: 17553.3945 - lr: 0.0070\n",
      "Epoch 299/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.7852 - val_loss: 17679.8926 - lr: 0.0070\n",
      "Epoch 300/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.0850 - val_loss: 17527.4551 - lr: 0.0070\n",
      "Epoch 301/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10924.3135 - val_loss: 17805.4531 - lr: 0.0070\n",
      "Epoch 302/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.7432 - val_loss: 17763.3574 - lr: 0.0070\n",
      "Epoch 303/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.7393 - val_loss: 17761.6914 - lr: 0.0070\n",
      "Epoch 304/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.3145 - val_loss: 17566.2246 - lr: 0.0070\n",
      "Epoch 305/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.8643 - val_loss: 17936.9062 - lr: 0.0070\n",
      "Epoch 306/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.6406 - val_loss: 17390.9746 - lr: 0.0070\n",
      "Epoch 307/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.1953 - val_loss: 18420.6465 - lr: 0.0070\n",
      "Epoch 308/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.6855 - val_loss: 17961.9414 - lr: 0.0070\n",
      "Epoch 309/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.0654 - val_loss: 17710.6934 - lr: 0.0070\n",
      "Epoch 310/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.5215 - val_loss: 17967.7949 - lr: 0.0070\n",
      "Epoch 311/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.1992 - val_loss: 17946.1094 - lr: 0.0070\n",
      "Epoch 312/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.7754 - val_loss: 17660.9648 - lr: 0.0070\n",
      "Epoch 313/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.0693 - val_loss: 17435.4844 - lr: 0.0070\n",
      "Epoch 314/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.0879 - val_loss: 17785.0039 - lr: 0.0070\n",
      "Epoch 315/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.5889 - val_loss: 17765.2500 - lr: 0.0070\n",
      "Epoch 316/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10919.0488 - val_loss: 18092.7715 - lr: 0.0070\n",
      "Epoch 317/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.6904 - val_loss: 17731.3535 - lr: 0.0070\n",
      "Epoch 318/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.8408 - val_loss: 17782.5215 - lr: 0.0070\n",
      "Epoch 319/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.1592 - val_loss: 17864.4746 - lr: 0.0070\n",
      "Epoch 320/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.0781 - val_loss: 17879.4297 - lr: 0.0070\n",
      "Epoch 321/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10923.3145 - val_loss: 17627.7852 - lr: 0.0070\n",
      "Epoch 322/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.1533 - val_loss: 16965.8672 - lr: 0.0070\n",
      "Epoch 323/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10930.1377 - val_loss: 17494.8984 - lr: 0.0070\n",
      "Epoch 324/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.3350 - val_loss: 17954.6777 - lr: 0.0070\n",
      "Epoch 325/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.0342 - val_loss: 17398.2051 - lr: 0.0070\n",
      "Epoch 326/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 9408.7129\n",
      "Epoch 00326: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.3848 - val_loss: 17656.3516 - lr: 0.0070\n",
      "Epoch 00326: early stopping\n",
      "run-fold: 0-1 SMAPE: 29.48242\n",
      "Fold 1.0\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuya/.pyenv/versions/3.9.7/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 239871.0781 - val_loss: 178388.1719 - lr: 0.0100\n",
      "Epoch 2/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 207345.7031 - val_loss: 139608.5312 - lr: 0.0100\n",
      "Epoch 3/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 150795.3594 - val_loss: 90978.6328 - lr: 0.0100\n",
      "Epoch 4/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 97720.6719 - val_loss: 60634.0430 - lr: 0.0100\n",
      "Epoch 5/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 71125.0234 - val_loss: 49042.0977 - lr: 0.0100\n",
      "Epoch 6/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 59461.2734 - val_loss: 41301.7578 - lr: 0.0100\n",
      "Epoch 7/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 50688.9492 - val_loss: 34234.6484 - lr: 0.0100\n",
      "Epoch 8/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 42855.7383 - val_loss: 28151.9941 - lr: 0.0100\n",
      "Epoch 9/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 35950.6406 - val_loss: 23175.3496 - lr: 0.0100\n",
      "Epoch 10/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 30213.5078 - val_loss: 19213.7695 - lr: 0.0100\n",
      "Epoch 11/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 25567.4785 - val_loss: 16417.6074 - lr: 0.0100\n",
      "Epoch 12/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 22135.6426 - val_loss: 14647.3018 - lr: 0.0100\n",
      "Epoch 13/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 19744.0918 - val_loss: 13662.1602 - lr: 0.0100\n",
      "Epoch 14/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 18166.7402 - val_loss: 13180.7354 - lr: 0.0100\n",
      "Epoch 15/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 17171.6172 - val_loss: 12970.0869 - lr: 0.0100\n",
      "Epoch 16/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 16540.7539 - val_loss: 12906.2520 - lr: 0.0100\n",
      "Epoch 17/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 16118.6055 - val_loss: 12845.4492 - lr: 0.0100\n",
      "Epoch 18/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15825.2988 - val_loss: 12795.3545 - lr: 0.0100\n",
      "Epoch 19/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15614.5879 - val_loss: 12788.9248 - lr: 0.0100\n",
      "Epoch 20/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15443.7588 - val_loss: 12730.3740 - lr: 0.0100\n",
      "Epoch 21/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15308.9258 - val_loss: 12672.3838 - lr: 0.0100\n",
      "Epoch 22/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15187.1289 - val_loss: 12633.6768 - lr: 0.0100\n",
      "Epoch 23/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15084.1045 - val_loss: 12579.4043 - lr: 0.0100\n",
      "Epoch 24/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14996.4639 - val_loss: 12557.7617 - lr: 0.0100\n",
      "Epoch 25/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14918.5068 - val_loss: 12520.4971 - lr: 0.0100\n",
      "Epoch 26/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14860.4434 - val_loss: 12494.7549 - lr: 0.0100\n",
      "Epoch 27/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14796.3105 - val_loss: 12454.0625 - lr: 0.0100\n",
      "Epoch 28/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14740.3848 - val_loss: 12406.6465 - lr: 0.0100\n",
      "Epoch 29/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14697.2178 - val_loss: 12384.8223 - lr: 0.0100\n",
      "Epoch 30/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14665.3105 - val_loss: 12386.6182 - lr: 0.0100\n",
      "Epoch 31/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14624.5762 - val_loss: 12370.7979 - lr: 0.0100\n",
      "Epoch 32/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14599.6064 - val_loss: 12301.3506 - lr: 0.0100\n",
      "Epoch 33/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14581.3711 - val_loss: 12372.8721 - lr: 0.0100\n",
      "Epoch 34/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14562.2041 - val_loss: 12383.4248 - lr: 0.0100\n",
      "Epoch 35/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14542.5986 - val_loss: 12302.7783 - lr: 0.0100\n",
      "Epoch 36/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14522.7266 - val_loss: 12319.2012 - lr: 0.0100\n",
      "Epoch 37/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14516.4580 - val_loss: 12325.3115 - lr: 0.0100\n",
      "Epoch 38/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14498.0566 - val_loss: 12318.3271 - lr: 0.0100\n",
      "Epoch 39/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14491.4180 - val_loss: 12345.9482 - lr: 0.0100\n",
      "Epoch 40/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14488.2559 - val_loss: 12369.0459 - lr: 0.0100\n",
      "Epoch 41/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.2754 - val_loss: 12311.6172 - lr: 0.0100\n",
      "Epoch 42/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.0264 - val_loss: 12366.3379 - lr: 0.0100\n",
      "Epoch 43/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.2295 - val_loss: 12249.4336 - lr: 0.0100\n",
      "Epoch 44/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.1191 - val_loss: 12334.3301 - lr: 0.0100\n",
      "Epoch 45/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.2070 - val_loss: 12279.1260 - lr: 0.0100\n",
      "Epoch 46/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.7256 - val_loss: 12365.1689 - lr: 0.0100\n",
      "Epoch 47/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.1309 - val_loss: 12362.0420 - lr: 0.0100\n",
      "Epoch 48/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.4189 - val_loss: 12230.4414 - lr: 0.0100\n",
      "Epoch 49/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.9092 - val_loss: 12311.1553 - lr: 0.0100\n",
      "Epoch 50/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.3398 - val_loss: 12362.4766 - lr: 0.0100\n",
      "Epoch 51/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.5312 - val_loss: 12296.9883 - lr: 0.0100\n",
      "Epoch 52/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.2461 - val_loss: 12374.1016 - lr: 0.0100\n",
      "Epoch 53/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.0742 - val_loss: 12267.7031 - lr: 0.0100\n",
      "Epoch 54/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.5518 - val_loss: 12285.5381 - lr: 0.0100\n",
      "Epoch 55/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7168 - val_loss: 12275.8682 - lr: 0.0100\n",
      "Epoch 56/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.9697 - val_loss: 12342.0498 - lr: 0.0100\n",
      "Epoch 57/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14453.7305 - val_loss: 12257.7393 - lr: 0.0100\n",
      "Epoch 58/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.6279 - val_loss: 12340.0264 - lr: 0.0100\n",
      "Epoch 59/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.4307 - val_loss: 12272.5127 - lr: 0.0100\n",
      "Epoch 60/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.9785 - val_loss: 12379.4307 - lr: 0.0100\n",
      "Epoch 61/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.4805 - val_loss: 12358.1211 - lr: 0.0100\n",
      "Epoch 62/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.0645 - val_loss: 12219.2783 - lr: 0.0100\n",
      "Epoch 63/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.0322 - val_loss: 12279.5127 - lr: 0.0100\n",
      "Epoch 64/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.3135 - val_loss: 12353.5059 - lr: 0.0100\n",
      "Epoch 65/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.2432 - val_loss: 12279.4082 - lr: 0.0100\n",
      "Epoch 66/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.2148 - val_loss: 12354.2285 - lr: 0.0100\n",
      "Epoch 67/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.5537 - val_loss: 12249.8340 - lr: 0.0100\n",
      "Epoch 68/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.4014 - val_loss: 12241.8467 - lr: 0.0100\n",
      "Epoch 69/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.3887 - val_loss: 12390.0840 - lr: 0.0100\n",
      "Epoch 70/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.4014 - val_loss: 12322.9668 - lr: 0.0100\n",
      "Epoch 71/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.9717 - val_loss: 12234.2646 - lr: 0.0100\n",
      "Epoch 72/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.0566 - val_loss: 12293.5713 - lr: 0.0100\n",
      "Epoch 73/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.2646 - val_loss: 12213.1035 - lr: 0.0100\n",
      "Epoch 74/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.1680 - val_loss: 12363.0146 - lr: 0.0100\n",
      "Epoch 75/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14454.4131 - val_loss: 12331.8555 - lr: 0.0100\n",
      "Epoch 76/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.9756 - val_loss: 12290.1338 - lr: 0.0100\n",
      "Epoch 77/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.9404 - val_loss: 12270.8486 - lr: 0.0100\n",
      "Epoch 78/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.2246 - val_loss: 12301.0176 - lr: 0.0100\n",
      "Epoch 79/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.9736 - val_loss: 12212.5381 - lr: 0.0100\n",
      "Epoch 80/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.3027 - val_loss: 12238.4521 - lr: 0.0100\n",
      "Epoch 81/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.4736 - val_loss: 12246.2051 - lr: 0.0100\n",
      "Epoch 82/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.9219 - val_loss: 12236.6885 - lr: 0.0100\n",
      "Epoch 83/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.5654 - val_loss: 12176.4268 - lr: 0.0100\n",
      "Epoch 84/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.9746 - val_loss: 12242.6641 - lr: 0.0100\n",
      "Epoch 85/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.7998 - val_loss: 12259.5811 - lr: 0.0100\n",
      "Epoch 86/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.8125 - val_loss: 12299.4355 - lr: 0.0100\n",
      "Epoch 87/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5645 - val_loss: 12264.5654 - lr: 0.0100\n",
      "Epoch 88/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.0049 - val_loss: 12176.9951 - lr: 0.0100\n",
      "Epoch 89/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.1445 - val_loss: 12304.8877 - lr: 0.0100\n",
      "Epoch 90/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.5293 - val_loss: 12201.2979 - lr: 0.0100\n",
      "Epoch 91/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14453.2715 - val_loss: 12200.3730 - lr: 0.0100\n",
      "Epoch 92/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.7051 - val_loss: 12148.6914 - lr: 0.0100\n",
      "Epoch 93/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.0752 - val_loss: 12297.5127 - lr: 0.0100\n",
      "Epoch 94/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.4883 - val_loss: 12304.7949 - lr: 0.0100\n",
      "Epoch 95/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.4658 - val_loss: 12321.4697 - lr: 0.0100\n",
      "Epoch 96/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0850 - val_loss: 12249.3047 - lr: 0.0100\n",
      "Epoch 97/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8643 - val_loss: 12257.3838 - lr: 0.0100\n",
      "Epoch 98/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.2812 - val_loss: 12305.1865 - lr: 0.0100\n",
      "Epoch 99/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.2402 - val_loss: 12176.0791 - lr: 0.0100\n",
      "Epoch 100/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.9248 - val_loss: 12285.3408 - lr: 0.0100\n",
      "Epoch 101/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14482.8213 - val_loss: 12407.1328 - lr: 0.0100\n",
      "Epoch 102/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.2588 - val_loss: 12156.0820 - lr: 0.0100\n",
      "Epoch 103/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.0732 - val_loss: 12309.7031 - lr: 0.0100\n",
      "Epoch 104/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.6836 - val_loss: 12172.4551 - lr: 0.0100\n",
      "Epoch 105/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.3896 - val_loss: 12157.6455 - lr: 0.0100\n",
      "Epoch 106/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.9307 - val_loss: 12234.8154 - lr: 0.0100\n",
      "Epoch 107/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.1016 - val_loss: 12269.2773 - lr: 0.0100\n",
      "Epoch 108/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.4150 - val_loss: 12177.0449 - lr: 0.0100\n",
      "Epoch 109/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.5186 - val_loss: 12254.4443 - lr: 0.0100\n",
      "Epoch 110/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.5010 - val_loss: 12217.8311 - lr: 0.0100\n",
      "Epoch 111/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6504 - val_loss: 12314.1670 - lr: 0.0100\n",
      "Epoch 112/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.7266 - val_loss: 12315.2734 - lr: 0.0100\n",
      "Epoch 113/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.3926 - val_loss: 12220.3799 - lr: 0.0100\n",
      "Epoch 114/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.5703 - val_loss: 12289.1816 - lr: 0.0100\n",
      "Epoch 115/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.4775 - val_loss: 12177.2861 - lr: 0.0100\n",
      "Epoch 116/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.2393 - val_loss: 12284.6914 - lr: 0.0100\n",
      "Epoch 117/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.7236 - val_loss: 12394.0449 - lr: 0.0100\n",
      "Epoch 118/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.2930 - val_loss: 12256.5488 - lr: 0.0100\n",
      "Epoch 119/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.5840 - val_loss: 12146.4570 - lr: 0.0100\n",
      "Epoch 120/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.9717 - val_loss: 12259.9629 - lr: 0.0100\n",
      "Epoch 121/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.4355 - val_loss: 12274.4863 - lr: 0.0100\n",
      "Epoch 122/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.0527 - val_loss: 12175.5479 - lr: 0.0100\n",
      "Epoch 123/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14484.5068 - val_loss: 12208.6699 - lr: 0.0100\n",
      "Epoch 124/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.4229 - val_loss: 12135.6865 - lr: 0.0100\n",
      "Epoch 125/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14486.6055 - val_loss: 12295.9512 - lr: 0.0100\n",
      "Epoch 126/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.5918 - val_loss: 12270.3154 - lr: 0.0100\n",
      "Epoch 127/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.3389 - val_loss: 12384.4355 - lr: 0.0100\n",
      "Epoch 128/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.3896 - val_loss: 12178.5908 - lr: 0.0100\n",
      "Epoch 129/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.2725 - val_loss: 12355.2227 - lr: 0.0100\n",
      "Epoch 130/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.4277 - val_loss: 12180.1348 - lr: 0.0100\n",
      "Epoch 131/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.7080 - val_loss: 12419.0107 - lr: 0.0100\n",
      "Epoch 132/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14483.6504 - val_loss: 12481.4961 - lr: 0.0100\n",
      "Epoch 133/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14513.8730 - val_loss: 12266.9355 - lr: 0.0100\n",
      "Epoch 134/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.6914 - val_loss: 12200.5557 - lr: 0.0100\n",
      "Epoch 135/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.0898 - val_loss: 12241.2764 - lr: 0.0100\n",
      "Epoch 136/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.8486 - val_loss: 12226.7422 - lr: 0.0100\n",
      "Epoch 137/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.9316 - val_loss: 12156.1182 - lr: 0.0100\n",
      "Epoch 138/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.4961 - val_loss: 12230.5117 - lr: 0.0100\n",
      "Epoch 139/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.9912 - val_loss: 12257.7520 - lr: 0.0100\n",
      "Epoch 140/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.5381 - val_loss: 12180.5186 - lr: 0.0100\n",
      "Epoch 141/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.9414 - val_loss: 12143.3389 - lr: 0.0100\n",
      "Epoch 142/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.2061 - val_loss: 12248.2881 - lr: 0.0100\n",
      "Epoch 143/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.9727 - val_loss: 12187.6572 - lr: 0.0100\n",
      "Epoch 144/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.1387 - val_loss: 12122.3457 - lr: 0.0100\n",
      "Epoch 145/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14540.8193 - val_loss: 12229.5264 - lr: 0.0100\n",
      "Epoch 146/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.4668 - val_loss: 12279.8877 - lr: 0.0100\n",
      "Epoch 147/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.5195 - val_loss: 12228.3232 - lr: 0.0100\n",
      "Epoch 148/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.0010 - val_loss: 12224.8994 - lr: 0.0100\n",
      "Epoch 149/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.5332 - val_loss: 12200.8193 - lr: 0.0100\n",
      "Epoch 150/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0576 - val_loss: 12223.4990 - lr: 0.0100\n",
      "Epoch 151/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.9609 - val_loss: 12163.7363 - lr: 0.0100\n",
      "Epoch 152/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.1650 - val_loss: 12190.1396 - lr: 0.0100\n",
      "Epoch 153/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.7109 - val_loss: 12129.7822 - lr: 0.0100\n",
      "Epoch 154/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.8594 - val_loss: 12258.8037 - lr: 0.0100\n",
      "Epoch 155/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.2148 - val_loss: 12285.2764 - lr: 0.0100\n",
      "Epoch 156/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.3076 - val_loss: 12127.4990 - lr: 0.0100\n",
      "Epoch 157/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.4971 - val_loss: 12242.8125 - lr: 0.0100\n",
      "Epoch 158/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.3467 - val_loss: 12226.1445 - lr: 0.0100\n",
      "Epoch 159/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5947 - val_loss: 12221.6553 - lr: 0.0100\n",
      "Epoch 160/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.0322 - val_loss: 12111.7070 - lr: 0.0100\n",
      "Epoch 161/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.3281 - val_loss: 12212.8906 - lr: 0.0100\n",
      "Epoch 162/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14491.2539 - val_loss: 12265.5674 - lr: 0.0100\n",
      "Epoch 163/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.1338 - val_loss: 12139.3740 - lr: 0.0100\n",
      "Epoch 164/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.7373 - val_loss: 12273.8350 - lr: 0.0100\n",
      "Epoch 165/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.6387 - val_loss: 12292.0928 - lr: 0.0100\n",
      "Epoch 166/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.4531 - val_loss: 12207.8828 - lr: 0.0100\n",
      "Epoch 167/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14495.8672 - val_loss: 12463.1025 - lr: 0.0100\n",
      "Epoch 168/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.2178 - val_loss: 12265.3174 - lr: 0.0100\n",
      "Epoch 169/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.3047 - val_loss: 12200.1982 - lr: 0.0100\n",
      "Epoch 170/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.2617 - val_loss: 12236.5498 - lr: 0.0100\n",
      "Epoch 171/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.5068 - val_loss: 12147.9766 - lr: 0.0100\n",
      "Epoch 172/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.3184 - val_loss: 12233.1543 - lr: 0.0100\n",
      "Epoch 173/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.7773 - val_loss: 12234.8223 - lr: 0.0100\n",
      "Epoch 174/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.1719 - val_loss: 12157.4150 - lr: 0.0100\n",
      "Epoch 175/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.7705 - val_loss: 12131.6270 - lr: 0.0100\n",
      "Epoch 176/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14485.7021 - val_loss: 12295.1914 - lr: 0.0100\n",
      "Epoch 177/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.5596 - val_loss: 12151.0166 - lr: 0.0100\n",
      "Epoch 178/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14493.8008 - val_loss: 12312.4014 - lr: 0.0100\n",
      "Epoch 179/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.5518 - val_loss: 12225.8955 - lr: 0.0100\n",
      "Epoch 180/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.1387 - val_loss: 12194.0664 - lr: 0.0100\n",
      "Epoch 181/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.4941 - val_loss: 12295.8896 - lr: 0.0100\n",
      "Epoch 182/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.3350 - val_loss: 12243.5566 - lr: 0.0100\n",
      "Epoch 183/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7617 - val_loss: 12435.6758 - lr: 0.0100\n",
      "Epoch 184/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.5889 - val_loss: 12199.1904 - lr: 0.0100\n",
      "Epoch 185/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.4277 - val_loss: 12228.2402 - lr: 0.0100\n",
      "Epoch 186/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14490.3926 - val_loss: 12257.0137 - lr: 0.0100\n",
      "Epoch 187/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.9785 - val_loss: 12302.9785 - lr: 0.0100\n",
      "Epoch 188/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.0195 - val_loss: 12246.4365 - lr: 0.0100\n",
      "Epoch 189/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2471 - val_loss: 12308.8916 - lr: 0.0100\n",
      "Epoch 190/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.6504 - val_loss: 12251.6123 - lr: 0.0100\n",
      "Epoch 191/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.2559 - val_loss: 12138.1328 - lr: 0.0100\n",
      "Epoch 192/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14494.5352 - val_loss: 12248.3320 - lr: 0.0100\n",
      "Epoch 193/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.4229 - val_loss: 12112.3877 - lr: 0.0100\n",
      "Epoch 194/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.1299 - val_loss: 12279.5039 - lr: 0.0100\n",
      "Epoch 195/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.4014 - val_loss: 12256.5762 - lr: 0.0100\n",
      "Epoch 196/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.8447 - val_loss: 12166.6650 - lr: 0.0100\n",
      "Epoch 197/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.0898 - val_loss: 12335.9609 - lr: 0.0100\n",
      "Epoch 198/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.9746 - val_loss: 12299.4766 - lr: 0.0100\n",
      "Epoch 199/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.7432 - val_loss: 12260.5186 - lr: 0.0100\n",
      "Epoch 200/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.2461 - val_loss: 12202.9404 - lr: 0.0100\n",
      "Epoch 201/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.0605 - val_loss: 12152.0068 - lr: 0.0100\n",
      "Epoch 202/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14490.5693 - val_loss: 12278.5654 - lr: 0.0100\n",
      "Epoch 203/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.4141 - val_loss: 12260.2363 - lr: 0.0100\n",
      "Epoch 204/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.2588 - val_loss: 12313.2646 - lr: 0.0100\n",
      "Epoch 205/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.2520 - val_loss: 12246.4609 - lr: 0.0100\n",
      "Epoch 206/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.2451 - val_loss: 12146.4463 - lr: 0.0100\n",
      "Epoch 207/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.6348 - val_loss: 12260.9746 - lr: 0.0100\n",
      "Epoch 208/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.3604 - val_loss: 12274.1416 - lr: 0.0100\n",
      "Epoch 209/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.7510 - val_loss: 12338.5693 - lr: 0.0100\n",
      "Epoch 210/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.5059 - val_loss: 12194.3398 - lr: 0.0100\n",
      "Epoch 211/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.3740 - val_loss: 12247.3564 - lr: 0.0100\n",
      "Epoch 212/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.0049 - val_loss: 12318.3096 - lr: 0.0100\n",
      "Epoch 213/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.2715 - val_loss: 12294.6406 - lr: 0.0100\n",
      "Epoch 214/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.5137 - val_loss: 12299.7559 - lr: 0.0100\n",
      "Epoch 215/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.2764 - val_loss: 12425.6504 - lr: 0.0100\n",
      "Epoch 216/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14485.1621 - val_loss: 12306.9102 - lr: 0.0100\n",
      "Epoch 217/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14483.7871 - val_loss: 12215.4307 - lr: 0.0100\n",
      "Epoch 218/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.1523 - val_loss: 12282.2803 - lr: 0.0100\n",
      "Epoch 219/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.2607 - val_loss: 12199.7314 - lr: 0.0100\n",
      "Epoch 220/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.2109 - val_loss: 12109.7529 - lr: 0.0100\n",
      "Epoch 221/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.2256 - val_loss: 12142.1807 - lr: 0.0100\n",
      "Epoch 222/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.4072 - val_loss: 12236.9951 - lr: 0.0100\n",
      "Epoch 223/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.4365 - val_loss: 12206.9473 - lr: 0.0100\n",
      "Epoch 224/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.5215 - val_loss: 12333.4814 - lr: 0.0100\n",
      "Epoch 225/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.3496 - val_loss: 12255.7490 - lr: 0.0100\n",
      "Epoch 226/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.5781 - val_loss: 12291.0469 - lr: 0.0100\n",
      "Epoch 227/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.3955 - val_loss: 12234.3652 - lr: 0.0100\n",
      "Epoch 228/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.0010 - val_loss: 12174.0576 - lr: 0.0100\n",
      "Epoch 229/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.0166 - val_loss: 12231.4590 - lr: 0.0100\n",
      "Epoch 230/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.1475 - val_loss: 12269.9434 - lr: 0.0100\n",
      "Epoch 231/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.3672 - val_loss: 12237.1787 - lr: 0.0100\n",
      "Epoch 232/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8701 - val_loss: 12277.5117 - lr: 0.0100\n",
      "Epoch 233/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.4336 - val_loss: 12220.0166 - lr: 0.0100\n",
      "Epoch 234/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.4521 - val_loss: 12312.4229 - lr: 0.0100\n",
      "Epoch 235/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14500.8105 - val_loss: 12264.9043 - lr: 0.0100\n",
      "Epoch 236/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.3418 - val_loss: 12358.5654 - lr: 0.0100\n",
      "Epoch 237/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.0332 - val_loss: 12226.3477 - lr: 0.0100\n",
      "Epoch 238/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.6543 - val_loss: 12393.3076 - lr: 0.0100\n",
      "Epoch 239/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14507.2793 - val_loss: 12188.8604 - lr: 0.0100\n",
      "Epoch 240/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7852 - val_loss: 12297.5889 - lr: 0.0100\n",
      "Epoch 241/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.1221 - val_loss: 12219.5947 - lr: 0.0100\n",
      "Epoch 242/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.3770 - val_loss: 12208.9014 - lr: 0.0100\n",
      "Epoch 243/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14507.5625 - val_loss: 12193.6514 - lr: 0.0100\n",
      "Epoch 244/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.2949 - val_loss: 12253.2441 - lr: 0.0100\n",
      "Epoch 245/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.1289 - val_loss: 12254.7900 - lr: 0.0100\n",
      "Epoch 246/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.7783 - val_loss: 12255.0859 - lr: 0.0100\n",
      "Epoch 247/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.4014 - val_loss: 12194.4053 - lr: 0.0100\n",
      "Epoch 248/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.1924 - val_loss: 12158.9688 - lr: 0.0100\n",
      "Epoch 249/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.2617 - val_loss: 12206.4854 - lr: 0.0100\n",
      "Epoch 250/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.5898 - val_loss: 12251.6816 - lr: 0.0100\n",
      "Epoch 251/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14492.7803 - val_loss: 12212.8203 - lr: 0.0100\n",
      "Epoch 252/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.0117 - val_loss: 12259.2090 - lr: 0.0100\n",
      "Epoch 253/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.4629 - val_loss: 12216.1924 - lr: 0.0100\n",
      "Epoch 254/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.9014 - val_loss: 12280.2979 - lr: 0.0100\n",
      "Epoch 255/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.3232 - val_loss: 12073.8438 - lr: 0.0100\n",
      "Epoch 256/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.1846 - val_loss: 12192.6367 - lr: 0.0100\n",
      "Epoch 257/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.6826 - val_loss: 12142.8555 - lr: 0.0100\n",
      "Epoch 258/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14501.8340 - val_loss: 12230.1064 - lr: 0.0100\n",
      "Epoch 259/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.7734 - val_loss: 12205.7549 - lr: 0.0100\n",
      "Epoch 260/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.3467 - val_loss: 12204.6113 - lr: 0.0100\n",
      "Epoch 261/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14487.0146 - val_loss: 12207.7568 - lr: 0.0100\n",
      "Epoch 262/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.4033 - val_loss: 12172.8467 - lr: 0.0100\n",
      "Epoch 263/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.9014 - val_loss: 12368.9756 - lr: 0.0100\n",
      "Epoch 264/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.7695 - val_loss: 12337.8398 - lr: 0.0100\n",
      "Epoch 265/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14484.9873 - val_loss: 12182.3047 - lr: 0.0100\n",
      "Epoch 266/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.5801 - val_loss: 12154.3809 - lr: 0.0100\n",
      "Epoch 267/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.1436 - val_loss: 12302.3301 - lr: 0.0100\n",
      "Epoch 268/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.0254 - val_loss: 12321.4170 - lr: 0.0100\n",
      "Epoch 269/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.7168 - val_loss: 12250.7588 - lr: 0.0100\n",
      "Epoch 270/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.6914 - val_loss: 12250.7686 - lr: 0.0100\n",
      "Epoch 271/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.7197 - val_loss: 12295.8174 - lr: 0.0100\n",
      "Epoch 272/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.9717 - val_loss: 12255.6270 - lr: 0.0100\n",
      "Epoch 273/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.6221 - val_loss: 12147.9170 - lr: 0.0100\n",
      "Epoch 274/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.5459 - val_loss: 12457.1650 - lr: 0.0100\n",
      "Epoch 275/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14487.1611 - val_loss: 12192.5312 - lr: 0.0100\n",
      "Epoch 276/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14498.2939 - val_loss: 12345.8857 - lr: 0.0100\n",
      "Epoch 277/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.9697 - val_loss: 12179.9141 - lr: 0.0100\n",
      "Epoch 278/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.7480 - val_loss: 12172.5723 - lr: 0.0100\n",
      "Epoch 279/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14502.7031 - val_loss: 12170.4209 - lr: 0.0100\n",
      "Epoch 280/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.9092 - val_loss: 12226.9814 - lr: 0.0100\n",
      "Epoch 281/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.0000 - val_loss: 12197.9658 - lr: 0.0100\n",
      "Epoch 282/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.3203 - val_loss: 12235.2334 - lr: 0.0100\n",
      "Epoch 283/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.5059 - val_loss: 12286.3730 - lr: 0.0100\n",
      "Epoch 284/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.5693 - val_loss: 12102.7295 - lr: 0.0100\n",
      "Epoch 285/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.6172 - val_loss: 12344.8477 - lr: 0.0100\n",
      "Epoch 286/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.1943 - val_loss: 12192.7549 - lr: 0.0100\n",
      "Epoch 287/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0996 - val_loss: 12279.8008 - lr: 0.0100\n",
      "Epoch 288/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.8525 - val_loss: 12301.2266 - lr: 0.0100\n",
      "Epoch 289/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.3242 - val_loss: 12224.4990 - lr: 0.0100\n",
      "Epoch 290/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14485.5537 - val_loss: 12159.2158 - lr: 0.0100\n",
      "Epoch 291/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.7549 - val_loss: 12173.7695 - lr: 0.0100\n",
      "Epoch 292/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.5381 - val_loss: 12183.1377 - lr: 0.0100\n",
      "Epoch 293/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.4395 - val_loss: 12302.1602 - lr: 0.0100\n",
      "Epoch 294/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14454.5293 - val_loss: 12150.5977 - lr: 0.0100\n",
      "Epoch 295/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.5859 - val_loss: 12161.3086 - lr: 0.0100\n",
      "Epoch 296/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14479.6104 - val_loss: 12233.5537 - lr: 0.0100\n",
      "Epoch 297/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.5479 - val_loss: 12369.5469 - lr: 0.0100\n",
      "Epoch 298/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8613 - val_loss: 12262.4873 - lr: 0.0100\n",
      "Epoch 299/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.9844 - val_loss: 12397.1963 - lr: 0.0100\n",
      "Epoch 300/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.3945 - val_loss: 12243.2637 - lr: 0.0100\n",
      "Epoch 301/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.1768 - val_loss: 12224.6338 - lr: 0.0100\n",
      "Epoch 302/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.5850 - val_loss: 12233.3408 - lr: 0.0100\n",
      "Epoch 303/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.4229 - val_loss: 12168.9473 - lr: 0.0100\n",
      "Epoch 304/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.5244 - val_loss: 12211.0088 - lr: 0.0100\n",
      "Epoch 305/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.3174 - val_loss: 12191.4326 - lr: 0.0100\n",
      "Epoch 306/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.3057 - val_loss: 12427.1875 - lr: 0.0100\n",
      "Epoch 307/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.1973 - val_loss: 12178.4531 - lr: 0.0100\n",
      "Epoch 308/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.0020 - val_loss: 12198.1738 - lr: 0.0100\n",
      "Epoch 309/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.4492 - val_loss: 12282.6270 - lr: 0.0100\n",
      "Epoch 310/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14495.1133 - val_loss: 12210.6455 - lr: 0.0100\n",
      "Epoch 311/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.5537 - val_loss: 12306.5088 - lr: 0.0100\n",
      "Epoch 312/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.2979 - val_loss: 12071.6895 - lr: 0.0100\n",
      "Epoch 313/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.4521 - val_loss: 12250.4463 - lr: 0.0100\n",
      "Epoch 314/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8428 - val_loss: 12296.1201 - lr: 0.0100\n",
      "Epoch 315/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.3740 - val_loss: 12167.0811 - lr: 0.0100\n",
      "Epoch 316/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.1709 - val_loss: 12250.8926 - lr: 0.0100\n",
      "Epoch 317/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.2773 - val_loss: 12320.2246 - lr: 0.0100\n",
      "Epoch 318/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.4668 - val_loss: 12138.7412 - lr: 0.0100\n",
      "Epoch 319/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.0742 - val_loss: 12175.8262 - lr: 0.0100\n",
      "Epoch 320/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.9580 - val_loss: 12230.7852 - lr: 0.0100\n",
      "Epoch 321/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.5000 - val_loss: 12286.5361 - lr: 0.0100\n",
      "Epoch 322/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.9531 - val_loss: 12270.2402 - lr: 0.0100\n",
      "Epoch 323/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.1250 - val_loss: 12211.3447 - lr: 0.0100\n",
      "Epoch 324/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14489.3643 - val_loss: 12457.7246 - lr: 0.0100\n",
      "Epoch 325/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.5850 - val_loss: 12196.7373 - lr: 0.0100\n",
      "Epoch 326/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14455.4951 - val_loss: 12166.9648 - lr: 0.0100\n",
      "Epoch 327/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.6445 - val_loss: 12255.1055 - lr: 0.0100\n",
      "Epoch 328/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.8398 - val_loss: 12169.3359 - lr: 0.0100\n",
      "Epoch 329/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.0859 - val_loss: 12240.5039 - lr: 0.0100\n",
      "Epoch 330/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14490.5439 - val_loss: 12239.2764 - lr: 0.0100\n",
      "Epoch 331/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.6963 - val_loss: 12219.7188 - lr: 0.0100\n",
      "Epoch 332/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.4434 - val_loss: 12158.5928 - lr: 0.0100\n",
      "Epoch 333/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.0479 - val_loss: 12193.4883 - lr: 0.0100\n",
      "Epoch 334/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.7803 - val_loss: 12229.4463 - lr: 0.0100\n",
      "Epoch 335/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.2051 - val_loss: 12262.6650 - lr: 0.0100\n",
      "Epoch 336/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.3047 - val_loss: 12217.7227 - lr: 0.0100\n",
      "Epoch 337/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.9014 - val_loss: 12196.9004 - lr: 0.0100\n",
      "Epoch 338/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.7949 - val_loss: 12208.8115 - lr: 0.0100\n",
      "Epoch 339/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8994 - val_loss: 12192.6660 - lr: 0.0100\n",
      "Epoch 340/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.4141 - val_loss: 12192.1592 - lr: 0.0100\n",
      "Epoch 341/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6914 - val_loss: 12210.4609 - lr: 0.0100\n",
      "Epoch 342/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.9414 - val_loss: 12254.8145 - lr: 0.0100\n",
      "Epoch 343/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.7910 - val_loss: 12227.9170 - lr: 0.0100\n",
      "Epoch 344/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14485.3877 - val_loss: 12155.3379 - lr: 0.0100\n",
      "Epoch 345/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5498 - val_loss: 12342.6279 - lr: 0.0100\n",
      "Epoch 346/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.5879 - val_loss: 12209.7959 - lr: 0.0100\n",
      "Epoch 347/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.4053 - val_loss: 12234.2217 - lr: 0.0100\n",
      "Epoch 348/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.2637 - val_loss: 12222.1807 - lr: 0.0100\n",
      "Epoch 349/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.9238 - val_loss: 12236.9189 - lr: 0.0100\n",
      "Epoch 350/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.0186 - val_loss: 12237.4121 - lr: 0.0100\n",
      "Epoch 351/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.6914 - val_loss: 12276.3516 - lr: 0.0100\n",
      "Epoch 352/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.6689 - val_loss: 12153.0371 - lr: 0.0100\n",
      "Epoch 353/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.1025 - val_loss: 12198.6865 - lr: 0.0100\n",
      "Epoch 354/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14476.3203 - val_loss: 12220.9111 - lr: 0.0100\n",
      "Epoch 355/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.7158 - val_loss: 12278.2686 - lr: 0.0100\n",
      "Epoch 356/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14492.6787 - val_loss: 12280.9482 - lr: 0.0100\n",
      "Epoch 357/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.9951 - val_loss: 12257.3047 - lr: 0.0100\n",
      "Epoch 358/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.6016 - val_loss: 12210.5752 - lr: 0.0100\n",
      "Epoch 359/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.0361 - val_loss: 12317.1992 - lr: 0.0100\n",
      "Epoch 360/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.3730 - val_loss: 12145.9473 - lr: 0.0100\n",
      "Epoch 361/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14483.6045 - val_loss: 12308.4375 - lr: 0.0100\n",
      "Epoch 362/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.0029 - val_loss: 12204.4727 - lr: 0.0100\n",
      "Epoch 363/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.8281 - val_loss: 12180.1016 - lr: 0.0100\n",
      "Epoch 364/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.5684 - val_loss: 12316.6777 - lr: 0.0100\n",
      "Epoch 365/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6113 - val_loss: 12216.1924 - lr: 0.0100\n",
      "Epoch 366/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.5928 - val_loss: 12258.0693 - lr: 0.0100\n",
      "Epoch 367/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.6094 - val_loss: 12183.4131 - lr: 0.0100\n",
      "Epoch 368/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.6660 - val_loss: 12116.8467 - lr: 0.0100\n",
      "Epoch 369/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.1357 - val_loss: 12202.0205 - lr: 0.0100\n",
      "Epoch 370/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.4629 - val_loss: 12305.6719 - lr: 0.0100\n",
      "Epoch 371/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.0557 - val_loss: 12337.0977 - lr: 0.0100\n",
      "Epoch 372/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.5469 - val_loss: 12307.4277 - lr: 0.0100\n",
      "Epoch 373/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14484.5430 - val_loss: 12153.5127 - lr: 0.0100\n",
      "Epoch 374/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.2744 - val_loss: 12166.3467 - lr: 0.0100\n",
      "Epoch 375/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14481.4150 - val_loss: 12233.7344 - lr: 0.0100\n",
      "Epoch 376/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.9756 - val_loss: 12180.4248 - lr: 0.0100\n",
      "Epoch 377/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.1631 - val_loss: 12319.2178 - lr: 0.0100\n",
      "Epoch 378/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.8008 - val_loss: 12103.1748 - lr: 0.0100\n",
      "Epoch 379/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.2607 - val_loss: 12318.7461 - lr: 0.0100\n",
      "Epoch 380/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14480.7500 - val_loss: 12097.8096 - lr: 0.0100\n",
      "Epoch 381/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.0361 - val_loss: 12079.2275 - lr: 0.0100\n",
      "Epoch 382/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.1953 - val_loss: 12354.0234 - lr: 0.0100\n",
      "Epoch 383/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.6875 - val_loss: 12314.1807 - lr: 0.0100\n",
      "Epoch 384/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.3447 - val_loss: 12223.8809 - lr: 0.0100\n",
      "Epoch 385/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14484.8984 - val_loss: 12282.8506 - lr: 0.0100\n",
      "Epoch 386/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.1973 - val_loss: 12230.9170 - lr: 0.0100\n",
      "Epoch 387/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.1650 - val_loss: 12129.2461 - lr: 0.0100\n",
      "Epoch 388/5000\n",
      "26/26 [==============================] - 0s 8ms/step - loss: 14468.7480 - val_loss: 12203.6758 - lr: 0.0100\n",
      "Epoch 389/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14473.6084 - val_loss: 12136.3682 - lr: 0.0100\n",
      "Epoch 390/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.5439 - val_loss: 12258.9180 - lr: 0.0100\n",
      "Epoch 391/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.5791 - val_loss: 12155.5381 - lr: 0.0100\n",
      "Epoch 392/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.7490 - val_loss: 12201.4912 - lr: 0.0100\n",
      "Epoch 393/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.6143 - val_loss: 12230.0967 - lr: 0.0100\n",
      "Epoch 394/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14464.4355 - val_loss: 12218.6836 - lr: 0.0100\n",
      "Epoch 395/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14462.8623 - val_loss: 12236.4209 - lr: 0.0100\n",
      "Epoch 396/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.5996 - val_loss: 12153.7949 - lr: 0.0100\n",
      "Epoch 397/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.8965 - val_loss: 12092.9990 - lr: 0.0100\n",
      "Epoch 398/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.8281 - val_loss: 12265.0801 - lr: 0.0100\n",
      "Epoch 399/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.3750 - val_loss: 12245.9971 - lr: 0.0100\n",
      "Epoch 400/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.2988 - val_loss: 12246.3350 - lr: 0.0100\n",
      "Epoch 401/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.2773 - val_loss: 12285.3428 - lr: 0.0100\n",
      "Epoch 402/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.0850 - val_loss: 12231.3857 - lr: 0.0100\n",
      "Epoch 403/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.1504 - val_loss: 12249.7266 - lr: 0.0100\n",
      "Epoch 404/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.0840 - val_loss: 12410.4717 - lr: 0.0100\n",
      "Epoch 405/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.5059 - val_loss: 12238.5166 - lr: 0.0100\n",
      "Epoch 406/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.9805 - val_loss: 12450.2412 - lr: 0.0100\n",
      "Epoch 407/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14472.3398 - val_loss: 12239.0098 - lr: 0.0100\n",
      "Epoch 408/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.9932 - val_loss: 12227.1426 - lr: 0.0100\n",
      "Epoch 409/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.5879 - val_loss: 12270.0801 - lr: 0.0100\n",
      "Epoch 410/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.3535 - val_loss: 12362.4795 - lr: 0.0100\n",
      "Epoch 411/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14477.7402 - val_loss: 12301.7842 - lr: 0.0100\n",
      "Epoch 412/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14488.8428 - val_loss: 12156.1152 - lr: 0.0100\n",
      "Epoch 413/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14512.0068 - val_loss: 12302.1064 - lr: 0.0100\n",
      "Epoch 414/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.1357 - val_loss: 12146.8984 - lr: 0.0100\n",
      "Epoch 415/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.1758 - val_loss: 12265.5020 - lr: 0.0100\n",
      "Epoch 416/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.6846 - val_loss: 12226.2139 - lr: 0.0100\n",
      "Epoch 417/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.9766 - val_loss: 12167.5098 - lr: 0.0100\n",
      "Epoch 418/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.7061 - val_loss: 12234.0938 - lr: 0.0100\n",
      "Epoch 419/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14502.4941 - val_loss: 12150.9453 - lr: 0.0100\n",
      "Epoch 420/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14507.1201 - val_loss: 12173.0342 - lr: 0.0100\n",
      "Epoch 421/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14481.1875 - val_loss: 12153.6230 - lr: 0.0100\n",
      "Epoch 422/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14492.8848 - val_loss: 12225.6035 - lr: 0.0100\n",
      "Epoch 423/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.7354 - val_loss: 12284.3740 - lr: 0.0100\n",
      "Epoch 424/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.5439 - val_loss: 12544.4297 - lr: 0.0100\n",
      "Epoch 425/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14492.8438 - val_loss: 12220.6143 - lr: 0.0100\n",
      "Epoch 426/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.3730 - val_loss: 12313.2764 - lr: 0.0100\n",
      "Epoch 427/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.6270 - val_loss: 12272.4131 - lr: 0.0100\n",
      "Epoch 428/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.1387 - val_loss: 12238.7197 - lr: 0.0100\n",
      "Epoch 429/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14499.1533 - val_loss: 12171.1064 - lr: 0.0100\n",
      "Epoch 430/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.2598 - val_loss: 12268.9502 - lr: 0.0100\n",
      "Epoch 431/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5557 - val_loss: 12232.4482 - lr: 0.0100\n",
      "Epoch 432/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.0713 - val_loss: 12252.2461 - lr: 0.0100\n",
      "Epoch 433/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.9668 - val_loss: 12407.4658 - lr: 0.0100\n",
      "Epoch 434/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14494.8330 - val_loss: 12436.2510 - lr: 0.0100\n",
      "Epoch 435/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.7715 - val_loss: 12409.4150 - lr: 0.0100\n",
      "Epoch 436/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14477.8135 - val_loss: 12114.9014 - lr: 0.0100\n",
      "Epoch 437/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14474.9248 - val_loss: 12245.5654 - lr: 0.0100\n",
      "Epoch 438/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.9043 - val_loss: 12214.0654 - lr: 0.0100\n",
      "Epoch 439/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.7354 - val_loss: 12326.8789 - lr: 0.0100\n",
      "Epoch 440/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14475.3496 - val_loss: 12233.9033 - lr: 0.0100\n",
      "Epoch 441/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.5781 - val_loss: 12297.6406 - lr: 0.0100\n",
      "Epoch 442/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.6152 - val_loss: 12323.8623 - lr: 0.0100\n",
      "Epoch 443/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.1787 - val_loss: 12161.4043 - lr: 0.0100\n",
      "Epoch 444/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.6484 - val_loss: 12237.9600 - lr: 0.0100\n",
      "Epoch 445/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.6963 - val_loss: 12223.1475 - lr: 0.0100\n",
      "Epoch 446/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14471.3965 - val_loss: 12173.3330 - lr: 0.0100\n",
      "Epoch 447/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.6738 - val_loss: 12153.8467 - lr: 0.0100\n",
      "Epoch 448/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14493.5732 - val_loss: 12322.9570 - lr: 0.0100\n",
      "Epoch 449/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.5518 - val_loss: 12254.3916 - lr: 0.0100\n",
      "Epoch 450/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.3730 - val_loss: 12334.1016 - lr: 0.0100\n",
      "Epoch 451/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.4404 - val_loss: 12200.3770 - lr: 0.0100\n",
      "Epoch 452/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.0127 - val_loss: 12215.1230 - lr: 0.0100\n",
      "Epoch 453/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.3525 - val_loss: 12163.2295 - lr: 0.0100\n",
      "Epoch 454/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.5635 - val_loss: 12163.3691 - lr: 0.0100\n",
      "Epoch 455/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14482.9639 - val_loss: 12218.4619 - lr: 0.0100\n",
      "Epoch 456/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.2324 - val_loss: 12142.5703 - lr: 0.0100\n",
      "Epoch 457/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14499.1758 - val_loss: 12323.6191 - lr: 0.0100\n",
      "Epoch 458/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.6270 - val_loss: 12163.3535 - lr: 0.0100\n",
      "Epoch 459/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.5361 - val_loss: 12220.2754 - lr: 0.0100\n",
      "Epoch 460/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.5566 - val_loss: 12249.7861 - lr: 0.0100\n",
      "Epoch 461/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.2432 - val_loss: 12253.4922 - lr: 0.0100\n",
      "Epoch 462/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 12994.2402\n",
      "Epoch 00462: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14469.4443 - val_loss: 12288.2129 - lr: 0.0100\n",
      "Epoch 463/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14470.2080 - val_loss: 12291.4463 - lr: 0.0070\n",
      "Epoch 464/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.7100 - val_loss: 12178.9707 - lr: 0.0070\n",
      "Epoch 465/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.1963 - val_loss: 12243.0801 - lr: 0.0070\n",
      "Epoch 466/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.6953 - val_loss: 12284.8965 - lr: 0.0070\n",
      "Epoch 467/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.0498 - val_loss: 12319.0186 - lr: 0.0070\n",
      "Epoch 468/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14478.5996 - val_loss: 12369.8955 - lr: 0.0070\n",
      "Epoch 469/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14451.7422 - val_loss: 12178.6973 - lr: 0.0070\n",
      "Epoch 470/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.4023 - val_loss: 12170.9775 - lr: 0.0070\n",
      "Epoch 471/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14484.6748 - val_loss: 12211.2588 - lr: 0.0070\n",
      "Epoch 472/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.7861 - val_loss: 12270.8184 - lr: 0.0070\n",
      "Epoch 473/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.4912 - val_loss: 12249.6113 - lr: 0.0070\n",
      "Epoch 474/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14448.4326 - val_loss: 12173.3262 - lr: 0.0070\n",
      "Epoch 475/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.3350 - val_loss: 12243.6816 - lr: 0.0070\n",
      "Epoch 476/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.9268 - val_loss: 12194.9062 - lr: 0.0070\n",
      "Epoch 477/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.0400 - val_loss: 12289.1992 - lr: 0.0070\n",
      "Epoch 478/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14465.1201 - val_loss: 12161.3398 - lr: 0.0070\n",
      "Epoch 479/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.6328 - val_loss: 12162.3789 - lr: 0.0070\n",
      "Epoch 480/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.8301 - val_loss: 12191.1553 - lr: 0.0070\n",
      "Epoch 481/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14466.0420 - val_loss: 12237.5635 - lr: 0.0070\n",
      "Epoch 482/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.5459 - val_loss: 12202.6455 - lr: 0.0070\n",
      "Epoch 483/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.9189 - val_loss: 12304.0293 - lr: 0.0070\n",
      "Epoch 484/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.2334 - val_loss: 12198.1250 - lr: 0.0070\n",
      "Epoch 485/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.1475 - val_loss: 12163.6016 - lr: 0.0070\n",
      "Epoch 486/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.3135 - val_loss: 12208.3682 - lr: 0.0070\n",
      "Epoch 487/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14473.5078 - val_loss: 12250.1836 - lr: 0.0070\n",
      "Epoch 488/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.1553 - val_loss: 12236.7129 - lr: 0.0070\n",
      "Epoch 489/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14455.8799 - val_loss: 12213.1787 - lr: 0.0070\n",
      "Epoch 490/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.2246 - val_loss: 12200.3633 - lr: 0.0070\n",
      "Epoch 491/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.4385 - val_loss: 12240.5410 - lr: 0.0070\n",
      "Epoch 492/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.8408 - val_loss: 12208.6484 - lr: 0.0070\n",
      "Epoch 493/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14469.9434 - val_loss: 12188.6572 - lr: 0.0070\n",
      "Epoch 494/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.0762 - val_loss: 12212.0732 - lr: 0.0070\n",
      "Epoch 495/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8428 - val_loss: 12353.5605 - lr: 0.0070\n",
      "Epoch 496/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.9775 - val_loss: 12209.1250 - lr: 0.0070\n",
      "Epoch 497/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.3613 - val_loss: 12395.3311 - lr: 0.0070\n",
      "Epoch 498/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.8955 - val_loss: 12259.2734 - lr: 0.0070\n",
      "Epoch 499/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.4238 - val_loss: 12231.0088 - lr: 0.0070\n",
      "Epoch 500/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.0889 - val_loss: 12181.7129 - lr: 0.0070\n",
      "Epoch 501/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.0117 - val_loss: 12255.8379 - lr: 0.0070\n",
      "Epoch 502/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.0527 - val_loss: 12286.2354 - lr: 0.0070\n",
      "Epoch 503/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.3213 - val_loss: 12334.6611 - lr: 0.0070\n",
      "Epoch 504/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.6592 - val_loss: 12204.9141 - lr: 0.0070\n",
      "Epoch 505/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.5791 - val_loss: 12239.4141 - lr: 0.0070\n",
      "Epoch 506/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.1123 - val_loss: 12161.7158 - lr: 0.0070\n",
      "Epoch 507/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.0771 - val_loss: 12182.4463 - lr: 0.0070\n",
      "Epoch 508/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.6602 - val_loss: 12373.1289 - lr: 0.0070\n",
      "Epoch 509/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14479.9014 - val_loss: 12391.5547 - lr: 0.0070\n",
      "Epoch 510/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14481.7695 - val_loss: 12225.0117 - lr: 0.0070\n",
      "Epoch 511/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14472.2617 - val_loss: 12355.1943 - lr: 0.0070\n",
      "Epoch 512/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.7988 - val_loss: 12208.6787 - lr: 0.0070\n",
      "Epoch 513/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.7881 - val_loss: 12209.4102 - lr: 0.0070\n",
      "Epoch 514/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.8350 - val_loss: 12212.6123 - lr: 0.0070\n",
      "Epoch 515/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.7002 - val_loss: 12236.3936 - lr: 0.0070\n",
      "Epoch 516/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2529 - val_loss: 12293.2754 - lr: 0.0070\n",
      "Epoch 517/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2646 - val_loss: 12261.8936 - lr: 0.0070\n",
      "Epoch 518/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.5693 - val_loss: 12245.6836 - lr: 0.0070\n",
      "Epoch 519/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6523 - val_loss: 12211.6016 - lr: 0.0070\n",
      "Epoch 520/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.8047 - val_loss: 12170.5732 - lr: 0.0070\n",
      "Epoch 521/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.8369 - val_loss: 12315.8936 - lr: 0.0070\n",
      "Epoch 522/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.6719 - val_loss: 12180.2607 - lr: 0.0070\n",
      "Epoch 523/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14453.1572 - val_loss: 12286.0615 - lr: 0.0070\n",
      "Epoch 524/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.5703 - val_loss: 12308.9551 - lr: 0.0070\n",
      "Epoch 525/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.2109 - val_loss: 12238.5381 - lr: 0.0070\n",
      "Epoch 526/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.9717 - val_loss: 12231.9219 - lr: 0.0070\n",
      "Epoch 527/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.6816 - val_loss: 12263.8252 - lr: 0.0070\n",
      "Epoch 528/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14464.3105 - val_loss: 12229.5449 - lr: 0.0070\n",
      "Epoch 529/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.6777 - val_loss: 12201.1836 - lr: 0.0070\n",
      "Epoch 530/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.6973 - val_loss: 12311.7627 - lr: 0.0070\n",
      "Epoch 531/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.5195 - val_loss: 12167.4736 - lr: 0.0070\n",
      "Epoch 532/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.0068 - val_loss: 12331.5205 - lr: 0.0070\n",
      "Epoch 533/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14493.8379 - val_loss: 12306.4961 - lr: 0.0070\n",
      "Epoch 534/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.2354 - val_loss: 12189.1865 - lr: 0.0070\n",
      "Epoch 535/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.2480 - val_loss: 12319.8252 - lr: 0.0070\n",
      "Epoch 536/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.3457 - val_loss: 12204.3311 - lr: 0.0070\n",
      "Epoch 537/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.7832 - val_loss: 12201.4521 - lr: 0.0070\n",
      "Epoch 538/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.6973 - val_loss: 12383.9688 - lr: 0.0070\n",
      "Epoch 539/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.8652 - val_loss: 12193.8184 - lr: 0.0070\n",
      "Epoch 540/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14453.2266 - val_loss: 12390.1797 - lr: 0.0070\n",
      "Epoch 541/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.8379 - val_loss: 12232.0713 - lr: 0.0070\n",
      "Epoch 542/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.7471 - val_loss: 12259.3223 - lr: 0.0070\n",
      "Epoch 543/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14464.2324 - val_loss: 12158.1348 - lr: 0.0070\n",
      "Epoch 544/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.9658 - val_loss: 12249.6406 - lr: 0.0070\n",
      "Epoch 545/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.4863 - val_loss: 12240.0039 - lr: 0.0070\n",
      "Epoch 546/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.8701 - val_loss: 12221.0527 - lr: 0.0070\n",
      "Epoch 547/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.6250 - val_loss: 12186.7363 - lr: 0.0070\n",
      "Epoch 548/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14465.1406 - val_loss: 12266.3867 - lr: 0.0070\n",
      "Epoch 549/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14459.7939 - val_loss: 12251.5859 - lr: 0.0070\n",
      "Epoch 550/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.0166 - val_loss: 12290.2158 - lr: 0.0070\n",
      "Epoch 551/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.8896 - val_loss: 12230.9385 - lr: 0.0070\n",
      "Epoch 552/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.7861 - val_loss: 12317.5186 - lr: 0.0070\n",
      "Epoch 553/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.1162 - val_loss: 12232.4648 - lr: 0.0070\n",
      "Epoch 554/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14453.2188 - val_loss: 12310.3672 - lr: 0.0070\n",
      "Epoch 555/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.9990 - val_loss: 12210.1641 - lr: 0.0070\n",
      "Epoch 556/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.3975 - val_loss: 12200.5410 - lr: 0.0070\n",
      "Epoch 557/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14468.2500 - val_loss: 12220.9590 - lr: 0.0070\n",
      "Epoch 558/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.1543 - val_loss: 12270.6465 - lr: 0.0070\n",
      "Epoch 559/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.9375 - val_loss: 12311.9746 - lr: 0.0070\n",
      "Epoch 560/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.6436 - val_loss: 12164.0967 - lr: 0.0070\n",
      "Epoch 561/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14479.7617 - val_loss: 12237.6006 - lr: 0.0070\n",
      "Epoch 562/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.1602 - val_loss: 12267.2188 - lr: 0.0070\n",
      "Epoch 563/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14453.4121 - val_loss: 12212.5928 - lr: 0.0070\n",
      "Epoch 564/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.6152 - val_loss: 12261.6973 - lr: 0.0070\n",
      "Epoch 565/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.3027 - val_loss: 12198.0713 - lr: 0.0070\n",
      "Epoch 566/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14468.7461 - val_loss: 12227.5352 - lr: 0.0070\n",
      "Epoch 567/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.7344 - val_loss: 12191.7461 - lr: 0.0070\n",
      "Epoch 568/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.7910 - val_loss: 12172.4482 - lr: 0.0070\n",
      "Epoch 569/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14480.9980 - val_loss: 12248.3633 - lr: 0.0070\n",
      "Epoch 570/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.7812 - val_loss: 12173.9170 - lr: 0.0070\n",
      "Epoch 571/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14473.7695 - val_loss: 12276.1455 - lr: 0.0070\n",
      "Epoch 572/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.9688 - val_loss: 12175.3906 - lr: 0.0070\n",
      "Epoch 573/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.0322 - val_loss: 12234.9678 - lr: 0.0070\n",
      "Epoch 574/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14457.6309 - val_loss: 12193.7910 - lr: 0.0070\n",
      "Epoch 575/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.9541 - val_loss: 12191.8770 - lr: 0.0070\n",
      "Epoch 576/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.0996 - val_loss: 12167.9053 - lr: 0.0070\n",
      "Epoch 577/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14458.5137 - val_loss: 12254.8311 - lr: 0.0070\n",
      "Epoch 578/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.4082 - val_loss: 12210.1914 - lr: 0.0070\n",
      "Epoch 579/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14456.5186 - val_loss: 12190.8838 - lr: 0.0070\n",
      "Epoch 580/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.8271 - val_loss: 12283.3516 - lr: 0.0070\n",
      "Epoch 581/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14463.7139 - val_loss: 12178.9121 - lr: 0.0070\n",
      "Epoch 582/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.5371 - val_loss: 12175.9590 - lr: 0.0070\n",
      "Epoch 583/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.8105 - val_loss: 12230.3291 - lr: 0.0070\n",
      "Epoch 584/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.3506 - val_loss: 12234.5938 - lr: 0.0070\n",
      "Epoch 585/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14483.0693 - val_loss: 12267.1064 - lr: 0.0070\n",
      "Epoch 586/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.9033 - val_loss: 12258.7568 - lr: 0.0070\n",
      "Epoch 587/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14475.6494 - val_loss: 12189.3623 - lr: 0.0070\n",
      "Epoch 588/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14466.8203 - val_loss: 12338.0098 - lr: 0.0070\n",
      "Epoch 589/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14460.3809 - val_loss: 12179.0645 - lr: 0.0070\n",
      "Epoch 590/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 14458.5752 - val_loss: 12301.9717 - lr: 0.0070\n",
      "Epoch 591/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14459.5049 - val_loss: 12198.2100 - lr: 0.0070\n",
      "Epoch 592/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.5713 - val_loss: 12301.0488 - lr: 0.0070\n",
      "Epoch 593/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14476.3447 - val_loss: 12284.4502 - lr: 0.0070\n",
      "Epoch 594/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.2051 - val_loss: 12358.8535 - lr: 0.0070\n",
      "Epoch 595/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14485.2959 - val_loss: 12194.0225 - lr: 0.0070\n",
      "Epoch 596/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14455.8018 - val_loss: 12300.6885 - lr: 0.0070\n",
      "Epoch 597/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14461.3271 - val_loss: 12263.8740 - lr: 0.0070\n",
      "Epoch 598/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14467.5576 - val_loss: 12090.6201 - lr: 0.0070\n",
      "Epoch 599/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14478.2148 - val_loss: 12243.3711 - lr: 0.0070\n",
      "Epoch 600/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14456.5488 - val_loss: 12325.6494 - lr: 0.0070\n",
      "Epoch 601/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14471.8027 - val_loss: 12218.3906 - lr: 0.0070\n",
      "Epoch 602/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14474.8906 - val_loss: 12219.6855 - lr: 0.0070\n",
      "Epoch 603/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14489.1523 - val_loss: 12217.3652 - lr: 0.0070\n",
      "Epoch 604/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14460.7061 - val_loss: 12301.1816 - lr: 0.0070\n",
      "Epoch 605/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14463.9346 - val_loss: 12182.6006 - lr: 0.0070\n",
      "Epoch 606/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14467.0986 - val_loss: 12263.5127 - lr: 0.0070\n",
      "Epoch 607/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14454.3438 - val_loss: 12291.6221 - lr: 0.0070\n",
      "Epoch 608/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14470.4795 - val_loss: 12175.7529 - lr: 0.0070\n",
      "Epoch 609/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14462.7832 - val_loss: 12415.1318 - lr: 0.0070\n",
      "Epoch 610/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14457.3076 - val_loss: 12223.9492 - lr: 0.0070\n",
      "Epoch 611/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14458.2207 - val_loss: 12239.5654 - lr: 0.0070\n",
      "Epoch 612/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 13737.8281\n",
      "Epoch 00612: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "Restoring model weights from the end of the best epoch: 312.\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 14461.0264 - val_loss: 12184.3730 - lr: 0.0070\n",
      "Epoch 00612: early stopping\n",
      "run-fold: 1-0 SMAPE: 39.25285\n",
      "Fold 1.1\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 577\n",
      "Trainable params: 577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuya/.pyenv/versions/3.9.7/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 6ms/step - loss: 188719.5625 - val_loss: 228469.5000 - lr: 0.0100\n",
      "Epoch 2/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 163277.8281 - val_loss: 181691.9062 - lr: 0.0100\n",
      "Epoch 3/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 117871.5391 - val_loss: 120625.2891 - lr: 0.0100\n",
      "Epoch 4/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 75529.7656 - val_loss: 80208.6562 - lr: 0.0100\n",
      "Epoch 5/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 54425.1797 - val_loss: 64112.7891 - lr: 0.0100\n",
      "Epoch 6/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 44801.0898 - val_loss: 54451.8008 - lr: 0.0100\n",
      "Epoch 7/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 37357.5312 - val_loss: 46145.5781 - lr: 0.0100\n",
      "Epoch 8/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 30851.2070 - val_loss: 38991.5195 - lr: 0.0100\n",
      "Epoch 9/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 25410.0371 - val_loss: 32809.0898 - lr: 0.0100\n",
      "Epoch 10/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 21016.1562 - val_loss: 27898.0449 - lr: 0.0100\n",
      "Epoch 11/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 17764.0684 - val_loss: 24194.6055 - lr: 0.0100\n",
      "Epoch 12/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 15556.7842 - val_loss: 21533.6855 - lr: 0.0100\n",
      "Epoch 13/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 14133.3916 - val_loss: 19737.5586 - lr: 0.0100\n",
      "Epoch 14/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 13251.4385 - val_loss: 18631.3301 - lr: 0.0100\n",
      "Epoch 15/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 12712.4053 - val_loss: 17839.4258 - lr: 0.0100\n",
      "Epoch 16/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 12358.8936 - val_loss: 17384.9746 - lr: 0.0100\n",
      "Epoch 17/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 12109.2490 - val_loss: 17070.7500 - lr: 0.0100\n",
      "Epoch 18/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11925.9961 - val_loss: 16884.6152 - lr: 0.0100\n",
      "Epoch 19/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11788.1621 - val_loss: 16763.3008 - lr: 0.0100\n",
      "Epoch 20/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11657.7246 - val_loss: 16681.3867 - lr: 0.0100\n",
      "Epoch 21/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11553.5234 - val_loss: 16613.7676 - lr: 0.0100\n",
      "Epoch 22/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11459.1455 - val_loss: 16531.4082 - lr: 0.0100\n",
      "Epoch 23/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11381.2266 - val_loss: 16574.4980 - lr: 0.0100\n",
      "Epoch 24/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11308.6758 - val_loss: 16536.4922 - lr: 0.0100\n",
      "Epoch 25/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11250.2725 - val_loss: 16530.0723 - lr: 0.0100\n",
      "Epoch 26/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11196.0898 - val_loss: 16605.2051 - lr: 0.0100\n",
      "Epoch 27/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11153.4580 - val_loss: 16732.1699 - lr: 0.0100\n",
      "Epoch 28/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11115.4336 - val_loss: 16633.3730 - lr: 0.0100\n",
      "Epoch 29/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11080.0391 - val_loss: 16626.4570 - lr: 0.0100\n",
      "Epoch 30/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11049.6865 - val_loss: 16803.9668 - lr: 0.0100\n",
      "Epoch 31/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 11023.9766 - val_loss: 16737.3691 - lr: 0.0100\n",
      "Epoch 32/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 11004.2207 - val_loss: 16804.8027 - lr: 0.0100\n",
      "Epoch 33/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10988.7334 - val_loss: 17012.7168 - lr: 0.0100\n",
      "Epoch 34/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10974.1416 - val_loss: 16982.5039 - lr: 0.0100\n",
      "Epoch 35/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10958.1055 - val_loss: 17013.9297 - lr: 0.0100\n",
      "Epoch 36/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10948.8916 - val_loss: 16998.0352 - lr: 0.0100\n",
      "Epoch 37/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10942.7207 - val_loss: 17148.2539 - lr: 0.0100\n",
      "Epoch 38/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10936.4102 - val_loss: 16996.1094 - lr: 0.0100\n",
      "Epoch 39/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10931.7578 - val_loss: 17504.2188 - lr: 0.0100\n",
      "Epoch 40/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.4121 - val_loss: 17021.8027 - lr: 0.0100\n",
      "Epoch 41/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.9004 - val_loss: 17420.4980 - lr: 0.0100\n",
      "Epoch 42/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10925.5996 - val_loss: 17538.5762 - lr: 0.0100\n",
      "Epoch 43/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.7109 - val_loss: 17353.2930 - lr: 0.0100\n",
      "Epoch 44/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.3994 - val_loss: 17460.6992 - lr: 0.0100\n",
      "Epoch 45/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.6299 - val_loss: 17497.9023 - lr: 0.0100\n",
      "Epoch 46/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.0332 - val_loss: 17347.2578 - lr: 0.0100\n",
      "Epoch 47/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.5488 - val_loss: 17375.8926 - lr: 0.0100\n",
      "Epoch 48/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.3818 - val_loss: 17408.2930 - lr: 0.0100\n",
      "Epoch 49/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.1885 - val_loss: 17956.0820 - lr: 0.0100\n",
      "Epoch 50/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.9639 - val_loss: 17386.4629 - lr: 0.0100\n",
      "Epoch 51/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.7227 - val_loss: 17511.0664 - lr: 0.0100\n",
      "Epoch 52/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.6484 - val_loss: 17560.3945 - lr: 0.0100\n",
      "Epoch 53/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10905.9053 - val_loss: 17531.2578 - lr: 0.0100\n",
      "Epoch 54/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10906.4980 - val_loss: 17471.8594 - lr: 0.0100\n",
      "Epoch 55/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.5195 - val_loss: 17614.0098 - lr: 0.0100\n",
      "Epoch 56/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.5596 - val_loss: 17648.9727 - lr: 0.0100\n",
      "Epoch 57/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.9209 - val_loss: 17518.9863 - lr: 0.0100\n",
      "Epoch 58/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.6572 - val_loss: 17717.4199 - lr: 0.0100\n",
      "Epoch 59/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.6270 - val_loss: 18037.6484 - lr: 0.0100\n",
      "Epoch 60/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.2607 - val_loss: 17533.1758 - lr: 0.0100\n",
      "Epoch 61/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.6660 - val_loss: 17581.7168 - lr: 0.0100\n",
      "Epoch 62/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.4443 - val_loss: 17398.5234 - lr: 0.0100\n",
      "Epoch 63/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.7666 - val_loss: 17964.3281 - lr: 0.0100\n",
      "Epoch 64/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5166 - val_loss: 17773.4922 - lr: 0.0100\n",
      "Epoch 65/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10906.7461 - val_loss: 17473.4023 - lr: 0.0100\n",
      "Epoch 66/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.7354 - val_loss: 17716.7715 - lr: 0.0100\n",
      "Epoch 67/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.3281 - val_loss: 17697.2266 - lr: 0.0100\n",
      "Epoch 68/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10905.7178 - val_loss: 17906.2227 - lr: 0.0100\n",
      "Epoch 69/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.4609 - val_loss: 17877.3125 - lr: 0.0100\n",
      "Epoch 70/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10925.7051 - val_loss: 17598.1289 - lr: 0.0100\n",
      "Epoch 71/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10939.1992 - val_loss: 17231.7812 - lr: 0.0100\n",
      "Epoch 72/5000\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 10913.7275 - val_loss: 17445.3203 - lr: 0.0100\n",
      "Epoch 73/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.3076 - val_loss: 17678.2852 - lr: 0.0100\n",
      "Epoch 74/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.6807 - val_loss: 17640.7559 - lr: 0.0100\n",
      "Epoch 75/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10929.2949 - val_loss: 17389.8770 - lr: 0.0100\n",
      "Epoch 76/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10905.5107 - val_loss: 17485.1602 - lr: 0.0100\n",
      "Epoch 77/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.7227 - val_loss: 17872.6094 - lr: 0.0100\n",
      "Epoch 78/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.5293 - val_loss: 17685.1680 - lr: 0.0100\n",
      "Epoch 79/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.2305 - val_loss: 17710.7051 - lr: 0.0100\n",
      "Epoch 80/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.6387 - val_loss: 17750.5410 - lr: 0.0100\n",
      "Epoch 81/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.6465 - val_loss: 17972.9062 - lr: 0.0100\n",
      "Epoch 82/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.6758 - val_loss: 17546.5645 - lr: 0.0100\n",
      "Epoch 83/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.0479 - val_loss: 17940.6973 - lr: 0.0100\n",
      "Epoch 84/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1582 - val_loss: 18083.5742 - lr: 0.0100\n",
      "Epoch 85/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.3486 - val_loss: 17627.7852 - lr: 0.0100\n",
      "Epoch 86/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.4580 - val_loss: 17495.8809 - lr: 0.0100\n",
      "Epoch 87/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.4580 - val_loss: 17543.6191 - lr: 0.0100\n",
      "Epoch 88/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.6924 - val_loss: 17524.6816 - lr: 0.0100\n",
      "Epoch 89/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.3838 - val_loss: 17539.7676 - lr: 0.0100\n",
      "Epoch 90/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.7881 - val_loss: 17564.8164 - lr: 0.0100\n",
      "Epoch 91/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.7393 - val_loss: 17541.5762 - lr: 0.0100\n",
      "Epoch 92/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.5273 - val_loss: 17908.6836 - lr: 0.0100\n",
      "Epoch 93/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10923.3486 - val_loss: 17858.6152 - lr: 0.0100\n",
      "Epoch 94/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10903.8818 - val_loss: 17462.7656 - lr: 0.0100\n",
      "Epoch 95/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.0596 - val_loss: 18007.4531 - lr: 0.0100\n",
      "Epoch 96/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.8633 - val_loss: 17892.8945 - lr: 0.0100\n",
      "Epoch 97/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.0635 - val_loss: 18179.7305 - lr: 0.0100\n",
      "Epoch 98/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.2617 - val_loss: 17657.6953 - lr: 0.0100\n",
      "Epoch 99/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.0449 - val_loss: 17586.7656 - lr: 0.0100\n",
      "Epoch 100/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.9365 - val_loss: 17700.4043 - lr: 0.0100\n",
      "Epoch 101/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.2939 - val_loss: 17230.7148 - lr: 0.0100\n",
      "Epoch 102/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10931.3086 - val_loss: 17240.5215 - lr: 0.0100\n",
      "Epoch 103/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10930.8213 - val_loss: 17487.9688 - lr: 0.0100\n",
      "Epoch 104/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10924.8242 - val_loss: 17978.2324 - lr: 0.0100\n",
      "Epoch 105/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10931.1006 - val_loss: 17982.0977 - lr: 0.0100\n",
      "Epoch 106/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.5859 - val_loss: 17323.9102 - lr: 0.0100\n",
      "Epoch 107/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.6279 - val_loss: 17763.9258 - lr: 0.0100\n",
      "Epoch 108/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.8828 - val_loss: 18081.9609 - lr: 0.0100\n",
      "Epoch 109/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10928.7129 - val_loss: 17922.5898 - lr: 0.0100\n",
      "Epoch 110/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.3184 - val_loss: 18045.7988 - lr: 0.0100\n",
      "Epoch 111/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.6768 - val_loss: 18082.4160 - lr: 0.0100\n",
      "Epoch 112/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.9092 - val_loss: 17865.3379 - lr: 0.0100\n",
      "Epoch 113/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.5254 - val_loss: 18001.4961 - lr: 0.0100\n",
      "Epoch 114/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.7695 - val_loss: 17525.6172 - lr: 0.0100\n",
      "Epoch 115/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.0146 - val_loss: 17599.7656 - lr: 0.0100\n",
      "Epoch 116/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.3008 - val_loss: 17319.8555 - lr: 0.0100\n",
      "Epoch 117/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.7178 - val_loss: 17291.7598 - lr: 0.0100\n",
      "Epoch 118/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10938.8750 - val_loss: 17205.6699 - lr: 0.0100\n",
      "Epoch 119/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.5293 - val_loss: 17618.4277 - lr: 0.0100\n",
      "Epoch 120/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.6299 - val_loss: 17814.5332 - lr: 0.0100\n",
      "Epoch 121/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.2666 - val_loss: 17730.3633 - lr: 0.0100\n",
      "Epoch 122/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.7705 - val_loss: 18237.3340 - lr: 0.0100\n",
      "Epoch 123/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10933.7021 - val_loss: 17909.8613 - lr: 0.0100\n",
      "Epoch 124/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.9121 - val_loss: 17845.2188 - lr: 0.0100\n",
      "Epoch 125/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.3857 - val_loss: 17694.8691 - lr: 0.0100\n",
      "Epoch 126/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.9590 - val_loss: 17892.9785 - lr: 0.0100\n",
      "Epoch 127/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.3740 - val_loss: 18126.8730 - lr: 0.0100\n",
      "Epoch 128/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.0830 - val_loss: 18077.3926 - lr: 0.0100\n",
      "Epoch 129/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.4668 - val_loss: 17780.2402 - lr: 0.0100\n",
      "Epoch 130/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.5420 - val_loss: 17585.6797 - lr: 0.0100\n",
      "Epoch 131/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.7461 - val_loss: 17688.4824 - lr: 0.0100\n",
      "Epoch 132/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.8027 - val_loss: 17402.1348 - lr: 0.0100\n",
      "Epoch 133/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.1133 - val_loss: 17777.5176 - lr: 0.0100\n",
      "Epoch 134/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.4111 - val_loss: 18174.4277 - lr: 0.0100\n",
      "Epoch 135/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.0801 - val_loss: 17374.5586 - lr: 0.0100\n",
      "Epoch 136/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10928.7373 - val_loss: 17204.5977 - lr: 0.0100\n",
      "Epoch 137/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.3623 - val_loss: 17455.5234 - lr: 0.0100\n",
      "Epoch 138/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.3096 - val_loss: 17681.1133 - lr: 0.0100\n",
      "Epoch 139/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.7109 - val_loss: 17873.3945 - lr: 0.0100\n",
      "Epoch 140/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.8262 - val_loss: 17689.8770 - lr: 0.0100\n",
      "Epoch 141/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.4521 - val_loss: 18258.2559 - lr: 0.0100\n",
      "Epoch 142/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.7285 - val_loss: 17727.5781 - lr: 0.0100\n",
      "Epoch 143/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.1475 - val_loss: 17942.2383 - lr: 0.0100\n",
      "Epoch 144/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.8828 - val_loss: 17740.7285 - lr: 0.0100\n",
      "Epoch 145/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.9766 - val_loss: 17765.0098 - lr: 0.0100\n",
      "Epoch 146/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.7988 - val_loss: 17494.0879 - lr: 0.0100\n",
      "Epoch 147/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.3271 - val_loss: 17327.6406 - lr: 0.0100\n",
      "Epoch 148/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10939.3379 - val_loss: 17306.0352 - lr: 0.0100\n",
      "Epoch 149/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.9873 - val_loss: 18349.8867 - lr: 0.0100\n",
      "Epoch 150/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10928.7402 - val_loss: 17386.5586 - lr: 0.0100\n",
      "Epoch 151/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.2822 - val_loss: 17950.2227 - lr: 0.0100\n",
      "Epoch 152/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.5371 - val_loss: 17936.8281 - lr: 0.0100\n",
      "Epoch 153/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.4834 - val_loss: 17661.5840 - lr: 0.0100\n",
      "Epoch 154/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.5928 - val_loss: 17429.3652 - lr: 0.0100\n",
      "Epoch 155/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.6318 - val_loss: 17565.0078 - lr: 0.0100\n",
      "Epoch 156/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.2266 - val_loss: 17668.0430 - lr: 0.0100\n",
      "Epoch 157/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10926.0918 - val_loss: 18306.1621 - lr: 0.0100\n",
      "Epoch 158/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.1963 - val_loss: 17460.7832 - lr: 0.0100\n",
      "Epoch 159/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.7656 - val_loss: 17585.2637 - lr: 0.0100\n",
      "Epoch 160/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10926.1562 - val_loss: 17544.8535 - lr: 0.0100\n",
      "Epoch 161/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.9414 - val_loss: 17380.5703 - lr: 0.0100\n",
      "Epoch 162/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.5156 - val_loss: 18185.7480 - lr: 0.0100\n",
      "Epoch 163/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.3574 - val_loss: 17628.5410 - lr: 0.0100\n",
      "Epoch 164/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.7236 - val_loss: 17351.0293 - lr: 0.0100\n",
      "Epoch 165/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.8604 - val_loss: 17676.4102 - lr: 0.0100\n",
      "Epoch 166/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.3467 - val_loss: 17691.3926 - lr: 0.0100\n",
      "Epoch 167/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10924.1064 - val_loss: 17759.0410 - lr: 0.0100\n",
      "Epoch 168/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10938.8057 - val_loss: 17824.2617 - lr: 0.0100\n",
      "Epoch 169/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.8174 - val_loss: 17407.1445 - lr: 0.0100\n",
      "Epoch 170/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10938.3701 - val_loss: 17408.7969 - lr: 0.0100\n",
      "Epoch 171/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10935.6094 - val_loss: 18073.0781 - lr: 0.0100\n",
      "Epoch 172/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.3066 - val_loss: 17599.3008 - lr: 0.0100\n",
      "Epoch 173/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.5332 - val_loss: 17333.3535 - lr: 0.0100\n",
      "Epoch 174/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10937.1104 - val_loss: 17823.2168 - lr: 0.0100\n",
      "Epoch 175/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 11741.1602\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.006999999843537807.\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.7344 - val_loss: 17651.4023 - lr: 0.0100\n",
      "Epoch 176/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.5771 - val_loss: 17170.5078 - lr: 0.0070\n",
      "Epoch 177/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.8066 - val_loss: 17203.3516 - lr: 0.0070\n",
      "Epoch 178/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.9854 - val_loss: 17633.0938 - lr: 0.0070\n",
      "Epoch 179/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.8838 - val_loss: 17697.1914 - lr: 0.0070\n",
      "Epoch 180/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.7100 - val_loss: 17230.6562 - lr: 0.0070\n",
      "Epoch 181/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.0537 - val_loss: 17354.5996 - lr: 0.0070\n",
      "Epoch 182/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.9307 - val_loss: 17695.0137 - lr: 0.0070\n",
      "Epoch 183/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.8594 - val_loss: 17376.9082 - lr: 0.0070\n",
      "Epoch 184/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.0996 - val_loss: 17949.6797 - lr: 0.0070\n",
      "Epoch 185/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.0176 - val_loss: 18066.5449 - lr: 0.0070\n",
      "Epoch 186/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.2793 - val_loss: 17798.6309 - lr: 0.0070\n",
      "Epoch 187/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.0771 - val_loss: 17539.6270 - lr: 0.0070\n",
      "Epoch 188/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10926.1445 - val_loss: 17536.8477 - lr: 0.0070\n",
      "Epoch 189/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.3604 - val_loss: 17262.4277 - lr: 0.0070\n",
      "Epoch 190/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.3809 - val_loss: 18033.8086 - lr: 0.0070\n",
      "Epoch 191/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.8584 - val_loss: 17620.8516 - lr: 0.0070\n",
      "Epoch 192/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.0771 - val_loss: 17965.8789 - lr: 0.0070\n",
      "Epoch 193/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.2637 - val_loss: 17999.1816 - lr: 0.0070\n",
      "Epoch 194/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.4570 - val_loss: 17467.4395 - lr: 0.0070\n",
      "Epoch 195/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.1133 - val_loss: 17883.6387 - lr: 0.0070\n",
      "Epoch 196/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.1865 - val_loss: 18272.0957 - lr: 0.0070\n",
      "Epoch 197/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.0859 - val_loss: 17948.2344 - lr: 0.0070\n",
      "Epoch 198/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.7266 - val_loss: 17836.5410 - lr: 0.0070\n",
      "Epoch 199/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10927.8232 - val_loss: 17641.2207 - lr: 0.0070\n",
      "Epoch 200/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.8643 - val_loss: 17951.4824 - lr: 0.0070\n",
      "Epoch 201/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.8477 - val_loss: 17521.7754 - lr: 0.0070\n",
      "Epoch 202/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.4912 - val_loss: 17753.9941 - lr: 0.0070\n",
      "Epoch 203/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.2578 - val_loss: 17639.1250 - lr: 0.0070\n",
      "Epoch 204/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.2812 - val_loss: 17733.4824 - lr: 0.0070\n",
      "Epoch 205/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.6670 - val_loss: 17678.5039 - lr: 0.0070\n",
      "Epoch 206/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10928.2832 - val_loss: 17925.7480 - lr: 0.0070\n",
      "Epoch 207/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10925.0898 - val_loss: 18073.4629 - lr: 0.0070\n",
      "Epoch 208/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10922.5361 - val_loss: 18341.4883 - lr: 0.0070\n",
      "Epoch 209/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.3096 - val_loss: 17673.3535 - lr: 0.0070\n",
      "Epoch 210/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.4873 - val_loss: 18031.6543 - lr: 0.0070\n",
      "Epoch 211/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.6279 - val_loss: 17546.6211 - lr: 0.0070\n",
      "Epoch 212/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.3652 - val_loss: 17471.2832 - lr: 0.0070\n",
      "Epoch 213/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.1221 - val_loss: 17432.3457 - lr: 0.0070\n",
      "Epoch 214/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.1846 - val_loss: 17545.0566 - lr: 0.0070\n",
      "Epoch 215/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.1680 - val_loss: 17837.8223 - lr: 0.0070\n",
      "Epoch 216/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.1719 - val_loss: 17362.4395 - lr: 0.0070\n",
      "Epoch 217/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.5234 - val_loss: 17863.9316 - lr: 0.0070\n",
      "Epoch 218/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1680 - val_loss: 17496.7363 - lr: 0.0070\n",
      "Epoch 219/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1963 - val_loss: 17490.0820 - lr: 0.0070\n",
      "Epoch 220/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.0820 - val_loss: 17533.8672 - lr: 0.0070\n",
      "Epoch 221/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.9980 - val_loss: 17449.1406 - lr: 0.0070\n",
      "Epoch 222/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.4365 - val_loss: 17445.1992 - lr: 0.0070\n",
      "Epoch 223/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.0859 - val_loss: 17557.1660 - lr: 0.0070\n",
      "Epoch 224/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.6338 - val_loss: 17967.8340 - lr: 0.0070\n",
      "Epoch 225/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.9727 - val_loss: 17942.8828 - lr: 0.0070\n",
      "Epoch 226/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.6797 - val_loss: 17495.6230 - lr: 0.0070\n",
      "Epoch 227/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10928.8838 - val_loss: 17803.5488 - lr: 0.0070\n",
      "Epoch 228/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.0234 - val_loss: 18362.2012 - lr: 0.0070\n",
      "Epoch 229/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.5762 - val_loss: 17497.5859 - lr: 0.0070\n",
      "Epoch 230/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.2588 - val_loss: 17733.8711 - lr: 0.0070\n",
      "Epoch 231/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10932.1514 - val_loss: 18206.8379 - lr: 0.0070\n",
      "Epoch 232/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.1113 - val_loss: 17673.6484 - lr: 0.0070\n",
      "Epoch 233/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.6963 - val_loss: 17875.2070 - lr: 0.0070\n",
      "Epoch 234/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.9844 - val_loss: 17841.5645 - lr: 0.0070\n",
      "Epoch 235/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.4922 - val_loss: 17393.9531 - lr: 0.0070\n",
      "Epoch 236/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10908.2510 - val_loss: 17882.1836 - lr: 0.0070\n",
      "Epoch 237/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10926.9502 - val_loss: 18099.0664 - lr: 0.0070\n",
      "Epoch 238/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10929.2617 - val_loss: 18196.6875 - lr: 0.0070\n",
      "Epoch 239/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.2734 - val_loss: 18096.0605 - lr: 0.0070\n",
      "Epoch 240/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.5322 - val_loss: 17668.9805 - lr: 0.0070\n",
      "Epoch 241/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10906.7295 - val_loss: 18232.9043 - lr: 0.0070\n",
      "Epoch 242/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.0234 - val_loss: 17869.0137 - lr: 0.0070\n",
      "Epoch 243/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.7969 - val_loss: 18014.3711 - lr: 0.0070\n",
      "Epoch 244/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.7393 - val_loss: 17739.1055 - lr: 0.0070\n",
      "Epoch 245/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.2471 - val_loss: 17870.5195 - lr: 0.0070\n",
      "Epoch 246/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.9551 - val_loss: 17561.7969 - lr: 0.0070\n",
      "Epoch 247/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.4453 - val_loss: 17478.3691 - lr: 0.0070\n",
      "Epoch 248/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.0059 - val_loss: 17199.7695 - lr: 0.0070\n",
      "Epoch 249/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.3984 - val_loss: 17491.0293 - lr: 0.0070\n",
      "Epoch 250/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.3330 - val_loss: 17571.2500 - lr: 0.0070\n",
      "Epoch 251/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.2842 - val_loss: 17550.7656 - lr: 0.0070\n",
      "Epoch 252/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.4053 - val_loss: 17720.7969 - lr: 0.0070\n",
      "Epoch 253/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10931.6982 - val_loss: 17670.4258 - lr: 0.0070\n",
      "Epoch 254/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10926.0625 - val_loss: 17841.2617 - lr: 0.0070\n",
      "Epoch 255/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.0430 - val_loss: 18235.6133 - lr: 0.0070\n",
      "Epoch 256/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10919.7480 - val_loss: 17995.6543 - lr: 0.0070\n",
      "Epoch 257/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10922.5771 - val_loss: 18019.7227 - lr: 0.0070\n",
      "Epoch 258/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10934.1934 - val_loss: 17699.3066 - lr: 0.0070\n",
      "Epoch 259/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10909.9326 - val_loss: 17471.5879 - lr: 0.0070\n",
      "Epoch 260/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.0293 - val_loss: 17908.3027 - lr: 0.0070\n",
      "Epoch 261/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.5742 - val_loss: 17738.9453 - lr: 0.0070\n",
      "Epoch 262/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10921.5791 - val_loss: 17782.9375 - lr: 0.0070\n",
      "Epoch 263/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.3242 - val_loss: 17647.4160 - lr: 0.0070\n",
      "Epoch 264/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.4404 - val_loss: 17702.1680 - lr: 0.0070\n",
      "Epoch 265/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.5146 - val_loss: 17581.4023 - lr: 0.0070\n",
      "Epoch 266/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.8008 - val_loss: 17774.8379 - lr: 0.0070\n",
      "Epoch 267/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.9492 - val_loss: 17386.0332 - lr: 0.0070\n",
      "Epoch 268/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10920.1602 - val_loss: 17622.8301 - lr: 0.0070\n",
      "Epoch 269/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10925.0049 - val_loss: 17572.7402 - lr: 0.0070\n",
      "Epoch 270/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.9727 - val_loss: 17874.1035 - lr: 0.0070\n",
      "Epoch 271/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10914.1377 - val_loss: 17625.3828 - lr: 0.0070\n",
      "Epoch 272/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.2090 - val_loss: 17552.7168 - lr: 0.0070\n",
      "Epoch 273/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10907.0713 - val_loss: 17811.7949 - lr: 0.0070\n",
      "Epoch 274/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10905.3086 - val_loss: 17340.5254 - lr: 0.0070\n",
      "Epoch 275/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.8877 - val_loss: 17534.3906 - lr: 0.0070\n",
      "Epoch 276/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.3604 - val_loss: 18039.8691 - lr: 0.0070\n",
      "Epoch 277/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10924.7197 - val_loss: 17800.0566 - lr: 0.0070\n",
      "Epoch 278/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5020 - val_loss: 17525.7207 - lr: 0.0070\n",
      "Epoch 279/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10920.3193 - val_loss: 17920.1074 - lr: 0.0070\n",
      "Epoch 280/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10914.3320 - val_loss: 18170.3926 - lr: 0.0070\n",
      "Epoch 281/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10930.1045 - val_loss: 17886.6074 - lr: 0.0070\n",
      "Epoch 282/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.0674 - val_loss: 17550.3223 - lr: 0.0070\n",
      "Epoch 283/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.6680 - val_loss: 18107.3340 - lr: 0.0070\n",
      "Epoch 284/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.2236 - val_loss: 17781.7344 - lr: 0.0070\n",
      "Epoch 285/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.5312 - val_loss: 17600.2402 - lr: 0.0070\n",
      "Epoch 286/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.0518 - val_loss: 17737.3184 - lr: 0.0070\n",
      "Epoch 287/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10915.0938 - val_loss: 17375.2188 - lr: 0.0070\n",
      "Epoch 288/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10911.0000 - val_loss: 17520.8574 - lr: 0.0070\n",
      "Epoch 289/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10919.4229 - val_loss: 17470.5469 - lr: 0.0070\n",
      "Epoch 290/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.3926 - val_loss: 17780.8711 - lr: 0.0070\n",
      "Epoch 291/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.9482 - val_loss: 17634.0352 - lr: 0.0070\n",
      "Epoch 292/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.2754 - val_loss: 17680.0039 - lr: 0.0070\n",
      "Epoch 293/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.6953 - val_loss: 17878.6816 - lr: 0.0070\n",
      "Epoch 294/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.9219 - val_loss: 17795.1328 - lr: 0.0070\n",
      "Epoch 295/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10910.1758 - val_loss: 17600.1270 - lr: 0.0070\n",
      "Epoch 296/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.9893 - val_loss: 17578.0020 - lr: 0.0070\n",
      "Epoch 297/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10931.1895 - val_loss: 17546.5176 - lr: 0.0070\n",
      "Epoch 298/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10917.4990 - val_loss: 17658.9121 - lr: 0.0070\n",
      "Epoch 299/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.0693 - val_loss: 17887.4102 - lr: 0.0070\n",
      "Epoch 300/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.8701 - val_loss: 17668.6895 - lr: 0.0070\n",
      "Epoch 301/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10910.1709 - val_loss: 17719.5352 - lr: 0.0070\n",
      "Epoch 302/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10915.5605 - val_loss: 17765.6543 - lr: 0.0070\n",
      "Epoch 303/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10938.8135 - val_loss: 17537.9375 - lr: 0.0070\n",
      "Epoch 304/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10912.9629 - val_loss: 17897.5566 - lr: 0.0070\n",
      "Epoch 305/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10930.1572 - val_loss: 18390.2910 - lr: 0.0070\n",
      "Epoch 306/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10940.5283 - val_loss: 18113.1348 - lr: 0.0070\n",
      "Epoch 307/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10917.5537 - val_loss: 17512.6328 - lr: 0.0070\n",
      "Epoch 308/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.4375 - val_loss: 17586.9902 - lr: 0.0070\n",
      "Epoch 309/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.1494 - val_loss: 17352.7188 - lr: 0.0070\n",
      "Epoch 310/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10921.7656 - val_loss: 17584.6934 - lr: 0.0070\n",
      "Epoch 311/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10913.8086 - val_loss: 17457.1523 - lr: 0.0070\n",
      "Epoch 312/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.0459 - val_loss: 17393.1621 - lr: 0.0070\n",
      "Epoch 313/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10916.9551 - val_loss: 17705.1523 - lr: 0.0070\n",
      "Epoch 314/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10937.2236 - val_loss: 17766.5566 - lr: 0.0070\n",
      "Epoch 315/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.1543 - val_loss: 17961.2500 - lr: 0.0070\n",
      "Epoch 316/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10918.7402 - val_loss: 17791.4473 - lr: 0.0070\n",
      "Epoch 317/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10912.3379 - val_loss: 17604.5938 - lr: 0.0070\n",
      "Epoch 318/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.4541 - val_loss: 17388.9551 - lr: 0.0070\n",
      "Epoch 319/5000\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 10916.8193 - val_loss: 17792.7500 - lr: 0.0070\n",
      "Epoch 320/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10909.3799 - val_loss: 18091.4629 - lr: 0.0070\n",
      "Epoch 321/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10913.7441 - val_loss: 17818.7988 - lr: 0.0070\n",
      "Epoch 322/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10911.0430 - val_loss: 18062.6270 - lr: 0.0070\n",
      "Epoch 323/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10918.4248 - val_loss: 17872.1953 - lr: 0.0070\n",
      "Epoch 324/5000\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10907.4111 - val_loss: 17665.9121 - lr: 0.0070\n",
      "Epoch 325/5000\n",
      " 1/26 [>.............................] - ETA: 0s - loss: 10772.6973\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 0.004899999825283885.\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 10908.7363 - val_loss: 17729.6621 - lr: 0.0070\n",
      "Epoch 00325: early stopping\n",
      "run-fold: 1-1 SMAPE: 29.01398\n",
      "Average SMAPE: 34.26574\n"
     ]
    }
   ],
   "source": [
    "# Make the results reproducible\n",
    "RUNS = 2\n",
    "\n",
    "history_list, score_list, test_pred_list = [], [], []\n",
    "oof_list = [np.full((len(train), 1), -1.0, dtype='float32') for run in range(RUNS)]\n",
    "\n",
    "for run in range(RUNS):\n",
    "    model = None\n",
    "\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, groups=train.year)):\n",
    "        x_train = X_train.iloc[train_idx]\n",
    "        y_train = Y_train.iloc[train_idx]\n",
    "        x_val = X_train.iloc[val_idx]\n",
    "        y_val = Y_train.iloc[val_idx]\n",
    "\n",
    "        print(f\"Fold {run}.{fold}\")\n",
    "        model, history = training_model(x_train, y_train, x_val, y_val)\n",
    "\n",
    "        val_pred = predict_model(model, x_val)\n",
    "        #print(y_val)\n",
    "        #print(val_pred)\n",
    "        score = calc_smape_score(y_val, val_pred)\n",
    "        score_list.append(score)\n",
    "        history_list.append(history)\n",
    "        print(f\"run-fold: {run}-{fold} SMAPE: {score:.5f}\")\n",
    "\n",
    "print(f\"Average SMAPE: {sum(score_list) / len(score_list):.5f}\") # Average over all runs and folds\n",
    "\n",
    "#with open('oof.pickle', 'wb') as handle: pickle.dump(oof_list, handle) # for further analysis\n",
    "\n",
    "#if RUNS > 1:\n",
    "#    y_va = train_df.num_sold\n",
    "#    print(f\"Ensemble SMAPE: {np.mean(smape_loss(y_va, sum(oof_list).ravel() / len(oof_list))):.5f}\")\n",
    "#print(f\"Total time: {str(datetime.now() - total_start_time)[:-7]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "plot_history(history_list[-1], title=f\"Validation SMAPE\", plot_lr=True, n_epochs=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAJdCAYAAAAfoFLiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAB4JklEQVR4nO3de3jU5Zn/8c/kfCQYSEKAgAKiCAgoB8UAnjhUiFiggCLU6lrbWnWt9Yw/rbZqqavFbbWHdQtW10PVeiqNWlZpLYjKKgiNoECAkJBkBghJyGGSzO+Ph29mMkySSTKTmUzer+vyCplMZp6ZZJvP3s/9vR+by+VyCQAAAAERFeoFAAAARBLCFQAAQAARrgAAAAKIcAUAABBAhCsAAIAAIlwBAAAEUEyoFwAg8r3wwgt64YUX1NDQIJvNprPOOku33nqrBg4c2Hyff/3rX1q9erX27NmjpKQkJSUl6brrrtOll14qSdq8ebOuv/56nXbaaS0ee+nSpbryyitb3LZ8+XIdPHhQqampstlscjqdGjNmjB544AElJiZ2+nXk5+fr+eef1x//+EetXr1aQ4cO1RVXXNHq/X/1q1/pzDPP1KWXXurX/QFEBsIVgKD6+c9/ri+//FK//e1vlZ2draamJr355ptasmSJ/vSnP2nAgAHatm2bfvjDH+rBBx/UhRdeKEnavXu3brnlFtntdi1dulSSNGTIEL3xxht+Pe8dd9yhOXPmSJJcLpduueUWPfnkk7rzzjsD8rpuueWWdu+zefNmjRgxwu/7A4gMhCsAQXPo0CG9+OKL+uCDD5SWliZJioqK0hVXXKHt27frt7/9re6//36tXr1a3/ve95qDlSQNHz5cq1at0ne+8x0tWLCgS+uw2WyaMmWK/v73v0uSxowZo0suuURffvmlHnvsMSUlJelnP/uZjh49qsbGRi1fvlyLFi2SJK1evVpvvfWW+vbtq6FDhzY/5l133aXTTz9d1113nbZu3aqf/vSnqqmpUWxsrO644w7t2bNH27dv16pVqxQdHa3169c33//TTz/VqlWrmu//7//+75o+fbpee+01vffee4qKitK+ffsUGxurn//85xo5cqTeffddPf3007LZbIqOjtYdd9yhSZMmdel9ARAchCsAQbN161YNGzasOVh5mjp1qn75y19Kkv7v//5Pd91110n3OeussyRJX3/9tSRp//79mj9/fvPXs7Oz9Zvf/KbddVRUVOivf/2rLr74YkmS0+nURRddpNWrV6uhoUHz58/XqlWrNHr0aFVWVmrJkiUaMWKE7Ha73n33Xb3++utKSEjQjTfeeNJjO51O3XjjjfrpT3+qCy+8UNu3b9fdd9+tN954Q/n5+Vq2bJlmzpyp9evXS5KOHDmim2++WU8//bTGjRunr776SldffbVeeeUVSdInn3yit99+WwMGDNBDDz2kZ555Rj//+c+1atUqPfbYYxo/frw+/PBDbd68mXAFhCnCFYCgamho8Hl7fX29bDZbh+7XkW3BVatW6emnn5Z1wtdFF12kFStWNH994sSJkqTCwkLt379f99xzT/PXamtr9a9//Uu7d+/WzJkzlZKSIklauHCh/vjHP7Z4nl27dikqKqq56jZmzBi99dZbra5r27ZtGjJkiMaNGydJOv3003XOOefo448/ls1m0+jRozVgwABJJly+9957kqS5c+fqhz/8oWbMmKELLrhA119/vV/vA4DuR7gCEDTjx4/Xvn37VF5eroyMjBZf27x5syZMmCBJOuecc7R582aNGjWqxX22bdum2NhYDRs2TJ9//nmHntuz58qXpKQkSVJjY6P69OnTIrTZ7XalpqbqF7/4hTyPX42Ojj7pcaKjo1uERMkErmHDhvl83qamppNuc7lcamhoUGxsrBISEppvt9lszc9/6623atGiRfrwww/12muv6Xe/+51ee+01RUVx0TcQbvi/SgBBk5WVpeXLl+tHP/qRSktLm29/9dVX9e677zZXX2677Tb913/9lzZs2NB8n927d+vuu+/WLbfcovj4+KCt8bTTTlN8fHxzuCopKdG8efO0fft2TZs2Tfn5+Tp27Jiampp8Vs2GDRsmm82mf/7zn5KkHTt26Nvf/raampoUHR19UkVu3Lhx2rt3r7Zt2yZJ+uqrr/TJJ59o8uTJra6xoaFBF198sY4fP64rr7xS999/v3bv3t1qtQ9AaFG5AhBUt912m/70pz/p+9//vurr61VfX6+xY8fqxRdf1KBBgySZ7a9nnnlGq1ev1sMPP6zo6Gj16dNHN910U5vVp0CIi4vTU089pZ/97Gf6r//6LzU0NOiWW27RueeeK0nauXOnFi5cqD59+ujMM8/UkSNHTvr+//zP/9TDDz+sVatWKTY2Vv/5n/+puLg4XXTRRfr5z38up9PZfP/09HStXr1aDz30kGpra2Wz2fTII4/otNNO02effeZzjTExMbrnnnv04x//WDExMbLZbHr44YcVFxcXvDcGQKfZXJ41bwAAAHQJ24IAAAABRLgCAAAIIMIVAABAABGuAAAAAihirxasra3V9u3blZGR4XM2DQAAQGc1NjaqvLxcY8aMaTGfTorgcLV9+3YtW7Ys1MsAAAAR7Pnnn28+8cESseHKmgb9/PPPNx8lAQAAEAiHDh3SsmXLTjp9QorgcGVtBQ4YMECDBw8O8WoAAEAk8tV6REM7AABAABGuAAAAAohwBQAAEECEKwAAgAAiXAEAAAQQ4QoAACCACFcAAAABRLgCAAAIIMIVAABAABGuAAAAAohwBQAAEECEKwAAgAAiXAEAAAQQ4QoAACCACFcAAAABRLgCAAAIIMIVAABAABGuAAAAAohwBQAAEECEKwAAgAAiXAEAAAQQ4QoAACCAghquVq9ercsuu0xz587VH/7wB0nSxo0blZeXp1mzZumJJ55ovm9BQYEWLlyo2bNn695771VDQ4Mkqbi4WMuWLdOcOXP0/e9/X9XV1cFcMgAA6KEcDoduueUWHT16NKTrCFq4+vjjj/XRRx/pzTff1Kuvvqo//vGP+vLLL3XPPffoqaee0rp167R9+3Zt2LBBknT77bfrvvvu0zvvvCOXy6WXX35ZkvSTn/xEV111lfLz8zVmzBg99dRTwVoyAADooRwOhxYvXqy3335bX331VUjXErRwNXnyZD377LOKiYmRw+FQY2Ojjh07pqFDhyonJ0cxMTHKy8tTfn6+Dh48qNraWo0fP16StGDBAuXn58vpdOqTTz7R7NmzW9wOAABgsYJVYWGh1q5dq0mTJoV0PUHdFoyNjdWTTz6puXPn6vzzz1dZWZkyMjKav56ZmanS0tKTbs/IyFBpaamOHDmilJQUxcTEtLgdAABAOjlY5ebmhnpJwW9ov/nmm7Vp0yaVlJSosLDwpK/bbDa5XK4O3Q4AACBJNTU1amxsDJtgJUkxwXrg3bt3q76+XqNGjVJiYqJmzZql/Px8RUdHN9+nrKxMmZmZysrKkt1ub769vLxcmZmZSk9PV1VVlRobGxUdHd18OwAA6N0qKiqUmpqqwYMHa/369S3yRagFrXJVVFSklStXqr6+XvX19Vq/fr2WLl2qvXv3at++fWpsbNTbb7+t6dOna9CgQYqPj9eWLVskSa+//rqmT5+u2NhYTZw4UevWrWtxOwAA6L0cDocWLFig//f//p8khVWwkoJYuZoxY4a2bt2qK664QtHR0Zo1a5bmzp2r9PR03XTTTaqrq9OMGTM0Z84cSdJjjz2mlStXqrq6WmeddZZWrFghSbr//vt111136emnn1Z2drYef/zxYC0ZAACEOc8eqwceeCDUy/HJ5vLV2BQBioqKdMkll2j9+vUaPHhwqJcDAAC6yDNYrVmzRtOmTQvZWtrKGUxoBwAAYa+pqUkrVqwIi2DVnqBtCwIAAARKVFSU7rjjDkVFRYV1sJIIVwAAIIw5HA599NFHmjt3rmbMmBHq5fiFcAUAAMKS1WO1f/9+TZkyRf379w/1kvxCuAIAAGHHu3m9pwQriYZ2AAAQZsLpqsDOIFwBAICw8te//rXHBiuJbUEAABAmXC6XbDabrr76as2YMUM5OTmhXlKnULkCAAAh53A4tGjRIn3xxReS1GODlUTlCgAAhJhnj1VFRUWol9NlVK4AAEDIeAartWvXKjc3N9RL6jLCFQAACInDhw9HXLCSCFcAACBEkpOTdfrpp0dUsJLouQIAAN3MbrcrKipK6enp+s1vfhPq5QQc4QoAAHQbu92uJUuWKDk5WW+88YZsNluolxRwhCsAANAtrGBlDQiNxGAl0XMFAAC6gXew6omT1/1FuAIAAEF3xx139IpgJbEtCAAAusFDDz2k/fv36/zzzw/1UoKOyhUAAAgKu92uX/ziF2psbNSgQYN6RbCSCFcAACAIrB6r3/zmN9q1a1eol9OtCFcAACCgvJvXR40aFeoldSvCFQAACJjedFVgawhXAAAgYPbu3avS0tJeG6wkrhYEAAABUF9fr7i4OE2aNEmbN29WcnJyqJcUMlSuAABAlzgcDl122WV67rnnJKlXByuJyhUAAOgCh8OhxYsXq7CwUEOHDg31csIClSsAANApnsGqN/dYeSNcAQCADqutre31VwW2hm1BAADQYQkJCVq8eLFGjRpFsPJCuAIAAH5zOBw6ePCgzj77bH33u98N9XLCEuEKAAD4xeqxOnz4sDZu3KjExMRQLyksEa4AAEC7vJvXCVato6EdAAC0iasCO4ZwBQAA2vT0008TrDqAbUEAANCmO++8U1dccYXGjBkT6qX0CFSuAADASex2u773ve/JbrcrNjaWYNUBhCsAANCC3W7XkiVL9N5772n37t2hXk6PQ7gCAADNrGBl9VhNmTIl1EvqcQhXAABA0snBiub1ziFcAQAASVJjY6NiYmIIVl3E1YIAAPRyR44cUWpqqrKysvTXv/5VUVHUXrqCdw8AgF7Mbrdr0aJFuuOOOySJYBUAvIMAAPRSnj1WCxYsCPVyIgbhCgCAXsgzWK1du1a5ubmhXlLEIFwBANDLuFwuXXvttQSrIKGhHQCAXsZms2nlypWqr68nWAUBlSsAAHoJh8OhV155RZI0efJkglWQULkCAKAXcDgcWrx4sfbt26cLLrhA2dnZoV5SxCJcAQAQ4axgZfVYEayCi21BAAAimHewYisw+AhXAABEsPfff1/79u0jWHUjtgUBAIhALpdLNptNixYtoseqm1G5AgAgwjgcDs2fP18ff/yxJBGsuhmVKwAAIohnj1VdXV2ol9MrUbkCACBCeAarNWvWaNq0aaFeUq9EuAIAIAIcOXKEYBUmCFcAAESA1NRUjRs3jmAVBui5AgCgB3M4HGpoaFBWVpYef/zxUC8HIlwBANBjWT1WMTEx+utf/6qoKDakwgHhCgCAHsi7eZ1gFT74SQAA0MNwVWB4I1wBANDDrFy5kmAVxghXAAD0MA899JBeeOEFglWYIlwBANADOBwOPfTQQ3I6nerfv78mT54c6iWhFYQrAADCnNVjtWbNGu3cuTPUy0E7CFcAAIQxz+b1tWvXasyYMaFeEtpBuAIAIEx5B6vc3NxQLwl+IFwBABCmDh48qMOHDxOsehiGiAIAEGZqamqUmJios88+Wxs3blRiYmKol4QOCGrl6le/+pXmzp2ruXPnatWqVZKku+++W7NmzdL8+fM1f/58vffee5KkjRs3Ki8vT7NmzdITTzzR/BgFBQVauHChZs+erXvvvVcNDQ3BXDIAACFlt9s1b948/e53v5MkglUPFLRwtXHjRn344Yf685//rNdff107duzQe++9p+3bt+u5557TG2+8oTfeeEMzZ85UbW2t7rnnHj311FNat26dtm/frg0bNkiSbr/9dt13331655135HK59PLLLwdryQAAhJTdbteSJUtUWFioUaNGhXo56KSghauMjAzdddddiouLU2xsrIYPH67i4mIVFxfrvvvuU15enp588kk1NTVp27ZtGjp0qHJychQTE6O8vDzl5+fr4MGDqq2t1fjx4yVJCxYsUH5+frCWDABAyHgGKyav92xB67k6/fTTm/9dWFiodevW6X/+53/08ccf68EHH1RSUpJuuOEGvfLKK0pKSlJGRkbz/TMzM1VaWqqysrIWt2dkZKi0tDRYSwYAICTq6+u1dOlSglWECPrVgl999ZWuvfZa3XnnnRo2bJh+/etfq1+/fkpMTNTy5cu1YcMGuVyuk77PZrO1ejsAAJEkLi5O11xzDcEqQgQ1XG3ZskXXXHONbrvtNn3zm9/Uzp079c477zR/3eVyKSYmRllZWbLb7c23l5WVKTMz86Tby8vLlZmZGcwlAwDQbex2uz755BNJ0tVXX02wihBBC1clJSW68cYb9dhjj2nu3LmSTJh6+OGHVVFRIafTqZdeekkzZ87UuHHjtHfvXu3bt0+NjY16++23NX36dA0aNEjx8fHasmWLJOn111/X9OnTg7VkAAC6jdVj9Z3vfEfV1dWhXg4CKGg9V88884zq6ur06KOPNt+2dOlSffe739WVV16phoYGzZo1S/PmzZMkPfroo7rppptUV1enGTNmaM6cOZKkxx57TCtXrlR1dbXOOussrVixIlhLBgCgW3g3rycnJ4d6SQggm8tXY1MEKCoq0iWXXKL169dr8ODBoV4OAACSuCowUrSVMzj+BgCAbrRmzRqCVYTj+BsAALrRrbfeqrlz5zIkNIJRuQIAIMgcDoeuu+46FRcXKzo6mmAV4ahcAQAQRA6HQ4sXL1ZhYaH27dungQMHhnpJCDIqVwAABIlnsFqzZo3OP//8UC8J3YBwBQBAEHgHK5rXew/CFQAAQWCz2ZScnEyw6oXouQIAIIAOHz6s5ORkpaen64033uBM3F6IyhUAAAFit9v1rW99SzfffLMkEax6KcIVAAAB4Dl5/eqrrw71chBChCsAALqII23giXAFAEAXuFwu3XDDDQQrNKOhHQCALrDZbHrggQdUUVGh3NzcUC8HYYDKFQAAnWC32/Xcc89JksaOHUuwQjMqVwAAdJBnj9WMGTOUk5MT6iUhjFC5AgCgA7yb1wlW8Ea4AgDAT1wVCH8QrgAA8NPmzZu1f/9+ghXaRM8VAADtaGpqUlRUlObOnaspU6aof//+oV4SwhiVKwAA2uBwOJSXl6cNGzZIEsEK7aJyBQBAKxwOhxYvXqzCwkJFR0eHejnoIahcAQDgg2ewWrt2LXOs4DfCFQAAXioqKghW6DTCFQAAXlJTU3X++ecTrNAp9FwBAHCCw+FQTU2NBg8erJ/+9KehXg56KMIVAABy91g1NTXpb3/7Gw3s6DTCFQCg1/NsXl+zZg3BCl1CzxUAoFfzDlZMXkdXEa4AAL3agw8+SLBCQLEtCADo1R588EFdffXVmjRpUqiXgghB5QoA0Os4HA6tXLlSNTU1SktLI1ghoAhXAIBexeqxeuGFF7Rz585QLwcRiHAFAOg1vJvXx48fH+olIQIRrgAAvQJXBaK7EK4AAL1CeXm5jh07RrBC0HG1IAAgoh0/flyJiYk688wz9eGHHyo+Pj7US0KEo3IFAIhYDodDeXl5+uUvfylJBCt0C8IVACAiefZYMWoB3YlwBQCIOJ7Bau3atcrNzQ31ktCLEK4AABGloaFBV155JcEKIUNDOwAgosTExOj73/++MjIyCFYICcIVACAiOBwO/etf/9K0adP0zW9+M9TLQS9GuAIA9HhWj1VJSYk++ugj9enTJ9RLQi9GuAIA9GjezesEK4QaDe0AgB6LqwIRjghXAIAe64UXXiBYIeywLQgA6LFuvPFGzZkzRyNGjAj1UoBmVK4AAD2K3W7X1Vdfrb1798pmsxGsEHYIVwCAHsNut2vJkiXatGmTSkpKQr0cwCfCFQCgR7CCVWFhodasWaOpU6eGekmAT4QrAEDYczgcLYLVtGnTQr0koFWEKwBA2IuLi1O/fv0IVugRuFoQABC2HA6HEhMTlZqaqpdeekk2my3USwLaReUKABCW7Ha7Fi9erO9973uSRLBCj0G4AgCEHc/m9euvvz7UywE6hHAFAAgr3lcF0mOFnoZwBQAIKz/84Q8JVujRaGgHAISVBx98UOXl5brgggtCvRSgU6hcAQBCzuFw6Pe//71cLpdGjhxJsEKPRuUKABBSDodDixcvVmFhoS655BINGzYs1EsCuoTKFQAgZDyD1Zo1awhWiAiEKwBASHgHK5rXESkIVwCAkPj8889VVFREsELEoecKANCtGhsbFR0drUsuuUSbNm1Senp6qJcEBBSVKwBAt7Hb7brsssv017/+VZIIVohIVK4AAN3Cc/J6SkpKqJcDBA2VKwBA0HGkDXoTwhUAIKgqKysJVuhV2BYEAARVSkqKLr30UuXm5hKs0CsQrgAAQWG321VRUaHhw4fr7rvvDvVygG4T1G3BX/3qV5o7d67mzp2rVatWSZI2btyovLw8zZo1S0888UTzfQsKCrRw4ULNnj1b9957rxoaGiRJxcXFWrZsmebMmaPvf//7qq6uDuaSAQABYPVYLV++XE6nM9TLAbpV0MLVxo0b9eGHH+rPf/6zXn/9de3YsUNvv/227rnnHj311FNat26dtm/frg0bNkiSbr/9dt13331655135HK59PLLL0uSfvKTn+iqq65Sfn6+xowZo6eeeipYSwYABIBn8/rPf/5zxcbGhnpJQLcKWrjKyMjQXXfdpbi4OMXGxmr48OEqLCzU0KFDlZOTo5iYGOXl5Sk/P18HDx5UbW2txo8fL0lasGCB8vPz5XQ69cknn2j27NktbgcAhCeuCgSCGK5OP/305rBUWFiodevWyWazKSMjo/k+mZmZKi0tVVlZWYvbMzIyVFpaqiNHjiglJUUxMTEtbgcAhKdVq1YRrNDrBX0Uw1dffaVrr71Wd955p4YMGXLS1202m1wuV4duBwCEp/vvv19/+tOfCFbo1YIarrZs2aJrrrlGt912m775zW8qKytLdru9+etlZWXKzMw86fby8nJlZmYqPT1dVVVVamxsbHE7ACB8OBwO3X777aqqqlJycrLOOeecUC8JCKmghauSkhLdeOONeuyxxzR37lxJ0rhx47R3717t27dPjY2NevvttzV9+nQNGjRI8fHx2rJliyTp9ddf1/Tp0xUbG6uJEydq3bp1LW4HAIQHh8OhxYsX67XXXtOuXbtCvRwgLARtztUzzzyjuro6Pfroo823LV26VI8++qhuuukm1dXVacaMGZozZ44k6bHHHtPKlStVXV2ts846SytWrJBkSsx33XWXnn76aWVnZ+vxxx8P1pIBAB1gBavCwkKtXbuWihVwgs3lq7EpAhQVFemSSy7R+vXrNXjw4FAvBwAiinewys3NDfWSgG7VVs7gbEEAQIdVVFSotraWYAX4wPE3AAC/VVZWKiUlRcOGDdMHH3zAgFDABypXAAC/OBwOXXHFFXr44YcliWAFtIJwBQBol2ePFVdtA20jXAEA2uQZrJi8DrSPcAUAaFVjY6OWLVtGsAI6gIZ2AECroqOjdeuttyopKYlgBfiJcAUAOInD4dD//d//aebMmZo9e3aolwP0KIQrAEALVo9VUVGRNm3apPT09FAvCehRCFcAgGbezesEK6DjaGgHAEjiqkAgUAhXAABJ0uuvv06wAgKAbUEAgCTp2muv1cUXX6zTTjst1EsBejQqVwDQizkcDi1ZskRffvmlbDYbwQoIACpXANBLefZYORyOUC8HiBhUrgCgF/IMVmvXrtUFF1wQ6iUBEYNwBQC9zOHDh1sEq9zc3FAvCYgohCsA6GUSExOVk5NDsAKChJ4rAOglHA6HYmNj1adPH61ZsybUywEiFpUrAOgF7Ha7Fi9erOuuu04ulyvUywEiGuEKACKc3W7XkiVLVFhYqJtvvlk2my3USwIiGuEKACKYZ7Bi8jrQPQhXABDBbr31VoIV0M1oaAeACPbQQw+puLhYU6dODfVSgF6DyhUARBi73a4nn3xSTU1NOvXUUwlWQDejcgUAEcSzx2rOnDkaOXJkqJcE9DpUrgAgQng3rxOsgNAgXAFABOCqQCB8EK4AIALs3LlTJSUlBCsgDNBzBQA9WENDg2JiYnTBBRfoo48+Up8+fUK9JKDXo3IFAD2Uw+HQN77xDb366quSRLACwgSVKwDogRwOhxYvXqzCwkJlZmaGejkAPFC5AoAexjNY0WMFhB/CFQD0IMePHydYAWGObUEA6EGSkpJ0+eWX65xzziFYAWGKcAUAPYDdbld5eblGjRqlW265JdTLAdAGwhUAhDlrQGhFRYX++c9/Kj4+PtRLAtAGwhUAhDHvyesEKyD80dAOAGGKI22AnolwBQBhavXq1QQroAdiWxAAwtS9996rRYsWady4caFeCoAOoHIFAGHEbrfrpptu0tGjR5WQkECwAnogwhUAhAmrx2rdunX66quvQr0cAJ1EuAKAMODdvD5p0qRQLwlAJxGuACDEuCoQiCyEKwAIsbq6OrlcLoIVECG4WhAAQqSiokKpqakaNGiQ3nvvPUVHR4d6SQACgMoVAISAw+HQggULtHLlSkkiWAERhHAFAN3M4XBo8eLFKiws1GWXXRbq5QAIMMIVAHQjz2C1du1a5ebmhnpJAAKMcAUA3aSpqUnf/va3CVZAhKOhHQC6SVRUlO644w5FRUURrIAIRrgCgCBzOBzauHGj8vLyNH369FAvB0CQEa4AIIisHqv9+/frvPPOU0ZGRqiXBCDICFcAECTezesEK6B3oKEdAIKAqwKB3otwBQBBkJ+fT7ACeim2BQEggFwul2w2m5YtW6YZM2Zo8ODBoV4SgG5G5QoAAsThcGjhwoXaunWrJBGsgF6KyhUABIBnj1VlZWWolwMghKhcAUAXeQarNWvW0GMF9HKEKwDogsOHD7cIVtOmTQv1kgCEGOEKALogJSVFZ5xxBsEKQDN6rgCgExwOh2w2m9LT0/XUU0+FejkAwkir4Wr58uWy2WytfuOzzz4blAUBQLizeqySkpL05ptvtvm/lQB6n1bD1dVXXy1Jeu+991RVVaWFCxcqOjpab7zxhvr06dNtCwSAcOLdvE6wAuCt1XA1e/ZsSdIzzzyjF198UVFRpj3rwgsv1JIlS7pndQAQRryDFT1WAHxpt6H9yJEjqqura/68urpaFRUVQV0UAISjO++8k2AFoF3tNrTPmzdPixcv1syZM+VyuZSfn6/Fixd3x9oAIKw89NBD2rdvn84777xQLwVAGGs3XN1yyy0aM2aMNm3aJEm66667NGPGjKAvDADCgcPh0DPPPKPbbrtN2dnZys7ODvWSAIQ5v+ZcZWRkaMSIEbrjjjtoZgfQa1g9Vr/97W+1c+fOUC8HQA/Rbrh69dVXdffdd+u//uu/VFlZqR/84Ad6+eWX/X6CqqoqzZs3T0VFRZKku+++W7NmzdL8+fM1f/58vffee5KkjRs3Ki8vT7NmzdITTzzR/P0FBQVauHChZs+erXvvvVcNDQ0dfY0A0GGezetr167VWWedFeolAegh2g1Xzz33nF566SWlpKSoX79+eu2117R27Vq/Hnzr1q268sorVVhY2Hzb9u3b9dxzz+mNN97QG2+8oZkzZ6q2tlb33HOPnnrqKa1bt07bt2/Xhg0bJEm333677rvvPr3zzjtyuVwdCnYA0BnewYqzAgF0RLvhKioqSikpKc2fZ2dnKzo62q8Hf/nll3X//fcrMzNTknT8+HEVFxfrvvvuU15enp588kk1NTVp27ZtGjp0qHJychQTE6O8vDzl5+fr4MGDqq2t1fjx4yVJCxYsUH5+fideJgD4b+/evSorKyNYAeiUdhva+/btq4KCguZBeW+++abS0tL8evCf/exnLT53OBw677zz9OCDDyopKUk33HCDXnnlFSUlJSkjI6P5fpmZmSotLVVZWVmL2zMyMlRaWurXcwNAR9XV1Sk+Pl4TJ07U5s2blZSUFOolAeiB2q1c3XPPPbr99tu1e/du5ebmavXq1Vq5cmWnniwnJ0e//vWv1a9fPyUmJmr58uXasGGDXC7XSfe12Wyt3g4AgWa32zV37lz98Y9/lCSCFYBOa7dyVVtbqzfeeEOFhYVqbGzUaaedptjY2E492c6dO1VYWNg8/d3lcikmJkZZWVmy2+3N9ysrK1NmZuZJt5eXlzdvMQJAoNjtdi1ZskSFhYU69dRTQ70cAD1cu5WrH//4x4qOjtbw4cM1cuTITgcryYSphx9+WBUVFXI6nXrppZc0c+ZMjRs3Tnv37tW+ffvU2Niot99+W9OnT9egQYMUHx+vLVu2SJJef/11TZ8+vdPPDwDePIMVk9cBBEK7laszzjhDb731ls4999wWZfK+fft2+MnOPPNMffe739WVV16phoYGzZo1S/PmzZMkPfroo7rppptUV1enGTNmaM6cOZKkxx57TCtXrlR1dbXOOussrVixosPPCwC+1NbWEqwABJzN5auxycPYsWPldDpbfpPNpoKCgqAurKuKiop0ySWXaP369Ro8eHColwMgTP3+97/XmWeeSbAC0CFt5Yx2K1dffPFF0BYGAKFgt9tVVFSk8ePH6/rrrw/1cgBEmHbDVWNjo1588UV9+OGHio6O1sUXX6wFCxZ0x9oAIOCsHiuHw6FNmzYpMTEx1EsCEGHaDVcPPfSQdu/erfnz58vlcumVV17Rvn37dOutt3bH+gAgYLyb1wlWAIKh3XC1ceNG/eUvf2m+SvDyyy/X5ZdfTrgC0KNwVSCA7tLuKIZTTjlFjY2NzZ/bbDb16dMnqIsCgED77W9/S7AC0C3arVwNGzZMV111lRYsWKDo6GitW7dOp5xyiv7whz9Ikr7zne8EfZEA0FV33HGH5s+frzFjxoR6KQAiXLuVK8nMutqxY4e2bdumwYMHq1+/ftq1a5d27doV7PUBQKc5HA5973vfU3l5uWJjYwlWALpFu5WrRx55pNWv/ehHPwroYgAgUBwOhxYvXqzCwkJ95zvfaXEIPAAEk1+Vq9bs3bs3UOsAgIDxDFZr1qzRlClTQr0kAL1Il8IVAIQb72BF8zqA7ka4AhBRGhsbFRcXR7ACEDLt9lwBQE9w5MgRpaamKjMzU3/5y18UFcX/7wggNPhfHwA9nt1u16JFi3TbbbdJEsEKQEi1+79AN910kzZu3Ojzay6XK+ALAoCO8Jy8vmjRolAvBwDaD1ezZs3SU089pdmzZ+uZZ57R0aNHm7/2xBNPBHNtANAmjrQBEI7a7bnKy8tTXl6edu/erVdffVXf+ta3NH78eC1fvlxnn312d6wRAE7icrl07bXXEqwAhB2/Gtqbmpq0b98+FRYWqqGhQf369dMDDzyg888/X7fffnuw1wgAJ7HZbLrvvvtUV1en3NzcUC8HAJq1G66eeOIJvfbaa8rJydFVV12l1atXKzY2VsePH9dFF11EuALQrex2u95//31961vf0qRJk0K9HAA4Sbvh6vDhw/r973+vM888s8XtSUlJ+o//+I+gLQwAvHn2WF1wwQUaOHBgqJcEACdpN1w99NBDrX6NUjyA7uLdvE6wAhCuGAYDIOxxVSCAnoRwBSDs/f3vf9e+ffsIVgB6BI6/ARC2XC6XbDabFixYoKlTp2rAgAGhXhIAtIvKFYCw5HA4dPnll+ujjz6SJIIVgB6DyhWAsONwOLR48eLm2XoA0JNQuQIQVjyD1dq1a7kqGUCPQ7gCEDaOHj1KsALQ4xGuAISNlJQUTZgwgWAFoEej5wpAyDkcDjmdTg0YMECPPfZYqJcDAF1CuAIQUlaPVXR0tPLz8xUVRUEdQM9GuAIQMp7N62vWrCFYAYgI/C8ZgJDwDlZMXgcQKQhXAELivvvuI1gBiEiEKwAh8dBDD+mFF14gWAGIOIQrAN3G4XDowQcfVH19vfr166fJkyeHekkAEHCEKwDdwuqxWrt2rXbu3Bnq5QBA0BCuAASdd/P62LFjQ70kAAgawhWAoOKqQAC9DeEKQFAVFxfryJEjBCsAvQZDRAEERU1NjRITEzV27Fj985//VGJiYqiXBADdgsoVgIBzOByaN2+efvOb30gSwQpAr0K4AhBQnj1WY8aMCfVyAKDbEa4ABIxnsFq7dq1yc3NDvSQA6HaEKwABUV9fryVLlhCsAPR6NLQDCIi4uDhdd911ysnJIVgB6NUIVwC6xG63a8+ePZo8ebKuvPLKUC8HAEKOcAWg0+x2u5YsWaLS0lJ99NFHSklJCfWSACDkCFcAOsUKVlaPFcEKAAwa2gF0mHewoscKQLtqa6Xt28zHCEe4AtBha9euJVgB6Jivd0mffmw+Rji2BQF02L//+79r7ty5OvPMM0O9FCDy1NaaADJipJSQEOrVBM6IkS0/RjAqVwD8Yrfb9Z3vfEcHDx5UdHQ0wQoIlkit8CQkSGPObhkYI3SrkMoVgHZ59ljt379fgwYNCvWSgMjViyo8zUFSMsErQhCuALTJM1itWbNG559/fqiXBEQ2q8LTE3V0SzNCgyTbggBa5XA4WgSradOmhXpJQEsRuq3UY3V0S9PXVmEEoHIFoFVRUVFKTU0lWCF8WX/MD5VIuTMi7o90jxOhlaiOIlwBOMnhw4eVnJysU045RX/+859ls9lCvSTAtxEjTbAqOmCCVk/dTosUPXlLM4DYFgTQgsPh0Le+9S3ddNNNkkSw6gy2qrpPQoKpWE2c3OurJQgfhCsAzRwOhxYvXqzCwkItX7481MvpuSL1UvqO6q6QGaF9O+i52BYEIKllsApoj1WkDkRsC30nRoReZg+0h3AFQC6XSzfccENwrgrsjX9g6TsxCJnopQhXAGSz2fTAAw/o6NGjgT8rsLf+ge2NFTtvhEz0UvRcAb2Yw+HQs88+K0kaM2KEcvv2CXx/TG/th4mUviua84EOo3IF9FKePVYXXnihhhw72vu274IpUip2vXFbF+giwhXQC3k3rw8ZMkSqzTRfDGQY6M1bY5GyJRbqkNibf4fQY7EtCPQyrV4VGIztu0jZGuvNQr2ty+8QeiAqV0Av8/HHH2v//v2+rwoMdJUg1FUP9Hz8DqEHIlwBvURTU5OioqL0jW98Q5s2bVL//v1PvlOg+2siZWsMocPvEHogtgWBXsBut2vevHl6//33Jcl3sJJMdaC7jxHhajQAEYZwBUQ4u92uJUuWaOfOnYqJaadYHYr+Gnpq4A9COHqQoIarqqoqzZs3T0VFRZKkjRs3Ki8vT7NmzdITTzzRfL+CggItXLhQs2fP1r333quGhgZJUnFxsZYtW6Y5c+bo+9//vqqrq4O5XCDiWMEqKJPXAyUU1bKeqLeHC0I4epCghautW7fqyiuvVGFhoSSptrZW99xzj5566imtW7dO27dv14YNGyRJt99+u+677z698847crlcevnllyVJP/nJT3TVVVcpPz9fY8aM0VNPPRWs5QIRp6KiIryDlRUWpN45ZLSj2goXvSF4tRXCe8PrR48StHD18ssv6/7771dmppmds23bNg0dOlQ5OTmKiYlRXl6e8vPzdfDgQdXW1mr8+PGSpAULFig/P19Op1OffPKJZs+e3eJ2AP5JTU3VBRdcYILVpEnh98eHSkTHtBYuamulDzdE7nvZVgi3vlawI3JfP3qkoF0t+LOf/azF52VlZcrIyGj+PDMzU6WlpSfdnpGRodLSUh05ckQpKSnNPSLW7QDaZrfbVVNTo5ycHD344IPmxu3bwm/Kdlcvse9twyVbu2ru611S0QFpcE73ba1253vf1hWs1tfGTWBrGWGl20YxuFyuk26z2Wwdvh1A66weq4aGBq1fv97dwB6Os4K6eok9x7IYnj/b7gqZ3fnet/W7G4rXDvih28JVVlaW7HZ78+dlZWXKzMw86fby8nJlZmYqPT1dVVVVamxsVHR0dPPtAHzzbl5vcWVgIGcFhUvFKBSBsaOvPRjvlfdjhmIOVHe+9229PmZgIUx12yiGcePGae/evdq3b58aGxv19ttva/r06Ro0aJDi4+O1ZcsWSdLrr7+u6dOnKzY2VhMnTtS6deta3A7gZN16VWC49Er1hLERnW1Cb+tr4fD+h/pIHCDMdVvlKj4+Xo8++qhuuukm1dXVacaMGZozZ44k6bHHHtPKlStVXV2ts846SytWrJAk3X///brrrrv09NNPKzs7W48//nh3LRfoUX76058GN1h5VkvCcYuxu7T32r2rSm3d359eIl9f683vP9BD2Fy+mpsiQFFRkS655BKtX79egwcPDvVygKCqqKjQrl27NGnSpOA8gdUQP3FyYLdhwmWLMVA68j619dq7833xfC4psn4eQBC1lTM4WxDooRwOhx5//HGtXLlSaWlpwQtWUvCqJd3VGN1dYaUj71Ooe4ms98TplLZ+5r6diwSALuP4GyDc+ei/cTgcWrx4sV588UXt3Lkz+GsIVo9Nd01n70yfUmcGU3blfQr0IMz2Hs8z2Fo/A6blAwFB5QoId17VHStYFRYWau3atc0DeHuk7rraywoLOUNM4PCnghWsqlprVbRAP197j+drjEE4DZkFejDCFRDuPP4Ieger3Nzc0K6tp/RMWSGuI8NUu3srNBDP15ELD3wFW2aHAQHBtiDQWd11npnHVpPdbldlZWV4BCspPMYC+NLaz6Yj216eW3y+Hq+zP/+cIWaaes6Qlo8jdX3r1fPn0ZktSrYFgYCgcgV0ViD+v3w/Kz/V1dVKSkrSGWecoQ8//FBxcXGde75AC8exANZZe0UHzOeeP5vObkP6+ln78/P39fM9sN+sbUC2lNY3sNWirv48GMoJBAThCuisQAQLP/6wWluBc+fO1Y9+9KPwCVbhqq2z9jq7jen9s66tNVfZjZvQ9s/f18/X+7ECGVAJR0BYIFwBnRWIP2Tt/GH17LEK2qiFrvRNhWOPjmfzuvfr6ux6vX/WX+8y4wsmTm77PfP18/V+rM7+HvWUfjegFyJcAR0VyD9qbfxh9QxWQT3SpisBKVBVl2C8p76a1wO1Xn8fJ5iVpHAMtgAkEa6AjuuGP2oNVVW68ptXqPBgcfDPCuxK4PAVHjoTlILxnvpTNeqscNh+C8d+NwCSCFdAx3X0j1onwkZM4R7dOPEcpV//by2DVTC2ggIdFDoTlAI9hiAhITwCUDBF+usDejDCFdBRHf2j1oGw4XA4tGPHDk2fPFnzr73u5LDh72OFsh+nI0HJc51dDQpskwUXPV6A35hzBXgL9PwqP2cHWT1WN3z3u6r4/P98/xHzdw6R9/wpz9cUqNfX2uN0ZL6SP3Oy/Fmvv1fvdZX3WtpbW3fNQusO4TrTDAhDhCv0br7++AX6j0h7wyjVsnn9d/ferbQv/+X7+f0NLt4hzPM1Ber1Fewwj/PF1s4HN3/CorXeDze0/hzW1XuxsSe/N+2tqeKo9M466ZPN5t/+nMdnvXftvZfdEUjaen2BDHcMGAX8xrYgejd/5hB1RHtbJz6ez+FwaPGiRSrct09rfv97TbvgAvdjdJb31qWv19TV12c57JBKit2fe76+9t6PhATzNc/H9Ly/VZHKHmhmV1khxftnljNEOlRiPno/Z8EOE7ycTmnCuS1fS8EOafdXUlWVeQ0VR30PH7XuX1Nj1mJNV2/vd2XESPO8Tqf5/q5up/l6P9vaDg3kVik9XoDfCFfoXbz/OAX6irKOHJZ7wosvvmiC1bKlmpaV0bXnby3MeD+m9W/r/n37msrNBdOlzCz/Xt+o0aZSlDNE2rPbBIhhw1u+Ps/7WyEqZ4iZUm6t0fM+Usv3z6pIZWWZUNO3r7RjuzR6bMufmefUc+/HaOu1bP3M/XlysnT2ePMYnsNCrffz613Sji/cz5fWt+X76uu9tz5az+MZ7nzxfgx/gmJbAa8zB1YD6DLCFXoPX8eiBPr/G+/EYbk/+MEPNPvCCzXC1dj1P4KthbvWQtcXW01giIuT6uulf/5d+ua3Tn5c6/szs9zn4nm+lthY87yxsS2fN2eIdLDIVHysYPDVTqmiwr3GtqpqI0aaipT1MzvskOrqpOPV0thx7rU5nSZwOZ3SoMEmiNXUmK8NGy457Oaj9Tpyhri/x14mlZZK1dXSZ1tahkvvcFhTY9ZgVa78ee99vY/+Vjb9ecy2foc7c2A1gC4jXKH3aOtYFE9duSrKz7DmcDh0yy236MEHH9SwYcM0YvRo84WO/hGsOGoqTpOmmEpKa9tQrW2NHXaYj8kpkq1aysj0vX1lfX9GplReJtXVSgMHm+qVdwXQ8/07sN9st5UUm8CTPdD8O7WPO/y0VlWzpPWVjh6VqipNsIqONuGsYId5LVYFKi3N3F64x3y0tiqL9pvPPataB4vM18dNkC6aaR6rrNS9Vsk8tufrSkiQYmLM13ftlBITW/6OtBasR40++X309Tp9PYb3R6ta2NEtXWZiAd2KcIXew/sPZWuCfEm/Z/P6oUOHNGzYMN9rbC3ked7+0Ubzx76xUZp9mbmfZxXJ2s5qaDDf29DQsjI24VxTBcrMknYWSF9/ZSo45001W32SqQTt+frE4u3mY3m5+a+s1Nz3wH73cTNOpztAWGHPCi6eAWvHF6ZqdNFM3z8P70pjbJzkrDevVTKPWVtrnsMKS6l9TJBK7SMNGWrCY0WFCV6ewaKmxh2irPehtlZ6/z1TxbLeL+9tv7JS82/PPjPr6979Y55bg9bPJHugCXSeFUqp5fe0dTROZyut9EsB3YpwhZ7P30qTv39gAt2E7MEzWK1du1ZTp05tuVXl2YtkVbEOlZjKlPU1q/pxsEhKSjYPfKzCVLHiE1qOJLCCYny8dOYoU3n59GNp55cmfMTEmPBx6jB36CkpNtUwK9R8+S9TMYqJMaEjLl6KjTEhrKRY2vihVHrIhLPKSlPdspq+reBScVT627vm/qPHmgbyymMmyPz1LenimWbtBTvMc44a7a40xseb53fWm8dN7+cON1/vMj/TzCzzeUKC1NRoHlsywc+q7Fk/R6vR3qo8ef7+DBho1iSdvD379S7zHINzWv48PLUWzD0DYGaW+d7W+sz8xdwpIGwRrtDzBaPS5LCbP+zePURdeciDB02wOnRIa9euVW5urvmCtX7P3iIr4FlXyUnuj5aSYilrgPl3dbUJL8OGm+CVnGIe79xJ7nDy1S5T3dqz24SPHV9Iw083YWHYcPPfhxukmloTxBobzXPU1UkpKZItSqqukqKjzPPFxUv1dSboZGW5Q0l5mflohZo9u03Fxwo8MTHSpbNM1e3YMRPuPtlstu2sipfn1ldmlumFSu9n+qysRu+CHe4APGq0+2dmOewwW4qXzj75h+EZtK0Qe7DIPMe4Ceb2tq4iTUgwj22NOmjrAgnr+awA6P1Ylo5u2TE0FQhbhCv0fIHuJ/HVmxWAKkH8wQPKjInWT35yvwlW3s3VDQ1Sv/7mj/Bbr5sgc8Yosy2XmWXCTk2NNPIM94M2NJiqkWTCy6ES05xeXWX+2/a5qUrtLDDfv+VjE4SsoLO/0Dx3v/4mnJSXm9u/2Cb1PcVUxBISTcCyvqemxgS2wUPMGAOnU6pzml4oa8suOtq8hxUV7u9LTZWGnOru05p9mbtn7OzxJtyMHut+ji+2miDWJ83c12K9b5J7ttWYs6XcGSZw1daadZ831b8fjGfTfEmxe5aTd2+TrysDPbdA27tAwrNfqr0+M3/X7fkRQNggXKHnC+YVf23NEmorcHl8zVFdrcTERKWcepr+55YfypY7w9zHulLvy39JQ08z/5442QSi6ipzn4NF0rz57i0pq/IRc+L/dK2gVVZqeqesoCWZHqWzx5sgtGe32VaTTXI43PexqmNWT1RysqlKNTWZQCaZz6WW4amuzmwNxsRKDU6p4ojkcpl12aKkceNMQLOCVXKK2TK03ktrfINkQpHVlD5uwskVKO/qofWzGDeh5VDLhAT3duKEc/0PwQkJ7mAm+Q4/3lpbQ3vPE8jfU/qogLBFuELv1FYw8vVHy3OkgNWH5bmdlzvD/Tgejdj2o0e15P6faODAgXp2xTLZjh0z4enS2e4r9aqqpF1fmmCTmWWe68gRE7CqKs0f/YYGszVXVSXt3+cOLWUnmq+trTjJVKEqj5nRCvsKzfM4602T99izzfahJSrKVIZ2FpivZ2aZqo8VqKz7NDWZ11ddLSUlScePS0cOm2AlmWAlSTabea6dX5oAJrn7pHZ84e6V8twCtWZlHSySig+a15KRKcklRUWfPPagtQsTfI3a8JfVG+Yvfy+OANArEa7QO3W0X8VzpMD+QhOOrCblogPusQCS+XfRAdldLi25/U4VlpXpgQcekO3ssabPKK2vOxQlJpmAYjXQf7bFbAPOmuOu7Eju4ZUJCSY49esvxUS3nIweFSWdfoYJR/X15rayUnfwSkkxAzg9NTVJR4+4m9mt0CaZoORytQxWkqlMSVJDowmE8YnS4RNXETqd5sq8jExTqUpONttz8QnuhvjMLNPf1a+/+R6r8d7ztVgjD6z3Pq2v+2utXZVnbedmDwzKxQgttFY1oskcgDhbED1dZ89OGzGy9e0cX485YqT5oy2Z0PDJZvPHM72fua2hwf19DQ2yV1VpyX8+pcKSEq1Zea+mTZtmAkZTowlKf//AhJ6a41J6unRKuqngJCWb0Ldnt/l81GjzX3y8eR6rGlRZKaX0Mf9OSHBXl6qrTJiJjTNfq6o0PVZZA0xYaWoyr+OMUe7X1vcU3++RVY2KjzevLSXFVK2qq6SUVLN9V11tKlUZmea+cfFmyntyivm8utqEI2scwY4vzMe0vu7hnscq3AM9R481W23nTTUfrcGg3mf++Tqzz/qZZmaZLcZQHDDM4cYAROUKPV1nrpjyd0q25wiEnCHmj3ZamtkOs64UszQ0uLekxk3QTa+9ocLDR7Rm+VWaNma0CQY1Ne4r6mqOm6pOUpK5zepnqqo0AaGmxj1u4byppim9+KB0yilmW7C+TjpU7H49ktnWi08w1bE+qaa3qqbG/Jc1wDSUlx5yh0rP176zwISu+nr3LKvERBO8SordAzqbuaQRp7u3KJMSzc31daZaNmy4ef/kcm/reZ7/J7nHPdjLzWsYnGO2V62fgdU/9enHLfuwvKe6e14YIJnn7sygzUCgyRyACFfo6Tr6x8yfvhzPq8ck89H6fOJkE2C2fma2rhpPDJu0l5sAkD1QGjVaDz78sMre/19dkJvr7jOyKl+SCVLV1aYalNrHvR3X0GCe//Mt5vOSYul/3zPBJibWPN+pw8yE9Noadw/U6Sca260r11JS3X1Lsrkb3a2jazznSVkaGqS9u1t+npllqnMNDeb5a46b7cKqKvNeWtuP/TPNjKiGBlNp2rPb/ZwH9pv3zApT/fqb8HP2eLMlWVVl3gdfBzNbP9ecIS3P/PM1SsHzZxaqRm+2CwGIcIWerqNXTPl7BE6//u4+n379TR+U1PIqN8ldzamtkb2qSq/+45/67sBBOl1NOj37RHN4eZkJVudNlT78u/k8NtaEkNJSE5Zqa832msNuwl/diWqUzeZ+jgan+a/koNketGZcnTbchKSCHWZu1YF9pgKWmtoy1KWmuitxVgg7VGK2FK2Gc09DTj1xaPKAllchZg80711Dg3mPYmLM+7Ptc/O5ddXf6LHuc/g833fJhKHBOaZ6V1Vles/6nmLuG5/gfq89A4ln35Wn1gKYv7oj+DCTCuhVCFfoXfy5ysuagG4Nxhw91gQHqyoyaYr7CrcTx6TYh56mJU9cp8KSEl3aN1XDTzvN3NcaqXDsmKlOHT1qPo870RPldErFRWYra2eBub3ogKk8SabvyRoCahk8xGwPWkGvtlb6y5vm8QfnuKtJfdJM6MkZ4j4mZ9dOE4ZGnG5ClhWaGhpMIPp6l6mGZWVJZ5xpjqep93ju5GR3n9mOL8z3xMaa+VmlpSYgWtuOVpO6NUzU8/23tvmyskxYs65Y7NfffaafrwqjryDkGbBbC2Bt6Y7gw3Yh0KvQ0I7exfpD7Dk2Yfs2d8N0xVH3+XHVx83H/fvMH/nUPubjrp3m9j27pR1fyJ6coiU3/lCFhw5pzdVXavjw4SYgpKV5TFCvkv6x4cSsKZkAk3ji6Jr6erOtGB9v/p2WZipPVgDr08fdMG6FnkMl7te0s8AEq5QUE8KSk1u+1rS+ZntPMmFp62cmCNXVmVEHkmm0Lys1W3tZA8w232dbTNXM2eB+rj5p7grXxMnm46cfS7KZf5+SbgJjwQ7znNZ79tHGlufoTZpigmBpqQmA3kHXu8Jo/ZwKdrTfMN7Rixy8L27o7EUSbfH+vQMQ0ahcoXezqhbWuW/WR6uB2+qH8pzVZC8zoWDE6bJHRWnJvfepsKhIa37/e01zNZhQsPUz8/0Nje7nGjjIzIeyl5ueK8+KkNVEnpZmztmzKk3x8WZq+rgJ5vkPHpA2/K97LIJVqUlJMaHKaphPO1G1so5msaaDW031Vtjq39+EiKYm9+iDtBMBKjnFhKX6OrPdmJJiKmye5x/W1rqreAf2t5xY7nnkTUmxqUT16+/+ujU81Ao1nk3oVvO7dSag1VflObSzte28jlaivLeWre93OltOVAcAPxGu0LtZf8xrakwA6JNmrtw7e7wJBzU1JmhkZpmAkphoqkqlpdK+fdpWsFMHDx7Umt/9TtPS+5r+obg4E6zi493bgpIZU+B0uqec22zur2Vkmu260WPMNprVd1RXZwJeQ4O5WrDqxOMlJLiPjJFMEEpJNdW2xAQTyLZ8Yrb9amrMmXySmeiemGjWsfsrE34mTjZhZuOH5vErKk7MtaqS6k7MprKXSdNOXMnnPXPKc0vO+wrKkmJTbYuKdjeze16t2NoRMAf2m/sPyDaP62s717OR3XOIa1e34KzvczrpkwLQKYQr9EyBakK2BlIW7HAf2LuzwN0Ybd1unYc3YqS04X/V2NSkaGe9Lj5zpDa98JxOSUxy/yG21NW17Jcq3OPRiO6S6p3ukQy5002IeGedCSTWnKr4ePchy9YsrJgYE5gOFpl1WUfXWP1Tw0eY43T2FZrPy0pP7l+yQpB1dZ9kHru6ylSuJk0xA0cTEiS73cyusg5Lltzvhy+eFajExJbbgd7/bu0xvAOSrwsXPK/q/HqX++tdPRbG+n6rKkefFIAOoucKPVMghzVa59pZxk1wB6utn5k/4NaBwF/vkuPrr3TZ7/+gv/zrS8nl0ilfFphqljU405KUZMJURoa7Hyq1jzQ114SnI4fN/aqrTf/WZ1uaG+SbRzRMm+G+2u/IYROEamrM59YEcqvBvF8/UwE7cMBcMdgnzdze1HSieXyAeV0VR83WovVcWz+TNv7jRF9ZqqlcHT1qthX37jE9XplZ7vfJc0Cnd3+S58/Fs8/I89/+/Oz86VGyzgT092w/X+ttC31SADqJyhWCK1iXuQfy6ivPbaCtn5k/1p5rLT1k/ouNleOUflr8/EsqLD2kPhMnSfU1JqR8eeKQ4+yBpjm8tNSMOJDMllhiktlyS0gwQarogAlYffqYKlltrfug5Ph4d/AqOdG43jddOnrYBKi6OvNYxQdbNuBHRbc8Y7C62qzHOv/v+IkG/U82u4/ySTrR/G41pA851V0Zk9xB03qfrCqXZwXKc+vMn5+L93268jvSmVEcbPUBCDLCFYIrWH/Murr1Y/H8wy65t4GsysYZo8ygy/4ZJlitWGGuClx2paaddqp7eGhDg9nem3Gx+b6vd0l9+5ogk5RkKkCSCT+ZWe5m8LpaMwuqweOKPGvrMCrKPH7pIffxN/VOafpF0j//bipMf33bowrjMv1NzkaprsYEsKoq6bRh5ricymNmXWeOMluKLpd0vNpckXf2eDNuYuQZLedgeQZNX4cbW4HL8yy/9n4urTWQS21/rz8hrL37MBIBQDcgXCG4OvvHLNiDHa3Hr6w0FaOqSncTueTeAhucI5UeUlVDgxbfcZcK9+83VwVmZZiA4jlc05pNZSkvNwHIamCPjTWVoX2F5vnj4twBKPnEuX3Hj5tJ6CmppuH8jFHm+84cZYJaRYXZPhw8RGrYa3qkoqJMZUs291yuslL3FYXFJ4aOWmMNPtzgPjcwK8v0V1lXJ/brb/qpnE53P1ZbByBbZwZ++rH56B2O/Pk5+vs74k8Ia+8+gQrlANAGwhWCq7N/zDpa8epoGLMeP/XE4ccHi0zQkkzj+QXTzZZYdZUUH69ke7kuHXm6cleu1LRLLjHbcQf2m2qVdYxN6SHTy2QdHGyNDbACnNNpQk/ViedJ6+ue1m6FJMls41lbfccq3NWwU4eZ/i9rSy8j03xfU5PU6JQOn5jkXlbq3h602Uywyh5onq9gh6lSOZ2SXNLUaeZ1WGMYJHeFyroaTzp5JEHFUfdwUM/xCdboB+t+/vwc/f0d6cyWIwCEAOEK4amjfySt5nOn8+StK4tnALPmKJ05ynxfdbW5Gq+62lSHtn0uDciW/fPPdLSmViMyM3T3+ZOl9L7msT7aaMJUSooJUXW1pu+qpNg0mA/OMTOhrPEEMTFm+y8m1j37SS6z/VdVadbd1GRujo2TppxnrtYrKZbef8/0U5UUu8OgZEJcZpbZOkxJNVUvp9P0fFmPW11tvtbU5B7+6bCbPq+tn7lnVlm9VJ5nDbY1ksA6J1CSLp1tbvcMY9b9Ahl2OrPlCAAhQLhC9/OnyhSIP5Lez/PFVhMwKitNxccKBzU1JphYDd9xcVJsrOxbP9eSP/xRx+vqtOHff6i4GI//c7Eawa0DjK3XkZLqHjLa2Og+9Lhovwk6sbHmflHRZtvQEhPrbjx31ptGditsWYNBk5NNMMvINANJrXEInqHGeqzSQ6ZaVV1tQlZVpfuqQ1/zpjwDqef71tpIgklTzOuzwqM1pkJqeT/vnyMHGAPoBQhX6H7WVpH38MeO8vxDbU0g99ya8tySGjHSPffpYJEJG2lpJ67ai235uPX1sn+xTUv+e60KDx/RmuVXmWCVkWnGKvz5T2q+us56PTlDzeNYQSYtzb1958mqEDU1mh4rm80EoH79WvZvWQHNl5gY83qt1z5ipAmIZaVSlE06d5L5t3X2oNUsHxNz8oR17/fRc1SC5B5F4B100/qaUQ2ffmxmWbV2P29crQegFyBcofu1NvyxI2prWw7GtKolu3aa6pTT6d7isoKW1esUG2uavutqTeXJCjwZGdLx47KXlrqD1YplmjbsxCHMAweZKeZWhcvS2Gj6tCypqaay88H/trwK0Jv1ONkDpfOmmsravkIzn6pvuglXyckmtERFm8BWXW3u6x1SEhPdfVYHi9yVKCuEWWMmHHYTaCXfIdTfcQqeX+/Ilh89UQB6AcIVup81/NFzBEJH1NaaxnGrBylniLvnyjq0uPigO9gU7DDBxGYzV8kdOWzWYFWVoqNNQBo4WKqt1arnX2iuWDUHK8mEr22ft7222FhTkfIcCOopKcls99XWmSpTU5MJWX9508zCcjrdPVHW+YbWVX/lZWYrzzp+5lCJee2SeR+tcxE93ycrgI6bYPrArEArtazqeX70d0u2M1u33dETxdYjgBAjXKF7effztHcfX1tXBTvcIaLymPRevgkmUssg4jlQMzraBCsrYDU0mKvvSg5KY8+Wdu6UDh+WDpXogcvmaOmUyTone0DLdb2/3j3CIDpaskW5+6QkU12KiTEVpxiv/9OKiZXiYltWvZpOPFbFUfOxvt58TEkxTeoVFWYrUi5JNtO8njPEBDfryJvGRnP7sOHuSe3Wgc1OpwlT1iHOkum18gy0vs74C4buDDxsPQIIMcIVupevfivvQZ6+zsGzKlUHi9whwmLNikrtY8JWSqo7TBw9bEKMFbRSUs19ystM0Kmrkz77P9krKvTo2j/qgblzlBIfr3OmTTPPVVvr/h7PeVVxXocyS6aPypps7l21anC2DGK+2Gxmey821t1/VVvjHhERE+Pe9pRa9nU57O73zJoQ71mt2va5+eg5FLS9mVT+niXoj+4MPGw9AggxwhW6l69+K+sPr7UlVnTAPfBSalmpskYdZGW1bPiOiTFX+fXrbx4jJsYElZ0F7uGeGZnuY2Ws75Fkr6ho7rG6cuI5OnfYaVK0xxEwNTVmBpV19Z41kTzQXK6WVy1KJlhFRZvA5Lnll5IiDRhoholajeoVFSYEVh93HzYtuc9FzB5oKl+tVZG8A5DnmYu+BoR2RHcGHsYxAAgxwhW6l69+K895SlawmjTFfR+rChQbZ8YU2MullD4tw1VDgwlV1iDOPV9LF13q7luSzNetgCRJxypkr6rSkj88q8IjR7VmxTKdOyTHPNaerz0eu4NBKj7ebPFZW4hd1dRogmNKH/O4KSmmUrezwFSirCBx6Szpf99zv16r6iS5zy38ZLPp2bKa/j1HMHj/PHydJdhZBB4AvQjhCp1nVUByhpx8eX9bPP/QelZR6mpNAJo0xTyeVc2yn9jSc9absGQdpOzNs7pUU2N6sZxOU3XxHNJ5gr2y0n1V4LXXaNqQwe4vdqUyVVfX9tejo01Ysipjnmw28zqsLUjJfF5fLx22u2/LHmh6raxzEK338Bt5La8OlEwVa9wE06dVdKDlY3uyZlV5VrVaG8gKAGgV4Qqd59k/5dkjZfGnidlzK0oyjzMg2101KT7o7pdK62sCmC8220nhqTkgtRKUKuvqVNfQaK4K9AxWlpiYtkcpdFbjid4sq7nek8vVsrfL6WwZGiUzfX3Gxe731HOIqFVhGjbcfX+rX8pXGPZGMzgAdBnhCp3neaacFYg8eQ/x9BW0fPXiWPepq215xd+xita32jqwBVdZW6uU+Hid1q+fPrjlRsVER5sveIedhobgBSzvwGR9bouSkhLNVY/DRkiHit3bfFkDzPvs3VyeM0Q6sM8Eppoa95af93mAnhVDa5yDN5rBAaDLCFdoW1vVp/b+WHv+oW6tIuLdi+MZwg4WtXy8APQwOaqrtfiZNbrw9BG67xuz3cGqtcdvbDr5NsnM02psNO9PZySnmNdeXmYa7dPSpK+/klxNJlgNzjFbcnWjzdWTNbVm8npmlvl+z5/Lgf3u/rOoKNOH5es8QH/QGwUAXUa4Qtu8Q1FH5hV5/qFurSJiXe7vWSXa8YWZVB4X53vrrJOsYFV4+IguGnm6f9/kaiVcVVdLCYkdX4R15Z+ryfSW/fPv5mN8vAlIlcdMP5U1puLrXe4zCD/bIs2+zPzbuypYU2MOhj5vqvu8P+/zAAEA3YJwhbZ5h6LO9uT4qoh4H2EjuQOLtR2Y2sccW9PFgOUZrNYuv0q5w4d16fEkmRlUrYmONsfYxMSYq/vKTlSWqqvNezH9IjN7qqLCfByQbYJVWpoJSFZwHTHSbPmVlrac7+X5c0lIMAHNExUoAAgZwhXa5mvbzvOjt9YqW94DKetq3WMDklOkmuMnjoU5EVisHqTKY11+CY1NTbp6zXOBDVbx8a1fFWizmS1D65gdyT05PjVVGnKqCV5WIDp7vKnUpaSa92PPbvdVegkJ0kUz3Y3o1nmAhCcACFuEK/jWWkhq7496a5Utz4GUDrsJJhUVZuvKe9K5dPKVf10QHRWlWy++UElxsYEJVpLvKxCtLUyXy5xD6HCY9zEqyjSjV1WZoaA7vpD27zNzqS6dbQKTNXXd4v3+jzm75VWB7QUrztcDgJCJCvUCEKaskGQd8muprTV/5Ftr5B4x0jRUW5Ut6/45Q8yspawBZhvQ2vYLxpV4Jziqq/VuwU5J0qxRZ3QuWNlsLT+PiTFbfr7Cn8vlHmJaW+e+T2KSef3VVeZ7JVOR+2Sz+XfOELP9KZl+q1Gjfb//3u9tW1r7+QEAgo7KFXxrbfvPV2XKV5XFUrDDVKwOFpnZTA0NLQeABmqKuRerx6roaIU23XaL0pOTO/dA3uvzFQZjY802YFOT+S852YSn6BipscEEJ6fTffahZP599nj3AcuVx8wVglYju6/3v7Xhq74qU4xUAICQIVzBt9a2/3z90fanyb2kWHr/by23AG02cwBya4NBO8m7eb3TwcpfVjXLqubV1ZuPjSeCWHmpVLhHGj3WXNGXlCwdrzZ9Vju+cE9Q99TaBQBWoGrvPacnCwBChnCFjvH1RztniJnSnjPk5PsPG26mrB894q5YRUebSo/LJSUlBTRcOWrrAn9VoMV7oKh11qF1jI3Vc9XgNK+r3nni38lmq8+qMv3tHfchzNY2nxWWrAOSfVWmvMcveH4EAIQNwhW67sB+9zgFa1ur4qj00UYTLqw5TZLpSRo42JyTl5RkmrwD6I3PPgtOsJJO3hKMjpIUa8ZHVFe17MM6ftx8TEmV5DKN7F9sNQHtzFFmG3DCue6hoP6MvPAev0BlCgDCEuEKJ+volWYjRrrPFyzYYaovB4vc1RnJ9CG5ZLbDDuwzt1VXm7DRJTaZBza+c94UXTxypE7tl97Fx21HbKx7G9DzKBurKieZipXNZsKlZ8AcnGOulCwrdYcrf0ZeEKgAoEfgakGcrKNXmiUkmIrVxMnm808/NmEqPl7KGWqugJtxsRmQKZlgYuny1YKu5h6rgkOlstlsgQ1WCQmmwpboNY19yKnm2Jrk5BPVKZnXawUrSaqvNxUq6wrC2FjTdzVpSvtX/VlBijEKANDjULnCybyrJv5UsqwwYB27cmCfmWVVdsh8PF5tqjUJCZ0/j88Hz+b1w9ZWXCBZa80eaEKWw2E+P15txklkDzQVuuho8zpT+0gDB0lF+92DQ/v1N/d1Ok1IS+vb+sHJAIAej8oVTuZdNelIJctzjIBkAkd8fMtg5T07qpM8g9Wa5VfpgmGndewBPCtobUntYwLU0aPm8/h4afQYc4WfVXlrbDSvr/KYmcJuVbOSk83W3+ix5v7WlPUABkwAQHghXKF9HRleKZkQVlpqQobkDli1tSd6r7o+2+qwV7Ca1tHm9eho6fwLpPT+vr8+aLAZeDp6rDRthruXymYzr+fLAhPOPM9AvHim+306d5LZBh08xIxbSEw0DewH9jPcEwAiHNuCaJ9VjfLcGmxrqzBniGlo75PmHsFgncNnbZV1UWJcnIamp+uBud/oeLCSTFAq+Jc0bJi5cjEqSoqLM7c7ndKxY9Lcy81r277N3Us1eIjkajJ9U/EJ7mNwhg03wSlniHlfnE5TrTt1WMtgyggFAIh4hCv4x3s0gPfMJc+gtWe32UYrKT4xHNNlKlkB4KiuVkxUlNISE/XfV1/ZsW9OTjYjEqzK2ZHD0rjx7qqUVVmzJqZbB003NJjm9fIyKTnJbPnFJ5jXah2wbJ37Z101OW6CO1R15GxGAECPR7iCfzwrLrW1JoCMm3DytPCcIdLXX5l/Z2WZ4ZlSQMKV1WN1SlKS/nTdNbJ1tHfLu2rW0CD9Y0PLK/wGDDTN6un9zOfWYdPJKeZjRYXZEjxU4p7pJbnfn5wh0oBsDkwGgF6Mniu0fxiz1LLJ/etdJnRYDeGeQeuTze4jbppcpvozbLjUr1+XlujZvH7LRTM6HqxaU1dnXpMtSjol3fy7pNj0SI0aba4GlMxrGpwjnTfVfCw60LJvynp/0voyQgEAermQVK5WrFghh8OhmBMDJB988EHt379fTz/9tJxOp6655hotW7ZMkrRx40Y98sgjqqur0ze+8Q3deuutoVhyZPPnbEDPHivPKpYVtMZNMEEqJcVUeaqrzDZaeZkJbl0IQ95XBXaqx6pPmnSswv255+DPpibTR3XksAlFVlCUzJV+6f3MsNNRo90zvaz3AgAAL90erlwul/bs2aMPPvigOVyVlpbq1ltv1Wuvvaa4uDgtXbpUU6ZM0eDBg3XPPffoj3/8o7Kzs3XDDTdow4YNmjFjRncvu2fyd9K6P03W3gHMCmHWuYK1tdLOAnObdUDxoRLT3+S57WbJyvJ7q/DWV1/vWrCSpBqvGVieFyxmZJq1pqaZqtWgweb9+myLOzhavVUSfVMAgDZ1e7jas2ePbDabrr/+ejkcDi1evFjJyck677zz1LdvX0nS7NmzlZ+fr8mTJ2vo0KHKycmRJOXl5Sk/P59w5S9/KlKSf2HBO4BVHDVbgCkpZossI9Pc3q+/VLin/asCO9CD9dN5l6no6FFN7egcK0/WVX2W+Hip9sSBy2VlJgAmJkhDJ5j7+juHqqNHBQEAIl6391wdO3ZM559/vn79619rzZo1evHFF1VcXKyMjIzm+2RmZqq0tFRlZWU+b4efOjqfyuLdg+UrQHyy2YSqg0Xmc2s6en1dQMYtOKqr9cv3N6ipqUlD0k/peLCy2czRO5K5AtBTXJwJVtbtznozk+q8qaaPbOtn0ocbTK/YxMnupnxfOnpUEAAg4nV75WrChAmaMGGCJCkpKUmLFi3SI488ou9973st7mez2eTyMWwyYI3MvUFnt6+8xyxs+F+zXVZZaXqpJk2Rzhwl2culM86Uvthmbk9OlmL8nHreBs8eq8tGj9LIzMyOP4jLJfXpY5rPk1PMlmVSsum1qqo090lKNkEwa4A0Ndc9p8oapzAg2/+KnjV5nQoWAPR63R6uPv30UzmdTp1//vmSTA/WoEGDZLfbm+9TVlamzMxMZWVl+bwdQWYFjJwh0hdbTbCS3OflHTks1TtNxWfr5+4tt+rqLletPIPV2uVXdS5YSeYcQMmEpNFjTYAqPWRuS042wcoyINs9OV1yN6znDDF9V5K7md2bFWCtOVcS/VgA0Mt1+7ZgZWWlVq1apbq6OlVVVenPf/6zfvGLX2jTpk06fPiwampq9O6772r69OkaN26c9u7dq3379qmxsVFvv/22pk+f3t1Ljhz+jFyQTNAoOmA+2svdt1tVqepqE6wkd7CKju74eqJa/vp5B6vczjSvJySYLb6puaapXjKHRx8+7L5PdbX7SsbsgSY4jRhpGtet12ONldj6mfmvvW2/zm7BAgAiTrdXri666CJt3bpVV1xxhZqamnTVVVfp3HPP1a233qoVK1bI6XRq0aJFOvts8//9P/roo7rppptUV1enGTNmaM6cOd295MjR1lR1q68qM8v0UY0ea75WWemu+FQcbf2xfV0R2B6vbd+dpWU6dKyy88FKMmf8lZeZQZ8lxea1lZ8IiDabacCvrHRPZU/v565SOewmVFrzu4oOmPCVmdV+aOIKQgDACTaXr8amCFBUVKRLLrlE69ev1+DBg0O9nPDg2ZhesKPlmAFrWystzUwhT0szBxF/+Hf34cTtsQJLBzkbGxV7ovJVWVurVH97lmJjTaXJZjNBLT5eumSWCYcNDaZyVVJsxj4cOWqa7ePizUfJHCydlGzCozUY1ApT1lmB9FABAHxoK2dw/E1v4BmqWquuWJWZpCRp44cmYP3lTam+3v/n6USwsldVaekfntUNuVP1rQnj/Q9WkjT0VBP80vpK+/eZaetWj1RJsQlK4yaYbb8vtko7vjAVquPVUlOjGQdRWWmC1aQppvfK6XRPn6cSBQDoBMJVb2BVqZxO9zDMUaNNdaes1Gz3xZ8INV/tMrdHRXUsWHWCvapKS/57rQoPH9GAPn069s3JJ670q6hwV5ZSUtzN96l9Wg4EHTvOHGmTM8R9VeCe3ea+VrN6Wl8TRGNj6Z0CAHQa4ao3279PqjwmfbTRhJBPP5bOGGUqO/X1Uk1N0J7aM1h1aPJ6SooJiZlZ0t49JkSdkm6qUINzzJZgZaU0ZKgJU1ZIsnqirKnrnkHTE71TAIAuIlxFktamhY8afXI15utdJlhJ5tw962tOp6kGRQXvQtKa+vrOBSvJDCttajJT1SXzGuKHm+2/shPbfGlp0sgzTCUKAIBuRriKJK0dd+OrGjNipKnyWFfUWfepOCpt+9x9qHEQJMbFaf7ZYzUhZ3DHzwpsajKN8zEnRj8kp0gZGWZsgtWIX1FhPs+dcXIzuq+gCQBAABGuIsmIkabyZJ2N11pzuFXhOm+qu/9o+zapb1/p7x+cNCIhUOxVVSqrqtJZAwbo5gvbmVdmXQFofZTcx9VYvVaSmQz//nrTTJ/ax1zhaB3N8+GGkwMW234AgCAjXEWShARTlfn0YzOzyVflRnJXuA4Wmd6lXTvNlXTWaIMgsHqsjtbU6p8/ulkJse0ck2MFKutjfLwkmxTt9SvrOf5hyFCzFZg7wwSrogOmmd+qVDFSAQDQDbp9QjuCwHPy+oiR7plNH27wPY3duk9JsWnuLjtxGHaQg1Xh4SP65aJvth+sfGlqMlWqY0fN5+n9zYyqlFRT0Rp+uhQT467Y5c4wE9MlDlYGAHQrKleRwLvXyrNy8/Uu39tg/fqbMFV6SFLw5sh2+qpAb0NPNecdxieYeVQxMSYYNj9RmbT7K/d8Kmv7j9EKAIBuRuUqEnifa+dZubH6qawKVm2tCV5bP5NOOcVMKW/oxNE1fnpywz86H6ySU8wxPKPHmn8PPc1sd8bEmMb0rCz3/SoqTDXO+/VaIYstQQBAN6FyFQl8NWlbt1nH2hwqMYGrYIepaGVlSQcOmK22ILpn1qVaOO5sjRs8qGPfGBMjxcWZilT/TBMGrTDV0GBe39Rppnn97PFma9M6L9HXFZMAAHQTKleRxrP/SmrZg1Www91fVVUdtGDlqK7WTX96VUeOH1dCbGzHgpXNZj42NEhHDpvhoGUnzv47Jd187bDDvL4D+83rKit1V6e8q3gAAHQzwlWksSo3X+9yj1yYNMXd3F1SbGZBBTFYLX5mjdbtKNDX5faOP4DnGAhrkGl1tQlRRw6b8wJLit3DUq0gZYVKiW1AAEBIsS0YCTwns1sVG19bZFZzd2aWtP5dc9BxAFnByuqxmjR0SNceMCZWqq8zU9klU8WyDmO2RitYW3/W9qfEdiAAIKSoXEUCz2qVZ+BwOk0QsZq8JROs/ve9oAerTl8VaImNNc32GZnm86ws8591MLN3ZYrtQABAmKByFQk8q1WWr3eZJvCJk01vklXV+Wqn79lXXVTrbJCkrgUrz2nskrkyMCPT9FtNmiLt2W2qVyc9eStnKgIAEAKEq0hgNXJ7BgwraOUMMRPYsweaqlVlpdlmC9DA0KM1NUqNj9egvml694ffV3R7Bz7HxJhmdV88w5Xn+ooOmI+TpvieWcUVggCAMEK4ihTeAcPaHvxsiznaRjJHxRQdMGfwBSBcWQNCJw0dokfn57UfrKS2g1VTkwlPcXGmiV2SomwmGBYdMMNDfYUnX5U7AABChJ6rSOGr56i21sy3kky/0pmjzJWCE84xFaQu8Jy8PnfM6M4/UHS0mRbvWbE6dZjpFcseaLYBM7Pa7qdiUCgAIIxQuYoUvgaJfr3rxPE2MoM4P9lsJpnv2W2mne/+qlNP1eUjbRIS3H1fjY2mUiWZSevVVSb4TTiXXioAQI9EuIpkI0a23P6rqDBX4KX1lQ4e6NRDulwuXfPH/+naVYGeDfXx8VKfNFOdGjbcNN97HuNDDxUAoIchXPVU/lR1EhJMBci6f2ysaWi3erA6wWaz6c5Zl0pS568KjIuXBg4yVwNWHpN2Fphtv7S+5mBmqlUAgB6MnqueynO2VVtqa01Te8EOc+XgwaJOPZ29qkpvfrFdkglVHQ5WMTFS375m6np9ndTglOZebg5lzh5o1ib5/7oAAAhTVK56qraukPOsalnzriRzBl9VZYefyuqx2nf4iM47dagyU1M7vt6GBunoUfPvhARz2PLXu0zoKik224FpfbnyDwDQ4xGuehrP4NRaP5LnWAarWtUnTbKXd/jpvJvXOxSsYmKl+BNjFeLipEGDzfrPm+oebDpuQssrAemzAgD0cISrnsbXeYHePUqe1Z+CHaYyVFnZ4apVl68KjIs1wSo52Xzs19+9ZutonlGj3evm6kAAQAQgXPU03ttmvqaTWxPaC3a4p5t3YjvwvS93du2qwKYm89EWZXqrnE6p4qgZCVF0wFSsPEMUk9YBABGAcNXTeG+beYctq/pTWWmuwvPkfXZfK1wul2w2m66ceK6mjxiuQX37dm6ttbVm1EJVpQlVRQekwj1mJMTgnJP7qjxfC1UsAEAPxdWCPU1trbR9m3tWlPd0cqv6s/vrk7/Xj2DlqK7Wgt//tz47YK4qbDdY2WwtP7eOwLE+xsaZqtWkKSZQWcEqd8bJocnztXDVIACgh6Jy1dO0t3U2YqRpFrcms3eAo7pai59Zo8LDR1RdX9/6HT0rYC6XdEq6Ga9QXe3eChw1Wirab8JUxVEzv6pff/PfqBPH5Wzf1npliqsGAQA9FJWrnsY6QzBniAknFUdbVrLqaqUjhzv8sJ7Bau3yq5Tr2WNls7WsUHlXwI5Xm2CVkmI+T0mVRp4hXTzTVKkmTXGPhIiN9a8yxXmBAIAeispVT2OFju3bTDg5VGJ6mQ4WmSNkykqltqpOPhw+frz1YCW5w1RSkmlKdzrNgcuNjWZO1bQZZoaV1edVVWma1nNnSJfONt8b7+NKRs+PAABECMJVT1RbK9XUmMnmZ483Iaek2PyXnNzhh0uJi9OoAVn6ydxvnBysvDmd7oOX09JMdSqtrzR4iAlUkqlcFR0wVSnPKxg9tzGZZwUAiFCEq3Dn66q5r3e5zwccNNhUrEqKzZZbdbXfD+04cd9+ycn61eJF7X/D8ePm6j/vYGWJOfHrNPRUKTGRqhQAoFciXIU7X0NDnU5zBZ5k/j1suPn3VzvN536weqwS4+L01g3/Jpv3VX+ePBvY6+pMsKqocB9ZYxk12gQ8xicAAHoxwlW48zU0dOtnpqldMsErNtac3Xf8uF8P6dm8vmb5VW0HK6llA3tsrJTeT0pKdh+2bGGrDwAAwlXYa2toaF2taWjPzJI2/K9fD+cdrFqdvN7awFGnU9q7x/zbu3IFAAAIV2HPu+fKM2xZx9scOex3r9Vdb7zl35E2bQ0cTUoyjfM1NWZ9bAECANCMcBXurJ6rQyW+p5pLHWpif2jeZbrOcVjnnXaqf98wZKipUFlhKy7ObD8ePy6Vl5vGdbYCAQBoxhDRcDdipBnEaY02sNTWmj6rxMR2H8JRXa1H3/2bGhobNaBPH/+DlWRCnWcVq75eysqSzhhlRkF4910BANDLEa7CXUKCqVhNnNxytIE1jqGmps1vt3qsfr/xI+0qL+/Yc8cnuAeSJidLGRnm3wMGmnWVFEt7dnfsMQEAiHBsC/YEVp+VdWhzzhATqlprOj/Bu3n9rAEDOva8TY2mOpXeTxo7ztxm9X8V7OjCCwIAIHIRrnoSq//qYJGpGrXB76sCWxMVZbYj9+4xVyNaocrqr/KcaQUAAJoRrsKd59WCVpCpqWk3XBU6Dqu8qrpzwcpmk5qapKhoU7k6VCyVlrZsqmemFQAAPhGuwp33hPYxZ0sVR6VdX/qcxl7X0KD4mBidOyRHH/3435UUF9fx57S2Gg/sc/dcpaWdfF4gAAA4CQ3t4S5niNmeyxni7rnas9tnsHJUV2vuU7/T2s0mjHUoWCUltfzcZnMHq9hYadKUk5vqAQDASQhX4e7AflMxOrC/ZRUrOrrF3aweq72HD2t4//4dfx7Pqw6jo031Kj7efO50Sju2m4oVA0MBAGgT24LhzvtswZoa0wPlwbN5fe3yq5Tb0R4rqeVVh40nrhJMTpYKC6UGp7liEAAAtItwFe68G8ftZaa5/IRap1NLnlnbtWBlsUY7ZA80Vwhu/czcPjjHPYoBAAC0iXDV49hafJYQG6urJp2jkZmZXQtWkpSRaQ6DnnCu1CfNffuo0WwHAgDgJ3quepqpuVJcnOxVVfrsQJEk6drzz+t6sJKk8jKpokLa9rkJUxPONf8RrAAA8BvhqqeJT5A9vb+W/OFZfee5F1RjXdHXGV5N8bLZzJbgpCldWyMAAL0Y24I9jP3Tj7Xknnube6wSOzPHytLY2PLzpibTa5XWt0trBACgN6Ny1YPYi4q05PY7TbD69tWd3wqMjZXSPcY1REdLZ4wy/y4rNfO0AABApxCuepDf/2KVCktKzFWBp53a+Qc6/wLJWef+PL2fdNZoc1VgSbGZpwUAADqFbcEe5PYrl+ryAZkanT2gaw/00UYzfT0mVurXTyo9ZIaU5s5wn2MIAAA6hcpVmLPb7fruv/2byv7+gWKio7serDyPtWlwSsePS6PHmkBlzdTi6kAAADqNylUYs9vtWrJkiQr37tF1g7OVOWJ41x/UcxJ7bKxUecwcBC2ZcwutkAUAADqFcBWmmoNVYaHW/PAHmpJ+itTQ0PUHTk6WqqvNv1PTJGe9dPb4lucWek6EBwAAHUK4CkMtgtXKezXN1tSxB0hMlFJSpPJyKSrKjFiwZA804aqpyfRaSWZoqDXbin4rAAC6hHAVhlwul+Lj47VmzRpNS4iTdnzh3zfGxUv1ddKwEVJMjGlYL2l5yLNKik24Gj1WGpBtRi8UHTBfy53BliAAAF1EuAojR44cUUpKijIyMvT2228rqvKY9Ld3/fvmcRPMGYBf75KcTnPo8uix5mt90kzYOuxwh62YGHO0TW2t9OEGE7C+3sWWIAAAXUS4ChMOh0OLFy/W6NGj9eSTTyoqKkr6ZLNpOPeXdbVfba1pVnc6TZgaNNh9e8EOc99Ro93fwwgGAAAChnAVBqxgVVhYqAceeMD9hTNHubfs2hIVJQ3zuJLQO2RZock6jNmbdX8AANBlzLkKMc9gtWbNGk2bNs39xS+2+vcgTU3Snt0n387cKgAAuh3hKoRcLpeuu+4638GqtlaqqAjd4gAAQKewLRhCNptNK1euVG1trXJzc1t+8etdHTtAedDgwC4OAAB0CpWrEHA4HHr55ZclSRMnTjw5WNXWmmb0vn39f9BtnwdsfQAAoPOoXHUzzx6rCy64QIMGDTr5Tl/vMqMUovzMvnFxZso6AAAIOSpX3ci7ed1nsJLM1X2pfVpOVm+NdRBzWWlgFwsAADqFylU3afOqQF/8rVplD5IGDmRGFQAAYaJHVK7eeustXXbZZZo5c6aef/75UC+nU/7xj39o3759/gWrgh1SxVH/HthRboIV4xYAAAgLYV+5Ki0t1RNPPKHXXntNcXFxWrp0qaZMmaIRI0aEeml+cblcstlsuuKKK3T++ecrKysrsE9QV8exNQAAhJGwr1xt3LhR5513nvr27aukpCTNnj1b+fn5oV6WX+x2u/Ly8rRp0yZJ8j9YWUfT+CMtjS1BAADCSNiHq7KyMmVkZDR/npmZqdLS8G/ettvtWrJkiQoKCtTQ0BCcJ4mKkiZNYUsQAIAwEvbhyuVynXSbzWYLwUr8ZwUrv5vXvX22pf37xMebqwm/LOjcIgEAQFCEfbjKysqS3W5v/rysrEyZmZkhXFHbjh492rVgJUkH9rV/n7o6M65h0pSOPz4AAAiasA9XU6dO1aZNm3T48GHV1NTo3Xff1fTp00O9rFalpqbqnHPO6XywkszsKn8MGy6l9e3ccwAAgKAI+6sFs7KydOutt2rFihVyOp1atGiRzj47/K6Ms9vtcjqdys7O1i9+8YuuPdi5k6S/f9D615OSpNOGd6zxHQAAdIuwD1eSlJeXp7y8vFAvo1VWj1VUVJTeeecdRfk7ALQ1X+1q++vHj0sxMTSyAwAQhnpEuApnns3ra9eu7XqwkqQ+aVJJ8cm322zSKf2kw/aTvwYAAMIC4aoLvINVbm5uYB44ppUfi8slZWRISYmm3woAAISdsG9oD2f3339/4IOVJLU1F+voYanogHRgf+CeDwAABAyVqy546KGHdM0112jSpEmBfeCjh1v/Wv9MKWcoU9kBAAhThKsuSE9PV3p6euAfuH+m5GsK/eix0thxNLIDABDG2BYMR2PHmaNtvI08wxzSXFvb/WsCAAB+IVyFo4QE38NBP9ksffqxCVgAACAssS0YrmKiW34+4nQpPkHq159+KwAAwhiVq3CVOcB8tI7Cqa2VdnwhxcbScwUAQBijchWuxo6TKo6asQuDc8wBzQOyqVoBABDmCFfhKiFByp1h+qtGjGy9DwsAAIQVwlU4S0iQxoTfIdUAAKB19FwBAAAEEOEKAAAggAhXAAAAAUS4AgAACCDCFQAAQAARrgAAAAKIcAUAABBAhCsAAIAAIlwBAAAEEOEKAAAggAhXAAAAAUS4AgAACCDCFQAAQAARrgAAAAKIcAUAABBAhCsAAIAAIlwBAAAEEOEKAAAggAhXAAAAAUS4AgAACKCYUC8gWBobGyVJhw4dCvFKAABApLHyhZU3PEVsuCovL5ckLVu2LMQrAQAAkaq8vFxDhw5tcZvN5XK5QrSeoKqtrdX27duVkZGh6OjoUC8HAABEkMbGRpWXl2vMmDFKSEho8bWIDVcAAAChQEM7AABAABGuAAAAAohwBQAAEECEKwAAgAAiXAEAAAQQ4QoAACCACFcAAAABRLgKorfeekuXXXaZZs6cqeeffz7UywlLK1as0Ny5czV//nzNnz9fW7dubfV927hxo/Ly8jRr1iw98cQTIVx1aFVVVWnevHkqKiqS1Pr7UlBQoIULF2r27Nm699571dDQIEkqLi7WsmXLNGfOHH3/+99XdXV1SF5HqHi/f3fffbdmzZrV/Dv43nvvSer4+xrpfvWrX2nu3LmaO3euVq1aJYnfPX/5eu/4vfPf6tWrddlll2nu3Ln6wx/+IKkH/O65EBSHDh1yXXTRRa4jR464qqurXXl5ea6vvvoq1MsKK01NTa4LLrjA5XQ6m29r7X2rqalxzZgxw7V//36X0+l0XXvtta4PPvgghKsPjc8//9w1b9481+jRo10HDhxo832ZO3eu67PPPnO5XC7X3Xff7Xr++eddLpfL9d3vftf19ttvu1wul+tXv/qVa9WqVSF5LaHg/f65XC7XvHnzXKWlpS3u15n3NZL985//dC1ZssRVV1fnqq+vd61YscL11ltv8bvnB1/v3bvvvsvvnZ82b97sWrp0qcvpdLpqampcF110kaugoCDsf/eoXAXJxo0bdd5556lv375KSkrS7NmzlZ+fH+plhZU9e/bIZrPp+uuv1+WXX67nnnuu1fdt27ZtGjp0qHJychQTE6O8vLxe+X6+/PLLuv/++5WZmSlJrb4vBw8eVG1trcaPHy9JWrBggfLz8+V0OvXJJ59o9uzZLW7vLbzfv+PHj6u4uFj33Xef8vLy9OSTT6qpqanD72uky8jI0F133aW4uDjFxsZq+PDhKiws5HfPD77eu+LiYn7v/DR58mQ9++yziomJkcPhUGNjo44dOxb2v3sRe3BzqJWVlSkjI6P588zMTG3bti2EKwo/x44d0/nnn68HHnhAtbW1WrFihb7xjW/4fN98vZ+lpaWhWHZI/exnP2vxeWvvi/ftGRkZKi0t1ZEjR5SSkqKYmJgWt/cW3u+fw+HQeeedpwcffFBJSUm64YYb9MorrygpKalD72ukO/3005v/XVhYqHXr1mn58uX87vnB13v3P//zP/r444/5vfNTbGysnnzySf33f/+35syZ0yP+d4/KVZC4fBzZaLPZQrCS8DVhwgStWrVKSUlJSk9P16JFi/Tkk0+edD+bzcb72YrW3peO3t5b5eTk6Ne//rX69eunxMRELV++XBs2bOD9a8VXX32la6+9VnfeeaeGDBly0tf53Wud53s3bNgwfu866Oabb9amTZtUUlKiwsLCk74ebr97hKsgycrKkt1ub/68rKyseSsCxqeffqpNmzY1f+5yuTRo0CCf7xvvp2+tvS/et5eXlyszM1Pp6emqqqpSY2Nji9t7q507d+qdd95p/tzlcikmJqbD72tvsGXLFl1zzTW67bbb9M1vfpPfvQ7wfu/4vfPf7t27VVBQIElKTEzUrFmztHnz5rD/3SNcBcnUqVO1adMmHT58WDU1NXr33Xc1ffr0UC8rrFRWVmrVqlWqq6tTVVWV/vznP+sXv/iFz/dt3Lhx2rt3r/bt26fGxka9/fbbvJ9Sq+/LoEGDFB8fry1btkiSXn/9dU2fPl2xsbGaOHGi1q1b1+L23srlcunhhx9WRUWFnE6nXnrpJc2cObPD72ukKykp0Y033qjHHntMc+fOlcTvnr98vXf83vmvqKhIK1euVH19verr67V+/XotXbo07H/3bC5f9TIExFtvvaXf/va3cjqdWrRoka6//vpQLyns/PKXv9Q777yjpqYmXXXVVfr2t7/d6vu2adMmPfLII6qrq9OMGTN0991397rSuOXiiy/Ws88+q8GDB7f6vnz55ZdauXKlqqurddZZZ+mRRx5RXFycDh48qLvuuksOh0PZ2dl6/PHHlZaWFuqX1K0837/nn39ezz//vBoaGjRr1iz9+Mc/ltT671tr72sk++lPf6pXX321xVbg0qVLdeqpp/K7147W3rumpiZ+7/z05JNPKj8/X9HR0Zo1a5ZuuummsP/fPcIVAABAALEtCAAAEECEKwAAgAAiXAEAAAQQ4QoAACCACFcAAAABRLgC0Ctt27ZN/+///b9QLwNABCJcAeiVvv766151PhuA7kO4AhARVq5cqccff7z58zfffFM33nijz/uWlJToySef1Keffqq7775bmzdv1uWXX66lS5fq8ssv1z/+8Q/Nmzev+f6bN29u8fnTTz+tb37zm5o/f75+8IMfENIAtEC4AhARli1bptdee00NDQ2SpJdeeklLly71ed/s7GzdfPPNmjhxoh555BFJ5mDd//iP/9Cbb77Z5uTr119/Xbt27dKf/vQnvfHGG5oxY4ZWrlwZ+BcEoMeKCfUCACAQRo0apcGDB+uDDz7QaaedprKyMuXm5vr9/dnZ2Ro0aFC793v//ff1xRdfaOHChZKkpqYm1dTUdHrdACIP4QpAxFi2bJleffVVnXrqqVq8eHGHzp5MSkpq/rfNZpPnyWBOp7P5301NTfq3f/s3XXXVVZKk+vp6VVRUBGD1ACIF24IAIsbs2bNVUFCgd999t7my1Jro6OjmLURv6enpKi4ulsPhkMvl0t/+9rfmr+Xm5uqVV15RVVWVJGn16tW64447AvciAPR4VK4ARIy4uDjNnj1bdrtd6enpbd53woQJ+uUvf6kbb7xRK1asaPG1ESNGaOnSpVq4cKEyMjJ04YUXNn/tW9/6lkpLS5srY9nZ2Xr00UeD8XIA9FA2l2ftGwB6sOPHj+vqq6/W/fffr3HjxoV6OQB6KSpXACLCP/7xD912221auHChxo0bpz179ujWW2/1ed/TTjtNv/zlL7t3gQB6DSpXAAAAAURDOwAAQAARrgAAAAKIcAUAABBAhCsAAIAAIlwBAAAEEOEKAAAggP4/woZ3NOuIA6EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction_scatter(y_val, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\n",
    "def plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n",
    "    demo_df = pd.DataFrame({'row_id': 0,\n",
    "                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n",
    "                            'country': country,\n",
    "                            'store': store,\n",
    "                            'product': product})\n",
    "    demo_df.set_index('date', inplace=True, drop=False)\n",
    "    demo_df = engineer(demo_df)\n",
    "    demo_df_f = pd.DataFrame(preproc.transform(demo_df[features]), columns=features, index=demo_df.index)\n",
    "    demo_df['num_sold'] = np.exp(model.predict([demo_df_f[features]]))\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n",
    "    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n",
    "    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n",
    "    plt.legend()\n",
    "    plt.title('Predictions and true num_sold for five years')\n",
    "    plt.show()\n",
    "\n",
    "# plot_five_years_combination(engineer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc8a78a13283e3ba74119858067a74c2c7a55702e09c935fdd8fe4b244251524"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('3.9.7': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
